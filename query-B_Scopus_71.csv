"Authors","Author full names","Author(s) ID","Title","Year","Link","Abstract","Author Keywords"
"Han, R.; Wang, X.; Bai, N.; Hou, J.; Zhang, W.; Li, J.","Han, Ruidong (58199903300); Wang, Xiaofeng (7501854020); Bai, Ningning (58102462000); Hou, Jianpeng (58861477200); Zhang, Wenshuo (60041028100); Li, Jianghua (55800222700)","58199903300; 7501854020; 58102462000; 58861477200; 60041028100; 55800222700","HSFF-Net: Hierarchical spectral-feature fusion network for deepfake detection and localization","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013114740&partnerID=40&md5=66d399fe80dc48060d8a44e3daa35710","The rapid development of deepfake techniques poses a serious threat to multimedia authenticity, driving increased attention to deepfake detection. However, most existing methods focus solely on classification while overlooking forgery localization, which is essential for understanding manipulation intent. To address this issue, we propose a novel Hierarchical Spectral-Feature Fusion Network (HSFF-Net) for deepfake detection and localization from spatial- and frequency-domain views. Specifically, the Spectral Detail Amplification (SDA) module enhances tampering cues around facial features in the frequency domain. The Dynamic Collaborative Fusion (DCF) unit integrates complementary dual-stream features across multiple hierarchical levels to highlight valuable information. The Adaptive Feature Elevation (AFE) module bridges coarse semantic and fine-grained details in a top-down manner. Furthermore, the Global Guidance Exposure (GGE) module injects localization cues across feature levels to improve forgery localization accuracy. Additionally, we design the contrastive clustering loss for the detection task, which guides features to cluster around their corresponding class centers while simultaneously pushing them away from other class centers, thereby promoting intra-class compactness and inter-class separability. Abundant experiments demonstrate that HSFF-Net achieves superior performance on both detection and localization tasks, with good generalization across diverse datasets and robustness against various perturbations. © 2025","Adaptive feature elevation; Contrastive clustering loss; Deepfake detection and localization; Dynamic collaborative fusion; Global guidance exposure; Spectral detail amplification"
"Jeon, M.; Woo, S.S.","Jeon, Minsun (59916876300); Woo, Simon S. (57202046772)","59916876300; 57202046772","Seeing Through the Blur: Unlocking Defocus Maps for Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023147626&partnerID=40&md5=7789f96de46db76af6c1da1e75866f8d","The rapid advancement of generative AI has enabled the mass production of photorealistic synthetic images, blurring the boundary between authentic and fabricated visual content. This challenge is particularly evident in deepfake scenarios involving facial manipulation, but also extends to broader AI-generated content (AIGC) cases involving fully synthesized scenes. As such content becomes increasingly difficult to distinguish from reality, the integrity of visual media is under threat. To address this issue, we propose a physically interpretable deepfake detection framework and demonstrate that defocus blur can serve as an effective forensic signal. Defocus blur is a depth-dependent optical phenomenon that naturally occurs in camera-captured images due to lens focus and scene geometry. In contrast, synthetic images often lack realistic depth-of-field (DoF) characteristics. To capture these discrepancies, we construct a defocus blur map and use it as a discriminative feature for detecting manipulated content. Unlike RGB textures or frequency-domain signals, defocus blur arises universally from optical imaging principles and encodes physical scene structure. This makes it a robust and generalizable forensic cue. Our approach is supported by three in-depth feature analyses, and experimental results confirm that defocus blur provides a reliable and interpretable cue for identifying synthetic images. We aim for our defocus-based detection pipeline and interpretability tools to contribute meaningfully to ongoing research in media forensics. The implementation is publicly available at: https://github.com/irissun9602/Defocus-Deepfake-Detection. © 2025 Copyright held by the owner/author(s).","deepfake detection; defocus blur; image forensics"
"Alam, I.; Islam, M.T.; Woo, S.S.","Alam, Inzamamul (58909433700); Islam, Md Tanvir (57214094988); Woo, Simon S. (57202046772)","58909433700; 57214094988; 57202046772","SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105024070638&partnerID=40&md5=8243cfaf3216cfd76b1cddf67490e46f","The increasing realism of content generated by GANs and diffusion models has made deepfake detection significantly more challenging. Existing approaches often focus solely on spatial or frequency-domain features, limiting their generalization to unseen manipulations. We propose the Spectral Cross-Attentional Network (SpecXNet), a dual-domain architecture for robust deepfake detection. The core Dual-Domain Feature Coupler (DDFC) decomposes features into a local spatial branch for capturing texture-level anomalies and a global spectral branch that employs Fast Fourier Transform to model periodic inconsistencies. This dual-domain formulation allows SpecXNet to jointly exploit localized detail and global structural coherence, which are critical for distinguishing authentic from manipulated images. We also introduce the Dual Fourier Attention (DFA) module, which dynamically fuses spatial and spectral features in a content-aware manner. Built atop a modified XceptionNet backbone, we embed the DDFC and DFA modules within a separable convolution block. Extensive experiments on multiple deepfake benchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly under cross-dataset and unseen manipulation scenarios, while maintaining real-time feasibility. Our results highlight the effectiveness of unified spatial-spectral learning for robust and generalizable deepfake detection. To ensure reproducibility, we release the full code on https://github.com/inzamamulDU/SpecXNet. © 2025 Copyright held by the owner/author(s).","deepfake detection; dual-domain learning; fake image classification; frequency domain; security"
"Battiato, S.; Casu, M.; Guarnera, F.; Guarnera, L.; Puglisi, G.; Pontorno, O.; Ragaglia, C.V.; Akhtar, Z.","Battiato, Sebastiano (6603989025); Casu, Mirko (57374838700); Guarnera, Francesco (57211180426); Guarnera, Luca (57196273602); Puglisi, Giovanni (55346939100); Pontorno, Orazio (58898116500); Ragaglia, Claudio Vittorio (58970164400); Akhtar, Zahid (46661628200)","6603989025; 57374838700; 57211180426; 57196273602; 55346939100; 58898116500; 58970164400; 46661628200","Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of AI-Generated Media (AADD-2025)","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105024068051&partnerID=40&md5=f6a14362d722829cf87fdd4bbefa1661","The rapid proliferation of AI-generated media, particularly hyper-realistic deepfakes, has underscored the critical need for robust detection systems to mitigate risks such as misinformation and identity theft. However, state-of-the-art deepfake detectors remain vulnerable to adversarial attacks-subtle perturbations designed to evade classification. To address this gap, we organized the Adversarial Attacks on Deepfake Detectors (AADD-2025) challenge, a competitive evaluation aimed at advancing methodologies to expose and strengthen weaknesses in deepfake detection models. The challenge tasked participants with generating adversarial examples capable of evading four diverse classifiers (including ResNet, DenseNet, and two blind models) while preserving structural similarity to original deepfakes. A dataset comprising 16 subsets of high- and low-quality deepfake images generated by GAN-based and diffusion models (e.g., StableDiffusion, StyleGAN3) was provided. Participants were evaluated using a weighted combination of Structural Similarity Index (SSIM) and attack success rates across all classifiers. Thirteen teams proposed innovative solutions leveraging techniques such as latent-space manipulation, ensemble gradient optimization, surrogate modeling, and frequency-domain perturbation. Top-performing approaches, including MR-CAS (1st place), Safe AI (2nd place), and RoMa (3rd place), achieved high SSIM scores (0.74-0.93) while successfully misleading classifiers. Notably, MR-CAS's latent diffusion model inversion strategy and Safe AI's consensus-orthogonal gradient weighting framework demonstrated superior transferability across architectures, including Vision Transformers. The challenge revealed critical insights: latent-space attacks outperformed pixel-level methods, ensemble-based strategies enhanced cross-model robustness, and adversarial perturbations optimized for both CNNs and transformers proved most effective. However, gaps persist in generalizing attacks across heterogeneous models and maintaining perceptual fidelity, highlighting the urgency of developing adaptive defenses and hybrid detection mechanisms. By fostering collaboration and innovation, AADD-2025 provides a benchmark for evaluating adversarial robustness in deepfake detection and underscores the need for resilient systems in the era of AI-generated media. © 2025 Copyright held by the owner/author(s).","adversarial attacks; ai-generated media; deepfake detection; diffusion models; digital forensics; ensemble methods; generative models; latent-space manipulation; perceptual quality; transferability; vision transformers"
"Sai Charan, G.; Karthik, R.; Muktha Sri, V.; Kandepi, D.S.S.","Sai Charan, Guntupalli V. (57866214000); Karthik, R. (55953378300); Muktha Sri, Vuppula (59899759800); Kandepi, Durjai Sri Sai (59900581900)","57866214000; 55953378300; 59899759800; 59900581900","CAL-Net: a continual adaptive learning network for deepfake detection using multi-track fusion and domain adaptation","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016099215&partnerID=40&md5=d4ba5410f860c6b6fbbbbdcdc619ad49","The fast advancement of deepfake generation methods poses a serious challenge to the detection of deepfakes using traditional methods. These methods rely on static models and low-level artifact analysis, limiting their ability to generalize across evolving manipulations. This widening gap requires a new approach to deepfake detection methods that are both adaptive and dynamic. To address this problem, we introduce the Continual Adaptive Learning Network (CAL-Net), a novel method that combines multi-track feature extraction and dynamic domain adaptation for deepfake detection. CAL-Net consists of three complementary feature extractors: frequency domain analysis to identify inconsistencies introduced by deepfake manipulations, Data-Efficient Image Transformer (DEIT) for representing global information, and Xception network for learning local information. By combining these multi-track features, CAL-Net effectively detects important manipulation traces. We also propose a domain adaptation module that leverages cosine similarity for effective domain detection. This module dynamically adapts to new manipulation methods while mitigating the issue of catastrophic forgetting. To the best of our knowledge, this is the first work to combine multi-track feature extraction and domain adaptation with continual learning for deep fake detection. Evaluating on the Continual Deepfake Detection Benchmark (CDDB) dataset, CAL-Net achieved 94% accuracy and outperformed existing methods. The proposed approach demonstrates cross-domain generalization and robustness to diverse and new deepfake attacks. The research emphasizes the need for continual learning and multi-track feature fusion to make deepfake detection systems scalable. © 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.","adaptive domain adaptation; catastrophic forgetting; continual learning; deepfake detection; multi-track feature extraction"
"Krasilnikov, M.; Nikitin, M.; Konushin, A.","Krasilnikov, Maksim (60116876300); Nikitin, Mikhail Yurievich (57199053672); Konushin, Anton S. (6506829728)","60116876300; 57199053672; 6506829728","VCF: A Real-World Video Conference Deepfake Benchmark for Face-Swap Detection and Robustness Evaluation","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017233520&partnerID=40&md5=b7f78ac48f3d1d86dd90471a67b8d487","The rapid advancement of deepfake generation techniques poses significant security and privacy risks, particularly in video conferencing scenarios where variable resolutions, compression artifacts, and environmental factors complicate detection. Existing benchmarks often fail to address these context-specific challenges, limiting their applicability to real-world communication platforms. To bridge this gap, we introduce VCF (Video Conference DeepFakes) dataset, the first, to the best of our knowledge, specialized benchmark designed for evaluating deepfake detection in video conferencing contexts. VCF leverages the VCD dataset as target videos and the LaPa dataset as a set of source faces, algorithmically ranking sources by similarity in gender, ethnicity, age, and facial hair to select optimal matches for enhanced deepfake visual plausibility. The dataset incorporates multi-resolution videos, H.264 compression artifacts from different compression rates, and diverse backgrounds to simulate conditions specific to video conferences. Comprehensive evaluations of 14 detection methods reveal significant performance degradation under video quality variations. Our results emphasize the critical need for robust detection frameworks resilient to resolution shifts, compression artifacts, and diverse generation pipelines. VCF provides a standardized, scenario-specific benchmark to drive advancements in securing digital communication platforms against evolving deepfake threats. © © 2025 Maksim Krasilnikov et al.","Benchmark; Dataset; Deepfake detection; Face-swapping detection"
"Jin, X.; Kou, Y.; Xie, Y.; Zhao, Y.; Mat Kiah, M.L.; Jiang, Q.; Zhou, W.","Jin, Xin (56991832300); Kou, Yuru (58988680500); Xie, Yuhao (60074940000); Zhao, Yuying (60075106400); Mat Kiah, Miss Laiha (57214221751); Jiang, Qian (57194699462); Zhou, Wei (56857006600)","56991832300; 58988680500; 60074940000; 60075106400; 57214221751; 57194699462; 56857006600","Learning Local Texture and Global Frequency Clues for Face Forgery Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014369586&partnerID=40&md5=d888b821b13e9abbe0916488284c6a01","In recent years, the rapid advancement of deep learning techniques has significantly propelled the development of face forgery methods, drawing considerable attention to face forgery detection. However, existing detection methods still struggle with generalization across different datasets and forgery techniques. In this work, we address this challenge by leveraging both local texture cues and global frequency domain information in a complementary manner to enhance the robustness of face forgery detection. Specifically, we introduce a local texture mining and enhancement module. The input image is segmented into patches and a subset is strategically masked, then texture enhanced. This joint masking and enhancement strategy forces the model to focus on generalizable localized texture traces, mitigates overfitting to specific identity features and enabling the model to capture more meaningful subtle traces of forgery. Additionally, we extract multi-scale frequency domain features from the face image using wavelet transform, thereby preserving various frequency domain characteristics of the image. And we propose an innovative frequency-domain processing strategy to adjust the contributions of different frequency-domain components through frequency-domain selection and dynamic weighting. This Facilitates the model’s ability to uncover frequency-domain inconsistencies across various global frequency layers. Furthermore, we propose an integrated framework that combines these two feature modalities, enhanced with spatial attention and channel attention mechanisms, to foster a synergistic effect. Extensive experiments conducted on several benchmark datasets demonstrate that the proposed technique demonstrates superior performance and generalization capabilities compared to existing methods. © 2025 by the authors.","bioinformatics; deep learning; deepfake detection; face forgery detection; frequency domain"
"Qi, Y.; Xie, H.; Gao, Y.; Lin, Y.; Zhang, H.; Han, H.","Qi, Yongfeng (54893032200); Xie, Hongli (59462971900); Gao, Yajuan (59982470400); Lin, Yuanzhe (59463341200); Zhang, Heng (59463525100); Han, Haixi (59982611300)","54893032200; 59462971900; 59982470400; 59463341200; 59463525100; 59982611300","Generalizable face forgery detection based on adaptive spatial-frequency information mining","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010001788&partnerID=40&md5=545dd79d953a33794c6084c8b64b78f1","In the current field of face forgery detection, researchers are focused on recognizing forgery cues through the combination of frequency information and convolutional neural networks (CNN). However, existing methods often fail to capture spatial correlations with image content when extracting frequency features, making it difficult to accurately recognize highly simulated forged images. In addition, these methods perform well on homogeneous datasets, but their effectiveness decreases significantly when evaluated on cross-dataset samples. To address these issues, we propose a novel adaptive spatial-frequency information mining (ASFIM) method for generalizable face forgery detection. Specifically, the ASFIM method first processes the original RGB image through a frequency-aware learning module. This module extracts forgery frequency information closely related to the image content, which is subsequently used as input for frequency branching. Next, a spatial texture enhancement module is introduced to enable interaction between spatial and frequency features at an early stage. This approach not only strengthens the expressiveness of forgery features in the spatial domain but also provides an effective guide for recognizing forgery cues in the frequency domain. Finally, we designed the cross-domain interactive attention (CDIA) module to enhance forgery cues by deeply fusing spatial texture and frequency-aware features. Extensive experimental results demonstrate that the proposed ASFIM method outperforms various advanced methods in terms of generalization ability across challenging benchmark tests. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.","DCT; Deepfake detection; Face forgery detection; Frequency-aware learning; Spatial texture enhancement"
"Yao, W.; Li, P.; Zhao, Y.; Wu, H.","Yao, Wenda (60042789100); Li, Panchi (16245350600); Zhao, Ya (60042679500); Wu, Hongchao (60042679600)","60042789100; 16245350600; 60042679500; 60042679600","Review of research on face deepfake detection methods; 人脸深度伪造检测方法研究综述","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013250991&partnerID=40&md5=2f1980f5956901189d7050a1b8a510b3","Deepfake technology refers to the synthesis of images，audio，and videos by using deep learning algorithms. This technology enables the precise mapping of facial features or other physical characteristics from one person onto a subject in another video，achieving highly realistic face-swapping effects. With advancements in algorithms and the increased accessibility of computational resources，the threshold for utilizing deepfake technology has gradually lowered，bringing convenience and numerous social and legal challenges. For example，deepfake technology is used to bring deceased actors back to the screen，providing a novel experience to the audience. Meanwhile，it is frequently exploited to impersonate citizens or leaders for fraudulent activities，produce pornographic content，or create fake news to influence public opinion. Consequently，the importance of deepfake detection technology is increasing，making it a significant focus of current research. To detect images and videos synthesized via deepfake technology，researchers must design models that can uncover subtle traces of manipulation within these media. However，accurately identifying these traces remains challenging due to several factors that complicate the detection process. First，rapid advancements in deepfake technology have made differentiating fake images and videos from authentic content increasingly difficult. As techniques such as generative adversarial networks（GANs）and diffusion models continue to evolve and improve，the texture，lighting，and motion within synthesized media become more seamlessly realistic，imposing significant challenges on detection models that seek to recognize subtle cues of manipulation. Second，forgers can employ a variety of countermeasures to obscure traces of manipulation，such as applying compression，cropping，or noise addition. Furthermore，forgers may create adversarial samples that are specifically crafted to exploit and bypass the vulnerability of detection models，making the identification of deepfake even more complex. Thirdly，the generalizability of deepfake detection methods remains a significant hurdle，because different generative techniques leave behind distinct forensic traces. For example，GAN-generated images frequently exhibit prominent grid-like artifacts in the frequency domain，while images produced through diffusion models typically leave only subtle，less detectable traces in this domain. Therefore，detection models that do not exclusively rely on low-level，technique-specific features but instead focus on capturing deep，generalized features that ensure robustness and applicability across diverse forgery types and detection scenarios are crucial. To address these multifaceted challenges，numerous scholars have proposed a variety of detection methods that are designed to capture nuanced traces left by deepfake manipulations. For example，certain approaches focus on identifying subtle forgery artifacts within the frequency domain of images，capitalizing on the distinct spectral anomalies that forgeries frequently introduce. Other methods prioritize assessing temporal consistency across video frames，because unnatural transitions or frame-level inconsistencies can indicate synthesized content. In addition，some detection strategies focus on evaluating synchronization among different modalities within videos，such as audio and visual elements，to detect inconsistencies that may reveal forgery. At present，several review papers in academia have summarized key research and developments within this domain. However，given the rapid advancements in generative artificial intelligence（AI），fake faces created with diffusion models have recently gained popularity，with scarcely any review that addresses the detection of such forgeries. Furthermore，as generative AI progresses continues advancing toward multimodal integration，deepfake detection methods are similarly evolving to incorporate features from multiple modalities. Nonetheless，the majority of existing reviews lack sufficient focus on multimodal detection approaches，underscoring a gap in the literature that this review seeks to address. To provide an up-to-date overview of face deepfake detection，this review first organizes commonly used datasets and evaluation metrics in the field. Then，it divides detection methods into image-level and video-level face deepfake detection. Based on feature selection approaches，image-level methods are categorized into spatial-domain and frequency-domain methods，while video-level methods are categorized into approaches based on spatiotemporal inconsistencies，biological features，and multimodal features. Each category is thoroughly analyzed with regard to its principles，strengths，weaknesses，and developmental trends. Finally，current research status and challenges in face deepfake detection are summarized，and future research directions are discussed. Compared with other related reviews，the novelty of this review lies in its summary of detection methods that specifically targets text-to-image/video generation and multimodal detection methods. This review is aligned with the latest trends in generative AI，offering a comprehensive and up-to-date summary of recent advancements in face deepfake detection. By examining the latest methodologies，including those developed to address forgeries created through advanced techniques，such as diffusion models and multimodal integration，this review reflects the ongoing evolution of detection technology. It highlights the progress made and the challenges that remain，positioning itself as a valuable resource for researchers who aim to navigate and contribute to the cutting-edge developments in this rapidly advancing field. A comprehensive analysis of face deepfake detection methods reveals that current techniques achieve nearly 100% accuracy within the training datasets，particularly those leveraging advanced models，such as Transformers. However，their performance frequently declines significantly in cross-dataset testing，particularly for spatial-domain and frequency-domain detection methods. This decline suggests that these approaches may fail to capture essential，generalizable features that are robust across varying datasets. By contrast，biological feature-based methods demonstrate superior generalization capabilities，successfully adapting to different contexts. However，they require carefully tailored training data and specific application conditions to reach optimal performance. Meanwhile，multimodal detection methods，which integrate features across multiple modalities，offer enhanced robustness and adaptability due to their layered approach. However，this added complexity frequently results in higher computational costs and increased model intricacy. Given the diversity in feature selection，along with the unique advantages and limitations inherent to each detection approach，no single method has yet provided a fully comprehensive solution to the deepfake detection challenge. This reality underscores the critical need for continued research in this evolving field and highlights the importance of this review in mapping current advancements and identifying future research directions. © 2025 Editorial and Publishing Board of JIG. All rights reserved.","deepfake detection; face forgery detection; face image; face video; frequency domain feature; multimodal features; spatial domain feature; temporal features"
"Zou, Z.; Peng, D.; Zhao, Y.; Tian, Z.; Cai, J.","Zou, Zheng (59797813300); Peng, Dunlu (8556510900); Zhao, Yu (59955122700); Tian, Zekun (57222006521); Cai, Jun (59955988400)","59797813300; 8556510900; 59955122700; 57222006521; 59955988400","FTA-DFI: a framework for generalizable deepfake detection based on distinctive features from various manipulations compared to genuine images","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008697615&partnerID=40&md5=cfd68e632e0780718d502101c4e74a77","Deepfakes have attracted considerable attention due to their detrimental impact, notably the production of convincing fake images and their unregulated dissemination. However, existing detectors suffer from a common defect that obsessing with low-level local artifacts or predefined forgery patterns while neglecting the intrinsic differences between the forged and genuine images. This leads to significant performance decline when the model encounters with unseen domains and forgeries. To address this issue, this paper proposes a novel framework, named as FTA-DFI, which leverages disentanglement learning and adversarial learning to analyze forgery traces amplified from the perspective of key facial features and frequency domain. FTA-DFI consists of the Fake Trace Amplification Strategy (FTAS) and the Distinctive Feature Identification Strategy (DFIS). FTAS suppresses method-specific features and identity information using the Face Features Random Mask and amplifies manipulated artifacts through the Frequency Domain Converter. DFIS employs the Feature Separation Disentangler and Intrinsic-Adversarial Learning modules to discern distinctive features between fake and genuine images, thereby further mitigating the overfitting of forgery-irrelevant information and specific forgery approaches. Extensive experimental results demonstrate that FTA-DFI outperforms current state-of-the-art models in generalization evaluations across mainstream datasets, validating the superior performance of the proposed architecture in terms of generalization capability. The code is available at https://github.com/zouzhengcs/FTA-DFI. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.","Adversarial learning; Deepfake detection; Disentanglement learning; Frequency domain; Generalization"
"Gupta, V.; Srivastava, V.; Yadav, A.; Kumar Vishwakarma, D.K.; Kumar, N.","Gupta, Varun (58825872700); Srivastava, Vaibhav (59641846400); Yadav, Ankit (57216392985); Kumar Vishwakarma, Dinesh Kumar (56711360100); Kumar, Narendra (57448406900)","58825872700; 59641846400; 57216392985; 56711360100; 57448406900","FreqFaceNet: an enhanced transformer architecture with dual-order frequency attention for deepfake detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218634871&partnerID=40&md5=58a8ee5814c72cf68a4f661d3d15068e","With the advent of AI-based image synthesis tools and techniques, Deepfakes have become a serious problem as they pose a massive threat to one’s information security and personal privacy. Several architectures have been proposed to achieve robust Deep Fake detection. However, these methods suffer a drastic drop in performance if the images are visually degraded or have low resolution. To resolve these two issues, a novel FreqFaceNet model has been proposed that employs two novel attentions namely, Wavelet Attention and Fourier Attention, for extracting important frequency-based features from low-resolution images. The extraction of frequency-based features ensures minimal interference of noise due to image compression or low resolution. The proposed model excels on two public benchmark datasets—the DFDC and CelebDF. On the DFDC dataset, FreqFaceNet achieves 98.041% accuracy, an AUC value of 99.748, and a Mathews Correlation Coefficient (MCC) value of 93.857, while on the CelebDF dataset, it obtains an accuracy of 98.325%, an AUC value of 99.81, and an MCC value of 92.819. Qualitative analysis of the proposed model indicates strong classification capabilities. An ablation study has also been conducted to verify the complementary contributions of both Wavelet and Fourier Attention mechanisms. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.","Attention mechanism; Deepfake detection; Fourier transform; Frequency domain; Vision transformer; Wavelet"
"Luo, X.; Wang, Y.","Luo, Xinyu (59736504900); Wang, Yu (55734069900)","59736504900; 55734069900","Frequency-Domain Masking and Spatial Interaction for Generalizable Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002577421&partnerID=40&md5=514607a29b8cc275e52351f2eb36ded0","Over the past few years, the rapid development of deepfake technology based on generative models has posed a significant threat to the field of information security. Despite the notable progress in deepfake-detection methods based on the spatial domain, the detection capability of the models drops sharply when dealing with low-quality images. Moreover, the effectiveness of detection relies on the realism of the forged images and the specific traces inherent to particular forgery techniques, which often weakens the models’ generalization ability. To address this issue, we propose the Frequency-Domain Masking and Spatial Interaction (FMSI) model. The FMSI model innovatively introduces masked image modeling in frequency-domain processing. This prevents the model from focusing too much on specific frequency-domain features and enhances its generalization ability. We design a high-frequency information convolution module for spatial and channel dimensions to help the model capture subtle forgery traces more effectively. Also, we creatively design a dual stream architecture for frequency-domain and spatial-domain information interaction and overcome single-domain detection limitations. Our model is tested on three public benchmark datasets (FaceForensics++, Celeb-DF, and WildDeepfake) through intra-domain and cross-domain experiments. The detection and generalization capabilities of the model are evaluated using the AUC and EER metrics. The experimental results demonstrate that our model not only possesses high detection capability but also exhibits excellent generalization ability. © 2025 by the authors.","deepfake detection; Fourier transform; masked image modeling; vision transformer"
"Dong, J.; Deng, Z.; Li, X.; Wang, S.","Dong, Jiale (59426734600); Deng, Zhengjie (55449481900); Li, Xiyan (51665335400); Wang, Shiyun (57219391654)","59426734600; 55449481900; 51665335400; 57219391654","Deepfake detection method based on multi-feature fusion of frequency domain and spatial domain; 基于频域和空域多特征融合的深度伪造检测方法","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217905606&partnerID=40&md5=86c2fcbe19195947ed8ba520fa654fed","In today’s society, the rapid advancement of facial forgery technology has posed a substantial challenge to social security, especially in the context where deep learning techniques have been widely employed to generate realistic fake videos. These high-quality forged contents not only threaten personal privacy but can also be utilized for illegal activities. Faced with this challenge, traditional forgery detection methods based on single features have become inadequate to meet detection demands. To address this issue, a deepfake detection method based on multi-feature fusion in both frequency and spatial domains was proposed to enhance the detection accuracy and generalization capability for facial forgeries. The frequency domain was dynamically divided into three bands to extract forgery artifacts that cannot be mined in the spatial domain. The spatial domain employed the EfficientNet_b4 network and Transformer architecture to segment image blocks at multiple scales, calculate differences between different blocks, perform detection based on consistency information between upper and lower image blocks, and capture more detailed forgery feature information. Finally, a fusion block using a query-key-value mechanism integrated the methods from the frequency and spatial domains, thereby more comprehensively mining feature information from both domains to enhance the accuracy and transferability of forgery detection. Extensive experimental results confirmed the effectiveness of the proposed method, demonstrating significantly superior performance compared to traditional deepfake detection methods. © 2025 Editorial of Board of Journal of Graphics. All rights reserved.","deepfake detection; EfficientNet_b4 network; feature fusion; frequency domain features; spatial domain features"
"Fang, S.; Zhang, Z.; Song, B.","Fang, Shuaijv (59752246300); Zhang, Zhiyong (56153229800); Song, Bin (36976236900)","59752246300; 56153229800; 36976236900","Deepfake Detection Model Combining Texture Differences and Frequency Domain Information","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003417956&partnerID=40&md5=dc1b65c7481a6171168517602e5701ca","In recent years, public security incidents caused by deepfake technology have occurred frequently around the world, which makes an efficient and accurate deepfake detection model crucial. The existing advanced methods use the manipulation features in the image to realize the binary classification of real and fake images by training complex neural network models. However, these models rely on a single manipulation feature, and the detection accuracy of these methods will be greatly reduced when the forgery technology or image quality of the training dataset and the validation dataset are different. Inspired by the existing work, we propose a two-stream collaborative learning framework that combines spatial texture differences and frequency information. The average difference convolution (ADC) is designed to extract the spatial texture difference information of the image, and the gray image frequency-aware decomposition (GFAD) is used to extract the artifact information of the image in the frequency domain. At the same time, the ViT idea is combined with cross-attention mechanism for feature fusion to comprehensively mine forged features in forged images. Experimental results show that the proposed model has good detection effects on three benchmark datasets. In terms of cross-dataset evaluation, the AUC on Celeb-DF dataset reaches 82.86%, which is better than the existing advanced methods. © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Additional Key Words and PhrasesTexture difference; average difference convolution; cross-attention; deepfake detection; frequency domain information"
"Wan, Y.; Wang, J.; Cui, J.; Sun, Y.","Wan, Yuanfei (59661409700); Wang, Jian (59661001500); Cui, Jinrong (54913143700); Sun, Yunlian (55866644500)","59661409700; 59661001500; 54913143700; 55866644500","Exposing Audio-Visual Forgeries in Frequency Domain","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219214520&partnerID=40&md5=63431d02b43304a07d359ad723ccf0bd","Recently, the rapid development of deepfake technology attracted strong attention from the community. Some previous work on deepfake detection achieved good results in the frequency domain, which inspires us to combine frequency-domain information with temporal and spatial domains of visual to detect deepfakes. In addition, the audio signal can be represented in the frequency domain, so we can explore multimodal frequency-domain cues by combining audio and visual modalities. In this paper, we propose a Frequency-aware Audio-Visual Deepfake Detection(FAVDD) method. Specifically, we design a Frequency-Temporal-Spatial(FTS) visual encoder that extracts spatial, frequency, and temporal forgery cues and embeds them into visual features to form a unified representation. In addition, we project the audio signal into the frequency domain by Fourier transform and capture the forgery traces, which are later combined with visual features for deepfake detection. The results show that our proposed framework effectively combines multiple cues and achieves good results on three multimodal deepfake datasets. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.","Audio-Visual; Deepfake Detection; Frequency; Multimodal"
"Zhou, K.; Sun, G.; Wang, J.; Wang, J.; Yu, L.","Zhou, Kai (57219160695); Sun, Guanglu (46961469200); Wang, Jun (57203534543); Wang, Jiahui (58519282400); Yu, Linsen (13609451800)","57219160695; 46961469200; 57203534543; 58519282400; 13609451800","FLAG: frequency-based local and global network for face forgery detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188841097&partnerID=40&md5=6bb7914944d5f436b0a8c7440a05bdbd","Deepfake detection aims to mitigate the threat of manipulated content by identifying and exposing forgeries. However, previous methods primarily tend to perform poorly when confronted with cross-dataset scenarios. To address the above issue, we propose an innovative hybrid network called the Frequency-based Local and Global (FLAG) network to explore local and global information with the help of frequency-domain cues for better generalization capability. In consideration of the fact that forged faces often exhibit flaws in the frequency domain, we design a Frequency-based Attention Enhancement Module (FAEM) to enhance the aggregation of CNN and Vision Transformer (ViT). In this design, local features from CNN are attentively enhanced by selected frequency coefficients in FAEM, facilitating generalizable global features learning by the ViT module. The effectiveness of the proposed method is validated via numerous experiments and the generalization performance is improved under cross-dataset scenarios. Especially, the proposed method have obtained an AUC of 99.26% and an ACC of 96.56% using intra-dataset experimental results on FaceForensics++ (C23). © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","Channel attention; Deepfake detection; Hybrid network; Multimedia forensics; Vision transformer"
"Shi, Z.; Lin, C.; Zhao, Z.; Peer, P.; Shen, C.","Shi, Zhengli (60206614100); Lin, Chenhao (57221245073); Zhao, Zhengyu (57197811483); Peer, Peter (7003277146); Shen, Chao (36446592900)","60206614100; 57221245073; 57197811483; 7003277146; 36446592900","Evading Deepfake Detectors via Adversarially Degrading and Restoring Forged Images","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022601826&partnerID=40&md5=2ef2bc6a6b9cab6c9731e262cec19f83","Deepfake detection can prevent the misuse of deep generative techniques but is known to be vulnerable to adversarial attacks. However, most existing attacks introduce noticeable noise, resulting in an unsatisfactory trade-off between attack effectiveness and imperceptibility. In this paper, we propose a new generative attack based on adversarially Degrading and Restoring (DR) fake images, eliminating the use of noisy perturbations. Specifically, degradation works by removing high-frequency deepfake artifacts with the guidance of adversarial loss from the detector, and the subsequent restoration aims to maintain the image quality by restoring high-frequency details of natural images. Our analysis confirms that combining degradation and restoration effectively aligns the distribution of adversarial (fake) images and real images in both frequency and pixel domains. Our experimental results across eight popular detectors and three popular deep-fake datasets prove the effectiveness of our method compared with several state-of-the-art methods. Our code is available at https://github.com/fanoflck/DR-attack. © 2025 IEEE.","Adversarial attack; Deepfake detection; Image degradation and restoration"
"Chen, T.; Hu, J.; Yang, S.","Chen, Tiewen (58897683400); Hu, Jing (57192187866); Yang, Shanming (60198383900)","58897683400; 57192187866; 60198383900","Deepfake detection based on Spatio-Temporal Fusion","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022067609&partnerID=40&md5=e2b3a1d8d3b62b722a628fa1b6c6e136","To address the limitations of existing detection algorithms that utilize temporal information but are weak in extracting local information of forged traces, this paper introduces a detection algorithm for forged faces which based on spatiotemporal feature fusion. The algorithm comprises a spatial-frequency domain hybrid feature enhancement module and a frequency-aware dynamic gating unit module. Through the frequency domain, the high-frequency components of forged traces are enhanced to capture the global spatial and local frequency-domain cues. Subsequently, the frequency-aware dynamic gating unit is employed to amplify subtle cues in the temporal dimension that are difficult to discern. These cues from both dimensions are then integrated through an information fusion module to obtain a more comprehensive feature representation. Experimental results demonstrate that the proposed algorithm exhibits superior generalization performance on datasets in the forged domain. © 2025 IEEE.","deepfake detection; Feature Enhancement; Spatiotemporal Feature Fusion"
"He, Z.; Guo, Z.; Wang, L.; Yang, G.; Diao, Y.; Ma, D.","He, Ziyuan (59925900300); Guo, Zhiqing (57219672095); Wang, Liejun (16833826600); Yang, Gaobo (8647279200); Diao, Yunfeng (57222760580); Ma, Dan (59925783500)","59925900300; 57219672095; 16833826600; 8647279200; 57222760580; 59925783500","WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020986670&partnerID=40&md5=f1e1fa362bf8d11229c7e006907184a4","Deepfake technology has great potential in the field of media and entertainment, but it also brings serious risks, including privacy disclosure and identity fraud. To counter these threats, proactive forensic methods have become a research hotspot by embedding invisible watermark signals to build active protection schemes. However, existing methods are vulnerable to watermark destruction under malicious distortions, which leads to insufficient robustness. Moreover, embedding strong signals may degrade image quality, making it challenging to balance robustness and imperceptibility. Although watermarked images look natural, their underlying structures are often different from the original images, which is ignored by traditional watermarking methods. To address these issues, this paper proposes a proactive watermarking framework called WaveGuard, which explores frequency domain embedding and graph-based structural consistency optimization. In this framework, the watermark is embedded into the high-frequency sub-bands by dual-tree complex wavelet transform (DT-CWT) to enhance the robustness against distortions and deepfake forgeries. By leveraging joint sub-band correlations and selected sub-band combinations, the framework enables robust source tracing and semi-robust deepfake detection. To enhance imperceptibility, we propose a Structural Consistency Graph Neural Network (SC-GNN) that constructs graph representations of the original and watermarked images to ensure structural consistency and reduce perceptual artifacts. Experimental results show that the proposed method performs exceptionally well in face swap and face replay tasks. © 1991-2012 IEEE.","Deepfake Detection; Frequency-domain Embedding; Graph Neural Network (GNN); Source Tracing"
"Erukude, S.T.; Marella, V.C.; Veluru, S.R.","Erukude, Sai Teja (59487510000); Marella, Viswa Chaitanya (60110797200); Veluru, Suhasnadh Reddy (60110890900)","59487510000; 60110797200; 60110890900","Fourier-Based GAN Fingerprint Detection Using ResNet50","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020968105&partnerID=40&md5=5a955da0981bfbd0c968cab3fd7a921a","The rapid rise of photorealistic images produced from Generative Adversarial Networks (GANs) poses a serious challenge for image forensics and industrial systems requiring reliable content authenticity. This paper uses frequency-domain analysis combined with deep learning to solve the problem of distinguishing StyleGAN-generated images from real ones. Specifically, a two-dimensional Discrete Fourier Transform (2D DFT) was applied to transform images into the Fourier domain, where subtle periodic artifacts become detectable. A ResNet50 neural network is trained on these transformed images to differentiate between real and synthetic ones. The experiments demonstrate that the frequency-domain model achieves a 92.8 percent and an AUC of 0.95, significantly outperforming the equivalent model trained on raw spatial-domain images. These results indicate that the GAN-generated images have unique frequency-domain signatures or 'fingerprints'. The method proposed highlights the industrial potential of combining signal processing techniques and deep learning to enhance digital forensics and strengthen the trustworthiness of industrial AI systems. © 2025 IEEE.","Deepfake Detection; Frequency-Domain Analysis; Image Forensics; StyleGAN Fingerprints; Trustworthy AI Systems"
"Song, X.; Wu, C.; Liu, Z.","Song, Xiaohong (59529053200); Wu, Chuankun (7501669847); Liu, Zhuo (59529495300)","59529053200; 7501669847; 59529495300","SpectralFusionNet: Adaptive Frequency-Spatial Feature Fusion for Robust Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018047395&partnerID=40&md5=6cf57967a053f11e8493a60bc1fdb6bd","With the rapid advancement of Generative Adversarial Networks (GANs), deepfake technology poses significant threats to social trust and information security, while current detection methods suffer from limited generalization ability, single feature dependency, and insufficient environmental robustness. To address these challenges, this paper proposes SpectralFusion-Net (SFN), a novel deepfake detection framework that adaptively fuses frequency domain and spatial domain. SFN introduces four key innovations: (1) An end-to-end fusion architecture enabling dynamic interaction between frequency-spatial domain features, (2) a frequency domain dynamic selection network that adaptively evaluates the importance of different frequency components, (3) a dual-channel attention fusion strategy for effective integration of complex frequency spectrum components, and (4) an adaptive frequency domain mask generation mechanism for flexible adaptation to different forgery types. Extensive experiments on DFFD and FaceForensics++ datasets demonstrate that SFN achieves superior performance with 96.05% average accuracy on FF++ and maintains 90.46% accuracy under heavy compression scenarios (c40), significantly outperforming existing methods in detection accuracy, generalization ability, and environmental robustness, particularly when facing unknown forgery types and low-quality images. © 2025 IEEE.","Attention mechanism; Deepfake detection; Feature fusion; Frequency domain analysis; Generative adversarial networks"
"Dutta, A.; Kumar das, A.K.; Naskar, R.; Chakra, R.S.","Dutta, Anurag (57820449400); Kumar das, Arnab (58264994300); Naskar, Ruchira (53866980700); Chakra, Rajat Subhra (60126927000)","57820449400; 58264994300; 53866980700; 60126927000","WaveDIF: Wavelet Sub-Band based Deepfake Identification in Frequency Domain","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017846289&partnerID=40&md5=25e3a49c32a6741b8757607c50b2db66","With the more realistic convergence of Deepfakes, its' identification becomes more demanding. Recently, numerous deepfake detection techniques have been proposed, most of which are in the spatio-temporal domain. While these methods have shown promise, many of them neglect convincing artifacts that exhibit different patterns across frequency domains. This research proposes WaveDIF, a strict frequency domain, lightweight deepfake video detection algorithm using wavelet sub-band energies. In WaveDIF, for feature extraction, each video undergoes a Discrete Fourier Transform to filter out high-frequency noisy details (quite evident in deepfakes). These representations are then decomposed into their respective wavelet sub-bands -LL (Low-Low), LH (Low-High), HL (High-Low), and HH (High-High) passing them through a Haar Filter, following which the energy values (particular to each sub-band) are computed. These energy values are then used to learn a linear decision boundary (using regression analysis), which is then used for classification. This enables an interpretable, lightweight deterministic technique for the detection of synthesized videos, besides achieving an accuracy comparable to the state-of-the-art. Experimental results on popular deepfake video datasets shows over 92% accuracy for in-dataset evaluation, and 88% accuracy for cross dataset evaluation. © 2025 IEEE.","discrete fourier transform; discrete wavelet transform; frequency domain analysis; lightweight deepfake detection; wavelet sub-band energies"
"Liu, X.; Xiao, W.; Lin, X.; He, S.; Huang, C.; Guo, D.","Liu, Xianming (60118108300); Xiao, Wenrun (60117986400); Lin, Xiaofeng (59680665300); He, Shan (60117511700); Huang, Chao (58417295100); Guo, Donghui (60117986500)","60118108300; 60117986400; 59680665300; 60117511700; 58417295100; 60117986500","Deepfake Detection via Spatial-Frequency Attention Network","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017314242&partnerID=40&md5=c37b3444363187c47cd60d674dec515e","Artificial Intelligence Generated Content (AIGC) makes creating realistic synthetic media a breeze. Facial forgery technologies threaten consumer electronics, such as smartphones, intelligent payment systems, and smart home Internet of Things (IoT) devices. These threats undermine identity authentication security and compromise user privacy. To address this problem, this paper introduces a wavelet-based Spatial and Frequency Domain Attention Network (SFANet) for deepfake detection. The SFANet comprises a wavelet transform-based feature decoupling module, a dual attention module, and a cross-modality fusion module. The spatial features are first decomposed into high and low-frequency components by wavelet transform. Subsequently, a dual attention mechanism is built to adjust frequency-domain weights dynamically. Finally, a cross-modality fusion module is introduced to integrate spatial and frequency-domain features for enhanced detection. Experimental results on three benchmark datasets, namely FaceForensics++, WildDeepfake, and Celeb-DF V2, prove that SFANet achieves superior detection performance compared to state-of-the-art models. This research provides a powerful tool to combat AI-driven misinformation and advances the field of synthetic content detection in consumer electronics. The implementation of SFANet will be publicly available at. © 1975-2011 IEEE.","Attention Mechanism; Cross-Domain Fusion; Deepfake Detection; Frequency Domain; Wavelet Transform"
"Li, Z.; Tang, W.; Gao, S.; Wang, Y.; Wang, S.","Li, Zifeng (59319982600); Tang, Wenzhong (36024278400); Gao, Shijun (59319890500); Wang, Yanyang (36127687700); Wang, Shuai (53983404200)","59319982600; 36024278400; 59319890500; 36127687700; 53983404200","Representation Alignment For Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015865303&partnerID=40&md5=cccf2e3d3324bd892fac8a24fe2d1389","DeepFake detection faces growing challenges with the rapid advancement of generative models, enabling the creation of increasingly sophisticated forgeries. Existing methods often rely on heuristic features from spatial or frequency domains without effectively integrating them into backbone architectures to capture general forgery representations. To address this, we propose the Representation Alignment (RA) technique to enhance backbone design by incorporating high-quality external semantic features. By aligning intermediate representations, RA enables the detector to capture robust spatial attributes and discriminative frequency features, improving its ability to generalize across diverse forgery types. The proposed RA-enhanced Xception detector demonstrates superior performance in both within-domain and cross-domain evaluations. These results validate the effectiveness of RA in enhancing generalization and robustness, offering a lightweight and efficient approach for advancing DeepFake detection. ©2025 IEEE.","DeepFake Detection; Forgery generalization; Representation Alignment; Xception detector"
"Sharma, V.K.; Rawat, S.","Sharma, Vishal Kumar (57199918726); Rawat, Seema (56521132600)","57199918726; 56521132600","Enhancing Deepfake Detection Through Dynamics of Facial Expressions","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013073675&partnerID=40&md5=30de418a8e7de90a2320f9a0c5812b6a","The rapid evolution of deepfake technology poses significant threats to digital security, media integrity, and public trust, necessitating the development of robust detection frame-works. Traditional deepfake detection methods primarily rely on pixel inconsistencies, frequency-domain analysis, or handcrafted features, but these approaches are increasingly vulnerable to advanced generative models that produce high-fidelity manipulations. In this study, we introduce MADDM, a Masked Autoencoder-based deepfake detection model that leverages facial expression dynamics to identify inconsistencies in muscle coordination - an aspect that remains challenging for deepfake generators to replicate accurately. Our model is trained in a self-supervised manner, first learning natural facial expressions from real datasets and then detecting anomalies in synthetic videos by reconstructing masked facial regions. Evaluations on Celeb-DF, DFDC, and FaceForensics++ datasets demonstrate that MADDM significantly outperforms existing detection methods, achieving an average accuracy of 81.1%, with state-of-the-art performance on Celeb-DF (86.3%). Further analysis through intra-dataset and cross-dataset testing confirms the model's superior generalization capabilities. The results highlight the potential of expression-based deepfake detection as a powerful and scalable solution for digital forensics and misinformation control. Future research should explore real-time implementation, transformer-based optimizations, and adversarial training strategies to enhance detection efficiency against evolving deepfake techniques. © 2025 IEEE.","convolutional neural network; deepfake detection; facial expressions analysis; masked auto encoders; spatio temporal"
"Maan, H.; Hooda, H.; Verma, H.; Kaushal, A.","Maan, Himanshu (60022961700); Hooda, Hemang (60023530100); Verma, Harshit (60023244200); Kaushal, Anukriti (58178665700)","60022961700; 60023530100; 60023244200; 58178665700","FALCON: Frequency-Aware Lightweight Convolutional Optimized Network","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012162760&partnerID=40&md5=8e26070b87d6027c98a51394779fd261","The rise of deepfakes-images or videos that are manipulated using AI techniques like GANs-has created serious challenges in the areas of media trust, security, and online safety. Many existing methods for detecting deepfakes depend on visible image features or large, complex models like transformers. These models often struggle to detect newer types of deepfakes and require high computational power, which makes them hard to use on mobile or low-power devices. To address these issues, we propose FALCON (Frequency-Aware Lightweight Convolutional Optimized Network), a simple and efficient deep learning model for deepfake detection. The main goal of FALCON is to improve accuracy while keeping the model small and fast enough to run on real-time systems and low-resource devices. FALCON works in the frequency domain, where deepfakes often leave hidden traces. It uses three key techniques: High-Frequency Residual Injection (HFRI) to boost fake signals early in the network, Frequency Convolutional Layers (FCL) to analyze amplitude and phase patterns, and High-Frequency Refined Features (HFRF) to improve feature detection in deeper layers. We trained the model using techniques like mixed-precision, gradient clipping, and early stopping for better performance and stability. On the Kaggle 'Deepfake and Real Images' dataset, FALCON achieved 90% accuracy, 0.90 F1-score, and 0.97 AUC, outperforming other lightweight models. FALCON is ideal for real-time, reliable deepfake detection on everyday devices. This study highlights the effectiveness of frequency-domain learning as a practical and generalizable approach for deepfake detection. © 2025 IEEE.","Convolutional Neural Networks; Deep Learning; Deepfake Detection; Frequency Domain; High-Frequency Features"
"Liu, C.; Zhang, G.; Guo, S.; Li, Q.; Jeon, G.; Gao, M.","Liu, Changcun (59985636800); Zhang, Guisheng (57865198700); Guo, Siyou (59173914500); Li, Qilei (57202858223); Jeon, Gwanggil (15022497800); Gao, Mingliang (26634962800)","59985636800; 57865198700; 59173914500; 57202858223; 15022497800; 26634962800","Context-Aware Deepfake Detection for Securing AI-Driven Financial Transactions","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010339808&partnerID=40&md5=eb51e693f6e06609779b9c5c6398cf92","The rapid advancement of deepfake technology has threatened the community’s sense of security, particularly in the context of face-based payment systems. Thus, deepfake detection has emerged as a critical issue demanding immediate attention. However, the generalization performance of existing detection models is limited as they are overly reliant on specific forged features while ignoring the common forged features. To address this problem, we introduce the context-aware decoupling network (CADNet) for deepfake detection. Specifically, a context self-calibration (CSC) module is constructed to guide the network to focus on local forged regions. It enlarges possible regions to increase the likelihood of forgery cues. Meanwhile, a frequency domain decoupling (FDD) module is introduced to extract and fuse different frequency components. It realizes the collaborative representation optimization of global semantics and local details. The experimental results prove that the proposed model exhibits strong generalization capability across multiple standard datasets. It achieves average area under the curve (AUC) values of 98.64% for in-domain evaluation and 75.52% for cross-dataset generalization. © 2014 IEEE.","Context self-calibration (CSC); deepfake detection; facial payment; frequency domain decoupling (FDD); generalization"
"Shi, Z.; Chen, H.; Jia, Y.; Zhang, D.; Lu, W.; Yang, X.","Shi, Zenan (57188923911); Chen, Haipeng (35753222600); Jia, Yixin (59920788300); Zhang, Dong (57201567643); Lu, Wei (57715097700); Yang, Xun (56537180500)","57188923911; 35753222600; 59920788300; 57201567643; 57715097700; 56537180500","Customized Transformer Adapter With Frequency Masking for Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006898654&partnerID=40&md5=1e430ca137a569c37df2215e0fd87345","The rapid advancement of AI-generated content has intensified concerns over deepfakes due to increasingly sophisticated and visually convincing forgeries. To this end, the pre-trained Vision Transformer (ViT) model has become a de facto choice for deepfake detection, thanks to its powerful learning capability. Despite favorable results achieved by existing ViT-based methods, they have inherent limitations that could result in suboptimal performance in scenarios with continuously evolving forgery techniques, such as overfitting to single forgery patterns or placing excessive emphasis on dominant forgery regions. In this paper, we propose CUTA, a simple yet effective deepfake detection paradigm that utilizes ViT adapters as the medium and fully exploits the spatial- and frequency-domain features of given images to overcome the limitations of existing methods. Specifically, CUTA focuses on frequency domain masking within the input space, which obscures parts of the high-frequency image to intensify the training challenge while preserving subtle forgery cues in the frequency domain to facilitate comprehensive forgery representations. Furthermore, we propose two task-customized modules within the ViT model, i.e., the texture enhancement module and the multi-scale perceptron module, to seamlessly integrate local texture and rich contextual features. These two modules ensure an organic interaction between the task-specific forgery patterns and general semantic features within the pre-trained ViT framework. The experimental results on several publicly available benchmarks demonstrate CUTA’s superiority in performance, particularly showcasing its significant advantages in both cross-dataset and cross-manipulation scenarios. © 2005-2012 IEEE.","Deepfake detection; frequency domain masking; vision transformer; ViT adapter"
"Gu, S.; Qin, Z.; Xie, L.; Wang, Z.; Hu, Y.","Gu, Siqi (59156426900); Qin, Zihan (59462880900); Xie, Lizhe (36471460700); Wang, Zheng (57844595900); Hu, Yining (55768942900)","59156426900; 59462880900; 36471460700; 57844595900; 55768942900","Multiscale Features Integrated Model for Generalizable Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001581289&partnerID=40&md5=e7e38bf59b4fa206bb59ef76af207ee5","Within the domain of Artificial Intelligence Generated Content (AIGC), technological strides in image generation have been marked, resulting in the proliferation of deepfake images that pose substantial security threats. The current landscape of deepfake detection technologies is marred by limited generalization across diverse generative models and a subpar detection rate for images generated through diffusion processes. In response to these challenges, this paper introduces a novel detection model designed for high generalizability, leveraging multiscale frequency and spatial domain features. Our model harnesses an array of specialized filters to extract frequency-domain characteristics, which are then integrated with spatial-domain features captured by a Feature Pyramid Network (FPN). The integration of the Attentional Feature Fusion (AFF) mechanism within the feature fusion module allows for the optimal utilization of the extracted features, thereby enhancing detection capabilities. We curated an extensive dataset encompassing deepfake images from a variety of GANs and diffusion models for rigorous evaluation. The experimental findings reveal that our proposed model achieves superior accuracy and generalization compared to existing baseline models when confronted with deepfake images from multiple generative sources. Notably, in cross-model detection scenarios, our model outperforms the next best model by a significant margin of 29.1% for diffusion-generated images and 15.1% for GAN-generated images. This accomplishment presents a viable solution to the pressing issues of generalization and adaptability in the field of deepfake detection. © © 2025 Siqi Gu et al. International Journal of Intelligent Systems published by John Wiley & Sons Ltd.","Artificial Intelligence Generated Content; deepfake detection; diffusion model; generative adversarial networks"
"Zhang, C.; He, X.; Shang, Z.","Zhang, Chunyue (59910215100); He, Xiping (26652968200); Shang, Zihan (59910056100)","59910215100; 26652968200; 59910056100","A method for detecting deepfake faces by integrating multi-scale deep-shallow spatial and frequency features","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005834456&partnerID=40&md5=99c7251961b6740a01e249328effa50c","In recent years, deep learning technology has been excessively used in the creation of fake videos. Deepfake technology alters or replaces the facial information of the original video, synthesizes false voices, and is used to create pornographic films, fake news, political rumors, and so on, posing significant challenges to the authenticity of information and personal privacy. Currently, most face deepfake detection networks are based on single-scale and single-source detection research, and the proposed networks and methods often lack the ability to recognize unknown attack methods and generalization ability. To address these issues, this paper proposes a face deepfake detection method that integrates deep and shallow multi-scale spatial and frequency-domain features to achieve high-accuracy recognition of deepfake faces. The network is based on feature extraction from shallow and deep layers, where the shallow branch captures detailed information from low-level features to detect small-scale targets such as minor noise, while the deep branch focuses on detecting larger and higher-level fake features with a larger receptive field. Each branch adopts a method to fuse multi-scale spatial and frequency-domain features. Multi-scale features can capture information at different levels and granularities, while frequency-domain features can supplement fake traces that are difficult to detect in the spatial domain. Extensive experiments show that tthis framework outperforms most face spoofing detection networks and achieves good performance. © 2024 Copyright held by the owner/author(s).","Face deepfake detection, Multi-scale features Frequency domain features Feature fusion"
"Ding, X.; Pang, S.; Guo, W.","Ding, Xinmiao (36730815200); Pang, Shuai (57979141800); Guo, Wen (36610134100)","36730815200; 57979141800; 36610134100","Noise-aware progressive multi-scale deepfake detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188178757&partnerID=40&md5=943ad34dd5b6ef9841c186aca229b51f","The proliferation of fake images generated by deepfake techniques has significantly threatened the trustworthiness of digital information, leading to a pressing need for face forgery detection. However, due to the similarity between human face images and the subtlety of artefact information, most deep face forgery detection methods face certain challenges, such as incomplete extraction of artefact information, limited performance in detecting low-quality forgeries, and insufficient generalization across different datasets. To address these issues, this paper proposes a novel noise-aware multi-scale deepfake detection model. Firstly, a progressive spatial attention module is introduced, which learns two types of spatial feature weights: boosting weight and suppression weight. The boosting weight highlights salient regions, while the suppression weight enables the model to capture more subtle artifact information. Through multiple boosting-suppression stages, the proposed model progressively focuses on different facial regions and extracts multi-scale RGB features. Additionally, a noise-aware two-stream network is introduced, which leverages frequency-domain features and fuses image noise with multi-scale RGB features. This integration enhances the model’s ability to handle image post-processing. Furthermore, the model learns global features from multi-modal features through multiple convolutional layers, which are combined with local similarity features for deepfake detection, thereby improving the model’s robustness. Experimental results on several benchmark databases demonstrate the superiority of our proposed method over state-of-the-art techniques. Our contributions lie in the progressive spatial attention module, which effectively addresses overfitting in CNNs, and the integration of noise-aware features and multi-scale RGB features. These innovations lead to enhanced accuracy and generalization performance in face forgery detection. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","Deepfake detection; Noise-awareness; Spatial attention; Two-stream network"
"Cheng, Z.; Wang, Y.; Wan, Y.; Jiang, C.","Cheng, Ziyuan (59310972800); Wang, Yiyang (59248725100); Wan, Yongjing (8334613000); Jiang, Cuiling (35324356900)","59310972800; 59248725100; 8334613000; 35324356900","DeepFake detection method based on multi-scale interactive dual-stream network","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202967018&partnerID=40&md5=ec2cb4bccd1dd6c6c10058dfd1b84789","DeepFake face forgery has a serious negative impact on both society and individuals. Therefore, research on DeepFake detection technologies is necessary. At present, DeepFake detection technology based on deep learning has achieved acceptable results on high-quality datasets; however, its detection performance on low-quality datasets and cross-datasets remains poor. To address this problem, this paper presents a multi-scale interactive dual-stream network (MSIDSnet). The network is divided into spatial- and frequency-domain streams and uses a multi-scale fusion module to capture both the facial features of images that have been manipulated in the spatial domain under different circumstances and the fine-grained high-frequency noise information of forged images. The network fully integrates the features of the spatial- and frequency-domain streams through an interactive dual-stream module and uses vision transformer (ViT) to further learn the global information of the forged facial features for classification. Experimental results confirm that the accuracy of this method reached 99.30 % on the high-quality dataset Celeb-DF-v2, and 95.51 % on the low-quality dataset FaceForensics++. Moreover, the results of the cross-dataset experiments were superior to those of the other comparison methods. © 2024 Elsevier Inc.","DeepFake detection; High-frequency noise; Interactive dual-stream; Multi-scale fusion"
"Wang, Y.; Sun, Q.; Rong, D.; Geng, R.","Wang, Yan (56374818700); Sun, Qindong (7402036692); Rong, Dongzhu (57273628000); Geng, Rong (58891256600)","56374818700; 7402036692; 57273628000; 58891256600","Multi-domain awareness for compressed deepfake videos detection over social networks guided by common mechanisms between artifacts","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199566566&partnerID=40&md5=5ae63f00d8010c47b08ab57172786b55","The viral spread of massive deepfake videos over social networks has caused serious security problems. Despite the remarkable advancements achieved by existing deepfake detection algorithms, deepfake videos over social networks are inevitably influenced by compression factors. This causes deepfake detection performance to be limited by the following challenging issues: (a) interfering with compression artifacts, (b) loss of feature information, and (c) aliasing of feature distributions. In this paper, we analyze the common mechanism between compression artifacts and deepfake artifacts, revealing the structural similarity between them and providing a reliable theoretical basis for enhancing the robustness of deepfake detection models against compression. Firstly, based on the common mechanism between artifacts, we design a frequency domain adaptive notch filter to eliminate the interference of compression artifacts on specific frequency bands. Secondly, to reduce the sensitivity of deepfake detection models to unknown noise, we propose a spatial residual denoising strategy. Thirdly, to exploit the intrinsic correlation between feature vectors in the frequency domain branch and the spatial domain branch, we enhance deepfake features using an attention-based feature fusion method. Finally, we adopt a multi-task decision approach to enhance the discriminative power of the latent space representation of deepfakes, achieving deepfake detection with robustness against compression. Extensive experiments show that compared with the baseline methods, the detection performance of the proposed algorithm on compressed deepfake videos has been significantly improved. In particular, our model is resistant to various types of noise disturbances and can be easily combined with baseline detection models to improve their robustness. © 2024 Elsevier Inc.","Common mechanism between artifacts; Compressed deepfake video; Deepfake detection; Multi-domain awareness; Multimedia forensics"
"Wang, Y.; Sun, Q.; Rong, D.; Wang, X.","Wang, Yan (56374818700); Sun, Qindong (7402036692); Rong, Dongzhu (57273628000); Wang, Xiaoxiong (58891708300)","56374818700; 7402036692; 57273628000; 58891708300","Deepfake Video Detection on Social Networks Using Multi-domain Aware Driven by Common Mechanism Analysis Between Artifacts; 伪影间共性机理驱动的多域感知社交网络深度伪造视频检测","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206082197&partnerID=40&md5=c34d52e6c743e5d8a0bacab68f5397c1","The misuse of deepfake technology on social networks has raised serious concerns about the authenticity and reliability of visual content. The degradation phenomenon of deepfake videos on social networks has not been adequately considered in existing detection algorithms, resulting in deepfake detection performance being limited by challenging issues such as compression artifacts interference and lack of context-related information. Compression encoding and up-sampling operations in deepfake generation algorithms can leave artifacts on videos, which can result in fine-grained differences between real videos and deepfake videos. The common mechanisms between compression artifacts and deepfake artifacts are analyzed to reveal the structural similarities between them, which provides a reliable theoretical basis for enhancing the robustness of deepfake detection models against compression. Firstly, to address the interference of compression noise on deepfake features, the frequency-domain adaptive notch filter is designed based on the structural similarity of compression artifacts and deepfake artifacts to eliminate the interference of compression artifacts on specific frequency bands. Secondly, the denoising branch based on residual learning is designed to reduce the sensitivity of the deepfake detection model to unknown noise. Additionally, the attention-based feature fusion method is adopted to enhance the discriminative features of deepfakes. Metric learning strategies are adopted to optimize network models, achieving deepfake detection with resistance to compression. Theoretical analysis and experimental results indicate that the detection performance of compressed deepfake videos is significantly enhanced by using the algorithm proposed in this paper. It can be used as a plug-and-play model combined with existing detection methods to enhance their robustness against compression. © 2024 Science Press. All rights reserved.","Common mechanism between artifacts; Compressed deepfake; Deepfake detection; Social network"
"Wang, B.; Wu, X.; Wang, F.; Zhang, Y.; Wei, F.; Song, Z.","Wang, Bo (57216234699); Wu, Xiaohan (57460009800); Wang, Fei (58366999700); Zhang, Yushu (55508709300); Wei, Fei (57202381047); Song, Zengren (57367945900)","57216234699; 57460009800; 58366999700; 55508709300; 57202381047; 57367945900","Spatial-frequency feature fusion based deepfake detection through knowledge distillation","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191316614&partnerID=40&md5=3daf62938c06899da6033ce2c194097e","While the misuse of Deepfake technology is drawing growing concern in the literature of information security, related forgery detection has become a significant challenge in practical applications. Most state-of-the-art detection methods achieve satisfactory results on raw images, but their performance drops significantly on processed images (e.g. compression). In this work, we propose a novel Deepfake detection method that integrates spatial and frequency domain information within a knowledge distillation framework for efficient forgery detection. Our method consists of two steps: (1) spatial-frequency fusion, and (2) multi-knowledge distillation. We first extract frequency-domain and spatial-domain features, then fuse them and utilize them in attention-based guidance to improve the classification results. Note that the spatial-frequency fusion serves as the basis for both the teacher and student models with spatial-frequency features and logits transferred as knowledge. We conducted comprehensive experiments on several benchmark datasets which successfully demonstrate the excellent generalization performance of our method on compressed images while outperforming state-of-the-art techniques. © 2024 Elsevier Ltd","Deepfake detection; Feature fusion; Frequency domain; Knowledge distillation"
"Jin, X.; Wu, N.; Jiang, Q.; Kou, Y.; Duan, H.; Wang, P.; Yao, S.","Jin, Xin (56991832300); Wu, Nan (57750887800); Jiang, Qian (57194699462); Kou, Yuru (58988680500); Duan, Hanxian (58988964800); Wang, Puming (57188640245); Yao, Shaowen (24473851600)","56991832300; 57750887800; 57194699462; 58988680500; 58988964800; 57188640245; 24473851600","A dual descriptor combined with frequency domain reconstruction learning for face forgery detection in deepfake videos","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190520119&partnerID=40&md5=15bb77bbd3876d97b34223df6e19fd6d","Conventional face forgery detectors have primarily relied on image artifacts produced by deepfake video generation models. These methods have performed well when the training and test sets were derived from the same deepfake algorithm, but accuracy and generalizability remain a challenge for diverse datasets. In this study, both supervised and unsupervised approaches are proposed for more accurate detection in in-domain and cross-domain experiments. Specifically, two descriptors are introduced to extract rich information in the spatial domain to achieve higher accuracy. A frequency domain reconstruction module is then included to expand the representation space for facial features. A reconstruction method based on an auto-encoder was also applied to obtain a frequency domain coding vector. In this process, reconstruction learning was sufficient for extracting unknown information, while a combination with classification learning provided essential high-frequency pixel differences between real and fake samples, thus facilitating forgery identification. A series of validation experiments with large-scale benchmark datasets demonstrated that the proposed technique was superior to existing methods. © 2024 Elsevier Ltd","Auto encoder; Deep learning; Deepfake detection; Digital forensics; Frequency domain analysis; Video forgery"
"Amin, M.A.; Hu, Y.; Li, C.-T.; Liu, B.","Amin, Muhammad Ahmad (58476434200); Hu, Yongjian (35766130600); Li, Chang Tsun (26648782200); Liu, Beibei (55544736500)","58476434200; 35766130600; 26648782200; 55544736500","Deepfake detection based on cross-domain local characteristic analysis with multi-domain transformer","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186126407&partnerID=40&md5=82cf393194d0f7a24c8f6f7ad903c390","Deepfake videos present a significant challenge in the current media landscape. While current deepfake detection methods demonstrate satisfactory performance, there is still room for improvement in their ability to generalize and detect unseen scenarios, particularly those involving imperceptible cues. This paper introduces a novel multi-modal deepfake detection model named SpectraVisionFusion Transformer (SVFT), which incorporates spatial and frequency domain statistical artifacts to improve generalization performance. The SVFT framework uses two different backbone encoder models to take advantage of both spatial and frequency domain cues in video sequences, along with a decoder and classifier, for common cross-attention and classification, respectively. The spatial domain branch uses a convolutional transformer-based encoder to analyze facial visual features. In contrast, the frequency domain branch employs a language transformer encoder. Additionally, we introduce a weighted feature embedding fusion mechanism that integrates spectral-based statistical feature embeddings and visual cues to achieve a more comprehensive and balanced spatial-frequency feature representation. By coordinately analyzing these modalities, our model exhibits improved detection and generalization capabilities in unseen scenarios. Our proposed SVFT model achieved 92.57% and 80.63% accuracy in extensive cross-manipulation and dataset evaluation, respectively, while surpassing the performance of traditional and single-domain-based approaches. © 2024 The Author(s)","Deepfake detection; Generalization performance; Multi-domain transformer; Spatial-frequency domains; Spectral anomalies"
"Xu, P.; Ma, Z.; Mei, X.; Shen, J.","Xu, Pengxiang (57222580968); Ma, Zhiyuan (58079714800); Mei, Xue (23091706700); Shen, Jie (57196190343)","57222580968; 58079714800; 23091706700; 57196190343","Detecting facial manipulated images via one-class domain generalization","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182641098&partnerID=40&md5=2fa8d6f79f2503bac583ff03ee2cd675","Nowadays, numerous synthesized images and videos generated by facial manipulated techniques have become an emerging problem, which promotes facial manipulation detection to be a significant topic. Much concern about the use of synthesized facial digital contents in society is rising due to their deceptive nature and widespread. To detect such manipulated facial digital contents, many methods have been proposed. Most detection methods focus on specific datasets. It is hard for them to detect facial images or videos manipulated by unknown face synthesis algorithms. In this paper, we propose a method to improve the generalization ability of the facial manipulation detection model using one-class domain generalization. We shape the problem into domain generalization. We divide the dataset into several domains according to different manipulation algorithms. We also try to process the images from the perspective of frequency domain. We utilize two-dimensional wavelet transform to preprocess the images to ensure the effect on compressed images. The results of experiments implemented on FaceForensics++ dataset exceed the baselines and recent works. The feature visualization analyses intuitively show that our method can learn robust feature representation that can be generalized to unseen domains. © 2024, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Adversarial training; DeepFake detection; Domain generalization; Facial manipulation detection"
"Pradeepan, P.; Raj, S.","Pradeepan, P. (59550572600); Raj, S. Gladston (57203177702)","59550572600; 57203177702","Detection of Deepfake Medical Images Based on Spatial and Frequency Domain Analysis","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218073087&partnerID=40&md5=6488c1dad21c4262dc28bc911ee45db1","The advent of deepfake technologies poses a significant threat to reliance on medical imaging. This can lead to misdiagnosis and threaten patient safety. To address this issue, this study introduces a novel deep-learning approach that integrates spatial and frequency-domain analyses to detect manipulated medical images. The proposed approach combines the pre-trained EfficientNet-B0 model for spatial feature extraction with the Discrete Wavelet Transform (DWT) for frequency-domain analysis. The model was validated using the CT-GAN dataset comprising real and GAN-generated fake CT scan images. Individually, the EfficientNet-B0 model achieved an accuracy of 97.03%, and the DWT-based analysis reached 97.84%. Combining both approaches, the hybrid model achieved an overall accuracy of 99.6%. Data augmentation and weighted cross-entropy loss were employed to enhance the training process and address class imbalance. Performance was assessed using standard metrics, including accuracy, precision, recall, F1-score, Receiver Operating Characteristic (ROC), and precision-recall curves. These results demonstrate the strengths of our model in detecting minor variations in an image and validating the power of combining frequency analysis with spatial evaluation. This method holds great promise for protecting the integrity of medical imaging against deepfake threats. © 2024 IEEE.","Biomedical imaging; Computational modeling; Deep learning; Medical Deepfake Detection"
"Seth, N.; Patel, N.; Jindal, R.","Seth, Nilesh (59410123100); Patel, Nitin (59411251500); Jindal, Rajni M. (8698929800)","59410123100; 59411251500; 8698929800","Enhancing Resilience of Deepfake Detection using Adversarial Frequency Masking","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209107603&partnerID=40&md5=837d6cee5dfb301a7b1c550519733b47","This research presents an advanced approach to deepfake detection that combines frequency domain masking with adversarial training techniques to enhance the precision and resilience of the current models against adversarial attacks. The approach consists of advanced preprocessing of the images with Fourier and Wavelet transforms for feature extraction along with the creative adversarial training techniques. The approach's usefulness is demonstrated by experimental findings, especially in identifying difficult and realistic looking deepfakes. The study presents a comparative analysis between multiple models when trained using this technique and trained without using this technique. This research can be used to counteract deepfake threats and what future research should focus on in order to improve deepfake detection capabilities. © Grenze Scientific Society, 2024.","adversarial training; deep learning; deepfake detection; Fake content; frequency masking"
"Zhang, L.; Yi, C.; Liu, L.","Zhang, Lei (57204592766); Yi, Ceyuan (59359214800); Liu, Liang (55715426000)","57204592766; 59359214800; 55715426000","BiFAT: Bilateral Filtering and Attention Mechanisms in a Two-Stream Model for Deepfake Detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205874498&partnerID=40&md5=10a7a229a939ef25fe3c7e01a076e1b5","Addressing the significant societal concerns triggered by the widespread dissemination of Deepfake facial forgeries on the internet, and the shortcomings of existing Deepfake video detection methods in terms of generalizability and resistance to compression artifacts, we introduce a model named BiFAT. BiFAT synergizes bilateral filtering and attention mechanisms within a two-stream model to transcend the limitations of traditional binary classification approaches in Deepfake detection. Firstly, we employ an attention mechanism combined with the Steganalysis Rich Filters and Attention (SRMA) for spatial feature extraction, capturing intricate local textures and structures. Secondly, discrete Fourier transform and a complex adaptive filter are applied for frequency domain feature extraction, ensuring a comprehensive analysis of the image. This dual-domain approach, augmented by attention layers, refines the feature extraction and amalgamation process, significantly enhancing detection performance on benchmark datasets such as DF-1.0, DFDC, Celeb-DF, and FaceForensics++. Finally, our method demonstrates a notable improvement in model convergence speed, addressing the challenge of managing an excessive number of features, a common issue in contemporary DeepFake detection models, thereby boosting the model’s generalizability and compression artifact resistance. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Deep Learning; DeepFake Detection; Face Forensics"
"Meng, Y.; Wang, X.; Wang, X.; Liu, Z.; Zhou, H.","Meng, Yuan (58665559500); Wang, Xiyuan (56104269700); Wang, Xueqin (59886325200); Liu, Ziliang (59302503500); Zhou, Hao (59302560900)","58665559500; 56104269700; 59886325200; 59302503500; 59302560900","Deepfake Detection Based on Multi-scale RGB-Frequency Feature Fusion","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202436489&partnerID=40&md5=b41287ff5f0be0ba69c7e3258367b0a7","Withthe rapid advancement of Deepfake technology, Deepfake content is becoming increasingly realistic and is being widely utilized in political forgery, financial fraud, and the dissemination of false news. In order to more accurately detect Deepfake images and adapt the detection model to various compression scenarios, we propose a forgery detection model based on multi-scale Rgb-Frequency domain feature extraction. This model employs different scale size feature extraction in CNN and extracts multi-scale features in the RGB domain. Subsequently, different feature sequences are input into the Transformer decoder to establish connections between modules and perform classification. The results demonstrate that MRFD exhibits strong robustness and generalization ability when adapting to changes in compression rates. On the LQ dataset of FF++, the ACC and AUC are 90.87% and 93.87%, respectively. The ACC on HQ dataset reaches 97.54% with an AUC of 99.27%. © 2024 IEEE.","CNN combined with ViT; Deepfake detection; multi-scale feature extraction; Rgb-Frequency domain feature fusion"
"Weng, J.","Weng, Jing (59247299200)","59247299200","Local frequency analysis for diffusion-generated image detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200464608&partnerID=40&md5=cad5eaaf41ae9cf24dcb8339ebe0a9c6","With the development of generative adversarial networks (GANs) and diffusion models, deepfake has become a serious security issue. This study mainly aims to detect images produced by diffusion models. For this reason, this paper introduces an innovative Local Frequency Analysis (LFA) method which based on Krawchouk moment mesoscale frequency analysis and Discrete Cosine Transform (DCT) finescale frequency analysis, which simultaneously performs the frequency domain analysis of images from multi-scale to extract more comprehensive features. In this paper, experiments are performed on two datasets, GenImage and DiffusionForensics, and the proposed method is evaluated and contrasted against the existing methods. Also, the generalization capability of LFA for detecting forged images from different sources is experimented on the GenImage dataset. The results of the experiments show that LFA performs superiorly in detecting diffusion-generated images. In addition, robustness experiments on two types of degradation, JPEG compression and Gaussian blur, are also conducted to further demonstrate the practicality of LFA. © 2024 SPIE.","Deepfake detection; diffusion-image; frequency; Krawtchouk moment"
"Zheng, W.; Zhou, F.; Ling, X.","Zheng, Wei (59247394600); Zhou, Fandi (57211124854); Ling, Xia (58663236700)","59247394600; 57211124854; 58663236700","FETNet: frequency-enhanced transformer network for face forgery detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200446956&partnerID=40&md5=787f45b16f351231eb25397cee67307f","With the development of deepfake methods, a large number of deepfake images and videos have been widely disseminated on the internet, raising public concerns about the authenticity of information. Therefore, deepfake detection has recently become a hot topic in the field of computer vision, and many methods have been proposed. Currently, frequency-based detection methods have achieved commendable results, but there are still two issues: a) These methods use fixed filters to focus on fixed frequency bands and areas, making them easily distracted by irrelevant information and lacking flexibility for different forgery methods. b) The methods that fuse frequency domain information with RGB information using CNNs do not consider global relationships, so they are insufficient to fully utilize both types of information. To address these issues, we introduce a Frequency-Enhanced Transformer Network (FETNet). Specifically, we propose a Frequency Feature Enhancement Module (FFEM), which is a learnable module capable of flexibly enhancing important frequency bands and regions in the original frequency features. Additionally, we present a Feature Fusion Transformer (FFT) that considers global information to fuse features from the RGB and frequency domains, achieving a more comprehensive feature representation. Through extensive experiments on the FF++ dataset, the effectiveness and superiority of our approach have been demonstrated. © 2024 SPIE.","convolutional neural networks; Deepfake detection; transformer"
"Sabareshwar, D.; Raghul, S.; Ravi, S.; Varalakshmi, M.; Peer Mohamed, P.U.","Sabareshwar, D. (59091564800); Raghul, Siva Venkatesh (59173430100); Ravi, Shawn (59091000700); Varalakshmi, Murugesan (58286484600); Peer Mohamed, P. U. (59091564900)","59091564800; 59173430100; 59091000700; 58286484600; 59091564900","A Lightweight CNN for Efficient Deepfake Detection of Low-resolution Images in Frequency Domain","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192564021&partnerID=40&md5=1f64c10f3ffbf3e32b63388365b6ad2b","Deepfake detection in frequency domain is extensively explored for two reasons -artifacts generated by the up-sampling layer of Generative Adversarial Networks are more prominent in the frequency domain and some image processing tasks are simpler and efficient in this domain. Traditional convolutional neural networks outperform the machine learning models for their ability to detect deepfakes of low-resolution images also. But these networks involve numerous trainable parameters resulting in huge computation cost, time and high memory requirements. This paper presents a lightweight Convolutional Neural Network (CNN) architecture that is optimized for efficient deepfake detection of low-resolution images in the frequency domain. The proposed model achieves almost the same accuracy as that of the traditional network but with the parameter requirement reduced by 92%. This renders the model suitable for use in memory-constrained and power-constrained environments such as mobile devices and other embedded systems. © 2024 IEEE.","Deepfake detection; depthwise separable convolution; frequency domain; lightweight CNN; pixel domain"
"Zhou, J.; Zhao, X.; Xu, Q.; Zhang, P.; Zhou, Z.","Zhou, Jiting (57195986446); Zhao, Xinrui (58985251300); Xu, Qian (58440887000); Zhang, Pu (58985170000); Zhou, Zhihao (58985328800)","57195986446; 58985251300; 58440887000; 58985170000; 58985328800","MDCF-Net: Multi-Scale Dual-Branch Network for Compressed Face Forgery Detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190741441&partnerID=40&md5=a0255f8bc105db4d309efd332569beaf","Face forgery detection aims to identify manipulated or altered facial images or videos created using artificial intelligence. Existing detection methods exhibit favorable performance on high-quality videos, but the videos in daily applications are commonly compressed into low-quality formats via social media. The detection difficulty is increased by the poor quality, indistinct detail features, and noises such as artifacts in these images or videos. To address this challenge, we propose a multi-scale dual-branch network for compressed face forgery, called MDCF-Net, effectively capturing cross-domain forgery features at various scales in compressed facial images. The MDCF-Net comprises two branches: an RGB domain branch utilizing Transformers to extract multi-scale fine-texture features from the original RGB images; a frequency domain branch designed to capture artifacts in low-quality videos by extracting global spectral features as a supplementary measure. Then, we introduce a feature fusion module (FFM) based on multi-head attention to merge diverse feature representations in a spatial-frequency complementary manner. Extensive comparative experiments on public datasets such as FaceForensics++, Celeb-DF, and WildDeepfake demonstrate the significant advantage of MDCF-Net in detecting highly compressed and low-quality forged images or videos, especially in achieving state-of-the-art performance on the FaceForensics++ low-quality dataset. Our approach presents a new perspective and technology for low-quality face forgery detection. © 2013 IEEE.","deepfake detection; Face forgery; feature fusion; frequency domain; transformers; two-branch"
"Huang, Y.; Juefei-Xu, F.; Guo, Q.; Liu, Y.; Pu, G.","Huang, Yihao (57214756217); Juefei-Xu, Felix (54911989900); Guo, Qing (57191163500); Liu, Yang (56911879800); Pu, Geguang (9534351100)","57214756217; 54911989900; 57191163500; 56911879800; 9534351100","Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174804409&partnerID=40&md5=a242998df0464825f77222b053b8e7a4","The current high-fidelity generation and high-precision detection of DeepFake images are at an arms race. We believe that producing DeepFakes that are highly realistic and 'detection evasive' can serve the ultimate goal of improving future generation DeepFake detection capabilities. In this paper, we propose a simple yet powerful pipeline to reduce the artifact patterns of fake images without hurting image quality by performing implicit spatial-domain notch filtering. We first demonstrate that frequency-domain notch filtering, although famously shown to be effective in removing periodic noise in the spatial domain, is infeasible for our task at hand due to the manual designs required for the notch filters. We, therefore, resort to a learning-based approach to reproduce the notch filtering effects, but solely in the spatial domain. We adopt a combination of adding overwhelming spatial noise for breaking the periodic noise pattern and deep image filtering to reconstruct the noise-free fake images, and we name our method DeepNotch. Deep image filtering provides a specialized filter for each pixel in the noisy image, producing filtered images with high fidelity compared to their DeepFake counterparts. Moreover, we also use the semantic information of the image to generate an adversarial guidance map to add noise intelligently. Our large-scale evaluation on 3 representative DeepFake detection methods (tested on 16 types of DeepFakes) has demonstrated that our technique significantly reduces the accuracy of these 3 fake image detection methods, 36.79% on average and up to 97.02% in the best case. © 1991-2012 IEEE.","DeepFake; DeepFake detection; DeepFake evasion"
"Sen, L.; Mukherjee, S.","Sen, Lord (59761169400); Mukherjee, Shyamapada (59037257500)","59761169400; 59037257500","A Novel Unified Approach to Deepfake Detection of Images","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003907847&partnerID=40&md5=18bb2c35a0784c60b338a10f47f8cb6a","The advancements in the field of AI is increasingly giving rise to various threats. One of the most prominent of them is the synthesis and misuse of Deep Fakes. To sustain trust in this digital age, detection and tagging of deepfakes is very necessary. In this paper, a novel architecture for Deepfake detection in images is presented. The architecture uses cross-attention between spatial and frequency domain features along with a blood detection module to classify an image as real or fake. This paper aims to develop a unified architecture and provide insights into each step. It is trained on two small datasets with 200 and 400 images respectively. On comparative analysis, our model was better than the other possibilities. Further, there was an increment in accuracy of 4.29% and 4.60% upon adding 200 images to the dataset. This shows that, if trained on a large dataset and hyper-parameters optimized, the performance will increase significantly. © 2024 IEEE.","Cross Attention; DeepFake detection; Fourier Transform etc"
"Wang, F.; Chen, Q.; Jing, B.; Tang, Y.; Song, Z.; Wang, B.","Wang, Fei (58366999700); Chen, Qile (59750265200); Jing, Botao (59336619000); Tang, Yeling (58120138500); Song, Zengren (57367945900); Wang, Bo (57216234699)","58366999700; 59750265200; 59336619000; 58120138500; 57367945900; 57216234699","Deepfake Detection Based on the Adaptive Fusion of Spatial-Frequency Features","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003150882&partnerID=40&md5=451678173b529f075493173354472720","Detecting deepfake media remains an ongoing challenge, particularly as forgery techniques rapidly evolve and become increasingly diverse. Existing face forgery detection models typically attempt to discriminate fake images by identifying either spatial artifacts (e.g., generative distortions and blending inconsistencies) or predominantly frequency-based artifacts (e.g., GAN fingerprints). However, a singular focus on a single type of forgery cue can lead to limited model performance. In this work, we propose a novel cross-domain approach that leverages a combination of both spatial and frequency-aware cues to enhance deepfake detection. First, we extract wavelet features using wavelet transformation and residual features using a specialized frequency domain filter. These complementary feature representations are then concatenated to obtain a composite frequency domain feature set. Furthermore, we introduce an adaptive feature fusion module that integrates the RGB color features of the image with the composite frequency domain features, resulting in a rich, multifaceted set of classification features. Extensive experiments conducted on benchmark deepfake detection datasets demonstrate the effectiveness of our method. Notably, the accuracy of our method on the challenging FF++ dataset is mostly above 98%, showcasing its strong performance in reliably identifying deepfake images across diverse forgery techniques. © © 2024 Fei Wang et al.","adaptive fusion; deepfake detection; spatial and frequency domain; wavelet transform"
"Liu, T.; Liu, Y.; Li, N.; Shi, H.","Liu, Tianju (59704205900); Liu, Yuan (59340657400); Li, Ningyi (59704751000); Shi, Hui (55448237600)","59704205900; 59340657400; 59704751000; 55448237600","Multi-domain Multi-scale DeepFake Detection for Generalization","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002319701&partnerID=40&md5=543d4af17a241a0383f2a75b5483935c","As deepfake detection technology progresses and application scenarios broaden, it becomes challenging for detection models within a single domain or single scale to handle the complex application environments, leading to limited accuracy and generalization capabilities of the detection models. To address the above problems, we propose a Multi-domain Multi-scale DeepFake Detection model. First, we employ the RetinaFace framework to preprocess and augment the image data, which directs the model's attention to the relevant regions of the image. Next, we utilize the EfficientNet-B0 network for rapid coarse-grained feature extraction. Building on these coarse-grained features, we design two modules to capture forgery traces at multiple levels: a spatial-domain fine-grained feature extraction module based on multi-scale analysis and a frequency-domain fine-grained feature extraction module leveraging the Fast Fourier Transform. Finally, the dual-domain features are fused using a multi-channel attention mechanism, enabling the model to comprehensively analyze images from various perspectives. Additionally, we implement a Particle Swarm Optimization (PSO) strategy to optimize the hyper-parameters of the network model. The proposed model demonstrates significant advantages over other algorithms, particularly in cross-dataset detection, exhibiting strong generalization, robustness, and adaptability. When trained on the FF++ dataset and tested on the Celeb-DF dataset, it achieves an AUC of 91.7%. This represents a 14.8% improvement in cross-dataset performance compared to the best existing model. © 2024 IEEE.","Data Preprocessing and Augmentation; DeepFake Detection; Generalization; Multi-domain Multi-scale; P article Swarm Optimization"
"Wu, J.; Zhang, B.; Li, Z.; Pang, G.; Teng, Z.; Fan, J.","Wu, Jianghao (58260652500); Zhang, Baopeng (57211546343); Li, Zhaoyang (58084749500); Pang, Guilin (57202025774); Teng, Zhu (36553668200); Fan, Jianping (57220794072)","58260652500; 57211546343; 58084749500; 57202025774; 36553668200; 57220794072","Interactive Two-Stream Network Across Modalities for Deepfake Detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159670945&partnerID=40&md5=7c9a44d4eb64d4a023c5088f157740ac","As face forgery techniques have become more mature, the proliferation of deepfakes may threaten the security of human society. Although existing deepfake detection methods achieve good performance for in-dataset evaluation, it remains to be improved in the generalization ability, where the representation of the imperceptible artifacts plays a significant role. In this paper, we propose an Interactive Two-Stream Network (ITSNet) to explore the discriminant inconsistency representation from the perspective of cross-modality. In particular, the patch-wise Decomposable Discrete Cosine Transform (DDCT) is adopted to extract fine-grained high-frequency clues, and information from different modalities communicates with each other via a designed interaction module. To perceive the temporal inconsistency, we first develop a Short-term Embedding Module (SEM) to refine subtle local inconsistency representation between adjacent frames, and then a Long-term Embedding Module (LEM) is designed to further refine the erratic temporal inconsistency representation from the long-range perspective. Extensive experimental results conducted on three public datasets show that ITSNet outperforms the state-of-the-art methods both in terms of in-dataset and cross-dataset evaluations. © 2023 IEEE.","cross-modality learning; Deepfake detection; inconsistency representation"
"Kingra, S.; Aggarwal, N.; Kaur, N.","Kingra, Staffy (57192255022); Aggarwal, Naveen (36875216400); Kaur, Nirmal (57189220908)","57192255022; 36875216400; 57189220908","SiamNet: Exploiting source camera noise discrepancies using Siamese Network for Deepfake Detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163196991&partnerID=40&md5=0a1d12c675526a544737a4808469475a","Recent advancements in deep neural networks especially GAN (Generative Adversarial Network) have resulted in the creation of more realistic deepfake media. This technology can swap the source person's face or alter facial expressions in an image or video; Media manipulated in such a way is termed deepfake. This type of manipulated media poses potential risks to journalism, politics, court proceedings and various social aspects. While existing approaches concentrate on the use of deep neural networks to directly extract facial artefacts for deepfake detection, they do not examine subtle inconsistencies in/across frame/frames. Moreover, state-of-the-art deepfake detection networks appear more complex and tend to overfit on specific artefacts which limits their generalizability on unseen data. This paper proposed a novel technique that tackles the problem of manipulated face detection in videos and images by exploiting the noise pattern inconsistency amongst face region and rest of the frame. To enable a comparison between the noise patterns of these two regions, we propose a two-stream Siamese-like network called SiamNet. This network can extract the noise patterns of the face region and patch through separate streams without increasing the number of parameters, thereby enhancing its efficiency and effectiveness. Each branch consists of pretrained Inception-v3 architecture for camera noise extraction. Siamese training is utilized to compare both noise patterns computed through different base models. The proposed two-branch network, SiamNet is found efficient for several large-scale deepfake datasets such as FF++, Celeb-DF, DFD and DFDC achieving accuracy rates of 99.7%, 98.3%, 96.08% and 89.2% respectively. Furthermore, the proposed technique exhibits greater generalizability and outperforms state-of-the-art of deepfake detection methods. Performance of the proposed model is also evaluated on FaceForensics benchmark dataset against different approaches. © 2023 Elsevier Inc.","Camera noise; Deepfake detection; Face-patch; Facial manipulation detection; Video forensics"
"Liang, Y.; Wang, M.; Jin, Y.; Pan, S.; Yong, Y.","Liang, Yufei (57547903500); Wang, Mengmeng (57193745284); Jin, Yining (57224528340); Pan, Shuwen (7402713446); Yong, Liu (56011543900)","57547903500; 57193745284; 57224528340; 7402713446; 56011543900","Hierarchical supervisions with two-stream network for Deepfake detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162908018&partnerID=40&md5=6403886f30c80d8770c356a2f008d15c","Recently, the quality of face generation and manipulation has reached impressive levels, making it difficult even for humans to distinguish real and fake faces. At the same time, methods to distinguish fake faces from reals came out, such as Deepfake detection. However, the task of Deepfake detection remains challenging, especially the low-quality fake images circulating on the Internet and the diversity of face generation methods. In this work, we propose a new Deepfake detection network that could effectively distinguish both high-quality and low-quality faces generated by various generation methods. First, we design a two-stream framework that incorporates a regular spatial stream and a frequency stream to handle the low-quality problem since we find that the frequency domain artifacts of low-quality images will be preserved. Second, we introduce hierarchical supervisions in a coarse-to-fine manner, which consists of a coarse binary classification branch to classify reals and fakes and a five-category classification branch to classify reals and four different types of fakes. Extensive experiments have proved the effectiveness of our framework on several widely used datasets. © 2023","Coarse to fine; Deepfake detection; Frequency domain; Two stream"
"Yang, G.; Wei, A.; Fang, X.; Zhang, J.","Yang, Gaoming (36976637300); Wei, Anxing (58134348700); Fang, Xianjin (26423619800); Zhang, Ji (57225122203)","36976637300; 58134348700; 26423619800; 57225122203","FDS_2D: rethinking magnitude-phase features for DeepFake detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160813488&partnerID=40&md5=7288cdb376c5c564646436e58cc35c99","To reduce the harm of forged information, more and more detection methods use frequency domain information. They mostly take spectra as clues to identify fake content. However, the current work tends to use only one of the magnitude and phase spectra for learning. In this paper, we notice that the magnitude and phase spectrum contain different image information. Only one spectrum is easily disturbed by noise, and the robustness of the method is difficult to guarantee. Therefore, we propose the Frequency Domain Separable DeepFake Detection (FDS_2D), which is a multi-branch network to obtain features in different frequency spectra. In FDS_2D, the spectral information is divided into three categories: the magnitude spectrum, the phase spectrum, and the relationship between the two spectra. According to their characteristics, we design independent modules for feature extraction from them. Moreover, to improve the utilization efficiency of multi-features, we propose a multi-input multi-output attention mechanism for information interaction between branches. The experimental results show that each part of FDS_2D effectively extracts and applies spectral information; The comprehensive performance of our model is verified on FaceForensic + + , Celeb-DF, and DFDC. It proves that the ability of FDS_2D to detect DeepFake is not inferior to existing models. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","DeepFake detection; Frequency; Magnitude spectra; Phase spectra; Spectrum relationship"
"Pang, G.; Zhang, B.; Teng, Z.; Qi, Z.; Fan, J.","Pang, Guilin (57202025774); Zhang, Baopeng (57211546343); Teng, Zhu (36553668200); Qi, Zige (58087970800); Fan, Jianping (57220794072)","57202025774; 57211546343; 36553668200; 58087970800; 57220794072","MRE-Net: Multi-Rate Excitation Network for Deepfake Video Detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147287860&partnerID=40&md5=320fa9985e9013574dca4cc987676a59","The current social media is flooded with hyper realistic face-synthetic videos due to the explosion of DeepFake technology that has brought a serious impact on human society security, which calls for further exploring on deepfake video detection methods. Existing methods attempt to isolated capture spatial artifacts or extract the homogeneous temporal inconsistency to detect deepfake video, but little attention has been paid to the exploitation of dynamic spatial-temporal inconsistency. To mitigate this issue, in this paper, we propose a novel Multi-Rate Excitation Network (MRE-Net) to effectively excite dynamic spatial-temporal inconsistency from the perspective of multiple rates for deepfake video detection. The proposed MRE-Net is composed of two components: Bipartite Group Sampling (BGS) and multiple rate branches. The BGS draws the entire video into multiple bipartite groups with different rates to cover various face motion dynamic evolution. We further design multiple rate branches to capture both short-term and long-term spatial-temporal inconsistency from corresponding bipartite groups of BGS. Concretely, for the early stages of the multi-rate branches, Momentary Inconsistency Excitation (MIE) module is developed to encode the spatial artifacts and intra-group short-term temporal inconsistency. Meanwhile, for the last stages of the multi-rate branches, Longstanding Inconsistency Excitation (LIE) module is constructed to perceive inter-group long-term temporal dynamics. Extensive experiments and visualizations conducted on four popular datasets demonstrate the effectiveness of the proposed method against state-of-the-art deepfake detection methods. © 2023 IEEE.","Deepfake detection; longstanding inconsistency; momentary inconsistency"
"Liang, P.; Liu, G.; Xiong, Z.; Fan, H.; Zhu, H.; Zhang, X.","Liang, Peifeng (59775033100); Liu, Gang (56403521800); Xiong, Zenggang (22636267500); Fan, Honghui (34871760900); Zhu, Hongjin (55637664700); Zhang, Xuemin (55715193100)","59775033100; 56403521800; 22636267500; 34871760900; 55637664700; 55715193100","A facial geometry based detection model for face manipulation using CNN-LSTM architecture","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150044861&partnerID=40&md5=1ebee9494551bb9b3c516cfb837567f9","This issue of DeepFake technique that may cause great threat to privacy, democracy, and national security has attracted the attention of deep learning researchers. DeepFake detection, therefore, has been a very hot issue in deep learning research. The face landmark feature maps are often used by many DeepFake approaches in generating fake faces. It also provides key information to help to detect manipulated face images. In this paper, we propose a detection approach for manipulated face images. To make full use of face landmark information, we propose a facial geometry prior module (FGPM) to extract facial geometry feature maps. Then the facial geometry feature maps are embedded into upsampling feature maps generated by the CNN-LSTM network. The proposed FGPM exploits facial maps and frequency domain correlation to analyze the discriminative characteristics between manipulated and non-manipulated regions by incorporating the CNN-LSTM network. Finally, a decoder is used to learn the mapping from low-resolution feature maps to pixel-wise to predict manipulation localization. Or a softmax classifier is used to predict real or fake face images. By experimenting on several popular datasets, the proposed detection model has demonstrated the capability of localizing manipulation at the pixel level and with a high prediction on real or fake face images. © 2023","CNN-LSTM; DeepFake detection; Facial analysis; Facial geometry prior module; Resampling"
"Ma, Z.; Mei, X.; Chen, H.; Shen, J.","Ma, Zhiyuan (58079714800); Mei, Xue (23091706700); Chen, Haoyang (58079458100); Shen, Jie (57196190343)","58079714800; 23091706700; 58079458100; 57196190343","Multi-Scale Feature Enhancement Network for Face Forgery Detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163285997&partnerID=40&md5=246d67660759851445d66992492d5d77","Nowadays, synthesizing realistic fake face images and videos becomes easy benefiting from the advance in generation technology. With the popularity of face forgery, abuse of the technology occurs from time to time, which promotes the research on face forgery detection to be an emergency. To deal with the potential risks, we propose a face forgery detection method based on multi-scale feature enhancement. Specifically, we analyze the forgery traces from the perspective of texture and frequency domain, respectively. We find that forgery traces are hard to be perceived by human eyes but noticeable in shallow layers of CNNs and middle-frequency domain and high-frequency domain. Hence, to reserve more forgery information, we design a texture feature enhancement module and a frequency domain feature enhancement module, respectively. The experiments on FaceForensics++ dataset and Celeb-DF dataset show that our method exceeds most existing networks and methods, which proves that our method has strong classification ability. © 2023 ACM.","DeepFake detection; Digital video forensics; Face forgery detection; Multi-scale feature fusion"
"Wang, B.; Wu, X.; Tang, Y.; Ma, Y.; Shan, Z.; Wei, F.","Wang, Bo (57216234699); Wu, Xiaohan (57460009800); Tang, Yeling (58120138500); Ma, Yanyan (57459880600); Shan, Zihao (59273548000); Wei, Fei (57202381047)","57216234699; 57460009800; 58120138500; 57459880600; 59273548000; 57202381047","Frequency Domain Filtered Residual Network for Deepfake Detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149052155&partnerID=40&md5=180ff6fe3df35a0b09379aef70e61a47","As deepfake becomes more sophisticated, the demand for fake facial image detection is increasing. Although great progress has been made in deepfake detection, the performance of most existing deepfake detection methods degrade significantly when these methods are applied to detect low-quality images for the disappearance of key clues during the compression process. In this work, we mine frequency domain and RGB domain information to specifically improve the detection of low-quality compressed deepfake images. Our method consists of two modules: (1) a preprocessing module and (2) a classification module. In the preprocessing module, we utilize the Haar wavelet transform and residual calculation to obtain the mid-high frequency joint information and fuse the frequency map with the RGB input. In the classification module, the image obtained by concatenation is fed to the convolutional neural network for classification. Because of the combination of RGB and frequency domain, the robustness of the model has been greatly improved. Our extensive experimental results demonstrate that our approach can not only achieve excellent performance when detecting low-quality compressed deepfake images, but also maintain great performance with high-quality images. © 2023 by the authors.","deepfake detection; feature fusion; frequency domain features; neural networks; wavelet transform"
"Zhao, Y.; Li, J.; Wang, L.","Zhao, Yulai (58906407900); Li, Jianhua (56724265500); Wang, Ling (58906754800)","58906407900; 56724265500; 58906754800","Harmonizing Dynamic Frequency Analysis with Attention Mechanisms for Efficient Facial Image Authenticity Detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190415742&partnerID=40&md5=5f03958f07a38c5e1813885f842aa12e","Deepfake detection is increasingly critical in security and digital forensics. Our research presents a novel method that synergizes dynamic frequency domain analysis with attention mechanisms to discern Deepfakes more effectively. This approach is developed in response to the inadequacies of existing techniques when confronted with sophisticated image alterations. By leveraging frequency domain subtleties and an advanced attention mechanism, our model achieves heightened accuracy in identifying key facial anomalies. Our experimental results show that DFAM could yield 93.94% accuracy rate which is the best compared with state-of-the-arts on Deepfakes dataset. © 2023 IEEE.","attention mechanisms; deepfake detection; dynamic frequency domain analysis"
"Lu, Y.; Luo, R.; Ebrahimi, T.","Lu, Yuhang (57352219800); Luo, Ruizhi (57565509600); Ebrahimi, Tourad (35560920500)","57352219800; 57565509600; 35560920500","Improving Deepfake Detectors against Real-world Perturbations with Amplitude-Phase Switch Augmentation","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176248132&partnerID=40&md5=60d8559c11ee3219a6f75c2bffaa4a92","In recent years, the remarkable progress in facial manipulation techniques has raised social concerns due to their potential malicious usage and has received considerable attention from both industry and academia. While current deep learning-based face forgery detection methods have achieved promising results, their performance often degrades drastically when they are tested in non-trivial situations under realistic perturbations. This paper proposes to leverage the information in the frequency domain, particularly the phase spectrum, to better differentiate between deepfakes and authentic images. Specifically, a new augmentation method called degradation-based amplitude-phase switch (DAPS) is proposed, which disregards the sensitive amplitude spectrum of a forged facial image and enforces the detection network to focus on phase components during the training process. Extensive evaluation results from a realistic assessment framework show that the proposed augmentation method significantly improves the robustness of two deepfake detectors analyzed and consistently outperform other augmentation approaches under various perturbations. © 2023 SPIE.","Data Augmentation; Deepfake Detection; Face Manipulation; Robustness"
"Liang, W.; Wu, Y.; Wu, J.; Xu, J.","Liang, Weiyun (57865819900); Wu, Yanfeng (57208537839); Wu, Jiesheng (57393654700); Xu, Jing (35778130600)","57865819900; 57208537839; 57393654700; 35778130600","FAClue: Exploring Frequency Clues by Adaptive Frequency-Attention for Deepfake Detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175530934&partnerID=40&md5=34c3f83ce1d23c92526d4634fd48849d","Detecting fake faces produced by face forgery technologies attracts intensive attention in recent years. Deep learning approaches have shown their effectiveness in deepfake detection task. Some previous deep learning-based methods exploit forgery artifacts in spatial domain but easily overfit the specific forgery patterns. Therefore, some works utilize additional frequency domain information to obtain generalized features. We consider to improve the frequency-based methods in two aspects: 1) extracting discriminative frequency features comprehensively; 2) mining complementary features in different domains sufficiently. In this paper, we propose a dual-stream network named FAClue for deepfake detection, which extracts comprehensive frequency information to complement spatial domain features. Specifically, the FAClue consists of three main components. A Frequency-Attention Extractor (FAE) is proposed to adaptively highlight prominent frequency bands from both global and local perspectives. A RGB-Frequency Complementary Enhancement (RFCE) module is developed to mine complementary information between RGB and frequency domains in an explicit manner. A Frequency Guided Attention (FGA) module is designed to fuse different domain features and generate discriminative features for detection. Extensive experiments on three benchmark datasets demonstrate the FAClue achieves competitive performance compared with state-of-the-art methods. © 2023 Technical Committee on Control Theory, Chinese Association of Automation.","Attention Mechanism; Deepfake Detection; Feature Fusion; Frequency Domain"
"Muller, N.M.; Sperl, P.; Böttinger, K.","Muller, Nicolas Michael (57211168855); Sperl, Philip (57211991783); Böttinger, Konstantin (55533323900)","57211168855; 57211991783; 55533323900","Complex-valued neural networks for voice anti-spoofing","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171563011&partnerID=40&md5=799b53ad0d2c829f0c5c0934682197dc","Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the”In-the-Wild” anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing. © 2023 International Speech Communication Association. All rights reserved.","audio deepfake detection; complex neural network; voice anti-spoofing"
"Xu, Y.; Zhang, D.D.; Sun, C.","Xu, Yijia (58567636900); Zhang, Dongdong (36633595100); Sun, Chengyu (35079450000)","58567636900; 36633595100; 35079450000","Frequency domain deepfake detection based on two-stream neural network","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170205700&partnerID=40&md5=8bce608ce5b4b8084e7ee2035d338659","Benefiting from the progress of deep learning driven generative models, face forgery technologies have rapidly become mature, thus raising public concerns about the illegal usage of these technologies. Despite the fact that fake images and videos are often unrecognizable to human eyes, recent work has found that hidden artifacts can be exposed in frequency domain. Our meth-od introduces the idea of separating human face area using landmarks before mining the forgery patterns in frequency domain. This we believe can help the network learn more discriminative features, also, a two-stream learning framework combining single-frame pathway and multi-frame pathway is developed to mine frequency clues. Compared with previous methods, our approach showed good results while using a few training data and little time. The effectiveness of our method is shown on different versions of the FF++ and Celeb-DF dataset. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deepfake Detection; Frequency Clues; Region Segmentation"
"Li, Y.; Bian, S.; Wang, C.; Lu, W.","Li, Ying (58447642500); Bian, Shan (55581279300); Wang, Chuntao (54586143100); Lu, Wei (57715097700)","58447642500; 55581279300; 54586143100; 57715097700","CNN and Transformer-coordinated deepfake detection; CNN 结合 Transformer 的深度伪造高效检测","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162869401&partnerID=40&md5=bec19966923b64d9d5491e0cceb635f1","Objective The research of deepfake detection methods has become one of the hot topics recently to counter deepfake videos. Its purpose is to identify fake videos synthesized by deep forgery technology on social networks, such as WeChat, Instagram and TikTok. Forged features are extracted on the basis of a convolutional neural network (CNN) and the final classification score is determined in terms of the features-forged classifier. When facing the deep forged video with low quality or high compression, these methods improve the detection performance by extracting deeper spatial domain information. However, the forged features left in the spatial domain decrease with the compression, and the local features tend to be similar, which degrades the performances severely. This also urges us to retain the frequency domain information of forged image artifacts as one of the clues of forensics, which contains less interference caused by JPEG compression. The CNN-based spatial domain feature extraction method can be conducted to capture facial artifacts via stacking convolution. But, its receptive field is limited, so it is better at modelling local information but ignores the relationship between global pixels. Transformer has its potentials at long-term dependency modelling in relevant to natural language processing and computer vision tasks, therefore it is usually employed to model the relationship between pixels of images and make up for the CNN-based deficiency in global information acquisition. However, the transformer can only process sequence information, making it still need the cooperation of convolutional neural network in computer vision tasks. Method First, we develop a novel joint detection model, which can leverage the advantages of CNN and transformer, and enriches the feature representation via frequency domain-related information. The EfficientNet-b0 is as the feature extractor. To optimize more forensics features, in the spatial feature extraction stage, the attention module is embedded in the shallow layer and the deep features are multiplied with the activation map obtained by the attention module. In the frequency domain feature extraction stage, to better learn the frequency domain features, we utilize the discrete cosine transform as the frequency domain transform means and an adaptive part is added to the frequency band decomposition. In the training process, to accelerate the memory-efficient training, we adopt the method of mixed precision training. Then, to construct the joint model, we link the feature extraction branches to a modified Transformer structure. The Transformer is used to model inter-region feature correlation using global self-attention feature encoding through an encoder structure. To further realize the information interaction between the dual-domain features, the cross attention is calculated between branches on the basis of the cross-attention structure. Furthermore, we design and implement a random data augmentation strategy, which is coordinated with the attention mechanism to improve the detection accuracy of the model in the scenarios of cross compression rate and cross dataset. Result Our joint model is compared to 9 state-of-the-art deepfake detection methods on two datasets called FaceForensics ++ (FF ++) and Celeb-DF. In the experiments of cross compression-rate detection on the FF ++ dataset, our detection accuracy can be reached to 90. 35%, 71. 79% and 80. 71% for Deepfakes, Face2Face and Neural Textures (NT) manipulated images, respectively. In the cross-dataset experiments, i. e., training on FaceForensics ++ and testing on Celeb-DF, our training time is reduced. Conclusion The experiments demonstrate that our joint model proposed can improve datasets-crossed and compression-rate acrossed detection accuracy. Our joint model takes advantage of the EfficientNet and the Transformer, and combines the characteristics of different domain features, attention, and data augmentation mechanism, making the model more accurate and efficient. © 2023 Journal of Image and Graphics. All rights reserved.","convolutional neural network (CNN); deepfake detection; frequency domain; spatial domain; Vision Transformer (ViT)"
"Jeon, S.M.; Seong, H.A.; Lee, E.C.","Jeon, Su-min (57580359800); Seong, Hyeonah (58250482500); Lee, Eui Chul (14009024200)","57580359800; 58250482500; 14009024200","Deepfake Video Detection Using the Frequency Characteristic of Remote Photoplethysmography","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159407357&partnerID=40&md5=e58671c5f09ecb995774c54455cf1dd8","Photoplethysmography is a technique for measuring the blood flow per unit time of an artery. Remote photoplethysmography is a method for obtaining photoplethysmography signals in a non-contact manner through a sensor such as a camera and has been recently applied to various fields. In this study, we propose a method for detecting Deepfake modulated color video based on remote photoplethysmography concept. As experimental data, 50 real videos and their 50 Deepfake videos using Face Swapping Generative Adversarial Networks were used. The photoplethysmography signals of face and neck regions were extracted, respectively, and the signals were preprocessed by detrending and performing Butterworth bandpass filtering. The 80 power values ​​in the frequency domain were defined as feature vectors. As a result of analyzing the L2 Norm between the two vectors extracted from the face region and the neck region, the L2 Norms of the real video and the fake video were 0.0000307 and 0.0001332, respectively, confirming that the distributions were clearly separated. It was confirmed that there is a significant difference between the real and the fake videos. Also, as a result of calculating the degree of separation of distributions with d-prime, 2.32 was derived. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Bio-signals; Deepfake detection; Face recognition; Face substitution; Remote photoplethysmography"
"Nallapati, S.R.; Dommeti, D.; Medhalavalasa, S.; Bonku, K.K.; Srinivas, P.V.V.S.; Bhattacharyya, D.","Nallapati, Siva Ramakrishna (58102073700); Dommeti, Dhiren (58102712400); Medhalavalasa, Saket (58246140300); Bonku, Kranthi Kiran (58244946000); Srinivas, P. V.V.S. (37067906600); Bhattacharyya, Debnath (57216142572)","58102073700; 58102712400; 58246140300; 58244946000; 37067906600; 57216142572","Identification of Deepfakes using Strategic Models and Architectures","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159079589&partnerID=40&md5=70274036a288295aceac2eaf6252e126","Deepfake technology has been rapidly evolving and expanding in recent years. It has become increasingly easy to manipulate multimedia content, making it harder to detect what is real and what is manipulated. The research aims to explore how neural networks can be used to detect deepfake in multimedia, helping to protect users from potentially malicious and deceptive content. The aim is to explore what neural networks are, how they can be used to detect deepfakes and the potential implications of this technology. The research also aims to evaluate the advantages and disadvantages of using neural networks for deepfake detection. As the world of deepfake technology continues to evolve, this research will provide an overview of the latest developments in deepfake detection and their potential impact. The goal of this research is to use neural networks to detect deepfakes and to identify suspicious content to alert users. This could help protect users from being exposed to malicious content and help content producers ensure the integrity of their work. As deepfake technology continues to evolve, neural networks may become an essential tool for quickly and accurately detecting deepfakes in multimedia. The research explores topics like, CNN, 3D CNN, GATED RECURRENT UNIT and Architectures like Xception, VGG16, InceptionV3 and ResNet50V2. The outcomes are graphically represented and analyzed. the comparative stratification of the approach is done to analyze and detect deepfakes. © 2023 IEEE.","Convolutional Neural Network; Deep Learning; Deepfake Detection; Gated Recurrent Unit; Image Noise Patterns"
"Han, R.; Wang, X.; Bai, N.; Wang, Q.; Liu, Z.; Xue, J.","Han, Ruidong (58199903300); Wang, Xiaofeng (56028744400); Bai, Ningning (58102462000); Wang, Qin (57876406800); Liu, Zinian (58199955400); Xue, Jianru Ru (8899818900)","58199903300; 56028744400; 58102462000; 57876406800; 58199955400; 8899818900","FCD-Net: Learning to Detect Multiple Types of Homologous Deepfake Face Images","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153802595&partnerID=40&md5=ae28cb679eed9a9f6e3a4f0d47d35531","With the rapid development of artificial intelligence technology, a variety of GAN generated deepfake face images/videos have emerged endlessly. The abuse of deepfake has brought serious negative effects to many industries. Therefore, there is an urgent need to develop advanced methods to combat the abuse of deepfake. As far as we know, there are almost no techniques that can distinguish multiple types of homologous deepfake face images. In this study, we propose a method based on the multi-classification task to address this issue. The proposed method relies on a novel network framework named FCD-Net that consists of the facial synaptic saliency module (FSS), the contour detail feature extraction module (CDFE), and the distinguishing feature fusion module (DFF). Utilizing this method, the imperceptible features introduced by deepfake can be exposed, and the differences caused by different types of deepfake can be distinguished, even if deepfake images are homologous. To test the proposed method and compare it with other SOTA methods, we establish a new homologous dataset named HDFD that contains real face images, entire face synthesis images, face swap images, and facial attribute manipulation images. Among them, the three types of deepfake images are all generated from the same real face images through different deepfake techniques. Abundant experiment results demonstrate that the proposed method has a high-level detection accuracy and relatively strong robustness against content-preserving manipulations. Moreover, the generalization of our method is superior to other SOTA methods. © 2005-2012 IEEE.","contour detail feature extraction; Deepfake detection; distinguishable feature fusion (DFF); facial synaptic saliency (FSS); homologous face images"
"Wang, G.; Jiang, Q.; Jin, X.; Li, W.; Cui, X.","Wang, Gaojian (57226340383); Jiang, Qian (57194699462); Jin, Xin (56991832300); Li, Wei (57196308169); Cui, Xiaohui (57195974967)","57226340383; 57194699462; 56991832300; 57196308169; 57195974967","MC-LCR: Multimodal contrastive classification by locally correlated representations for effective face forgery detection","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131412788&partnerID=40&md5=e25a98d20823670e8a9432aa96f2e401","As the remarkable development of facial manipulation technologies is accompanied by severe security concerns, face forgery detection has spurred recent research. Most detection methods train a binary classifier under global supervision to judge whether a face is real or fake. However, advanced manipulations only perform small-scale tampering, posing challenges to comprehensively capturing subtle and local forgery artifacts, especially in high-compression settings and cross-dataset scenarios. To address such limitations, we propose a framework, multimodal contrastive classification by locally correlated representations (MC-LCR), for effective face forgery detection. Instead of specific appearance features, MC-LCR amplifies implicit local discrepancies between authentic and forged faces from both the spatial and frequency domains. A shallow style representation block measures the pairwise correlation of shallow feature maps, encoding local style information to extract more discriminative features in the spatial domain. We observe that subtle forgery artifacts can be further exposed in the patch-wise phase and amplitude spectrum, and that they exhibit different clues. According to the complementarity of amplitude and phase information, we develop a patch-wise amplitude and phase dual attention module to capture locally correlated inconsistencies in the frequency domain. The collaboration of supervised contrastive loss with cross-entropy loss helps the network learn more discriminative and generalized representations. Through extensive experiments and comprehensive studies, we achieve state-of-the-art performance and demonstrate the robustness and generalization of our method. © 2022 Elsevier B.V.","Deepfake detection; Face forgery detection; Local feature correlation; Multimedia forensics"
"Wang, J.; Wu, Z.; Ouyang, W.; Han, X.; Chen, J.; Lim, S.-N.; Jiang, Y.-G.","Wang, Junke (57219972671); Wu, Zuxuan (56377225900); Ouyang, Wenhao (57695813800); Han, Xintong (57189657869); Chen, Jingjing (55613576900); Lim, Sernam (57215775116); Jiang, Yugang (14054081900)","57219972671; 56377225900; 57695813800; 57189657869; 55613576900; 57215775116; 14054081900","M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134060827&partnerID=40&md5=b5a34c9e7f98aabe9e7dd176d322a7a7","The widespread dissemination of Deepfakes demands effective approaches that can detect perceptually convincing forged images. In this paper, we aim to capture the subtle manipulation artifacts at different scales using transformer models. In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which operates on patches of different sizes to detect local inconsistencies in images at different spatial levels. M2TR further learns to detect forgery artifacts in the frequency domain to complement RGB information through a carefully designed cross modality fusion block. In addition, to stimulate Deepfake detection research, we introduce a high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and facial reenactment methods. We conduct extensive experiments to verify the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection methods by clear margins. © 2022 ACM.","deepfake dataset; deepfake detection; multiscale transformer"
"Shen, D.; Zhao, Y.; Quan, C.","Shen, Dongyao (57337921800); Zhao, Youjian (35933102400); Quan, Chengbing Bin (57098796100)","57337921800; 35933102400; 57098796100","Identity-Referenced Deepfake Detection with Contrastive Learning","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134193142&partnerID=40&md5=cd202e0e45623df8f9a58651b51ca9f9","With current advancements in deep learning technology, it is becoming easier to create high-quality face forgery videos, causing concerns about the misuse of deepfake technology. In recent years, research on deepfake detection has become a popular topic. Many detection methods have been proposed, most of which focus on exploiting image artifacts or frequency domain features for detection. In this work, we propose using real images of the same identity as a reference to improve detection performance. Specifically, a real image of the same identity is used as a reference image and input into the model together with the image to be tested to learn the distinguishable identity representation, which is achieved by contrastive learning. Our method achieves superior performance on both FaceForensics++ and Celeb-DF with relatively little training data, and also achieves very competitive results on cross-manipulation and cross-dataset evaluations, demonstrating the effectiveness of our solution. © 2022 Owner/Author.","deepfake detection; deepfakes; forensics"
"Wang, J.; Sun, Y.; Tang, J.","Wang, Jian (57221358673); Sun, Yunlian (55866644500); Tang, Jinhui (56364850900)","57221358673; 55866644500; 56364850900","LiSiam: Localization Invariance Siamese Network for Deepfake Detection","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133737405&partnerID=40&md5=19092f3c703bec5717e04bcec65f1672","Advances in facial manipulation technology have led to increasing indistinguishable and realistic face swap videos, which raises growing concerns about the security risk of deepfakes in the community. Although current deepfake detectors can gain promising performance when handling high-quality faces under within-database settings, most detectors suffer from performance degradation in cross-database evaluation. Moreover, when test faces' quality is different from training faces, the performance degrades even under within-database settings. To this end, we propose a novel Localization invariance Siamese Network (LiSiam) to enforce localization invariance against different image degradation for deepfake detection. Specifically, our Siamese network-based feature extractor takes the original image and the corresponding quality-degraded image as pairwise inputs and outputs two segmentation maps. A localization invariance loss is further proposed to impose localization consistency between the two segmentation maps. In addition, we design a Mask-guided Transformer to capture the co-occurrence between the forgery region and its surroundings. Finally, a multi-task learning strategy is utilized to obtain a robust and discriminative feature representation and jointly optimize multiple objective functions (i.e., segmentation, classification, and localization invariance losses) in an end-to-end manner. Experimental results on two public datasets, i.e., FaceForensics++ and Celeb-DF, demonstrate the superior performance of our proposed method to state-of-the-art methods. © 2005-2012 IEEE.","attention mechanism; Beepfake detection; localization invariance; multi-task learning; Siamese network"
