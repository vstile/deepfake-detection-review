Nallagula Karthik Sagar, Srinivas Arukonda,
A Novel CNN-LSTM Approach for Robust Deepfake Detection,
Procedia Computer Science,
Volume 258,
2025,
Pages 1844-1855,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.04.436.
(https://www.sciencedirect.com/science/article/pii/S187705092501539X)
Abstract: The rapid spread of deepfake videos poses significant challenges to the credibility of digital media, raising concerns over pri- vacy, misinformation, and trustworthiness. This research introduces a hybrid model combining Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs) to enhance deepfake detection. By leveraging ResNeXt-50 for extracting relevant features and LSTMs for capturing frame-to-frame dependencies, the proposed architecture effectively detects altered facial features in videos. Key preprocessing techniques, including face detection, extraction, and segmentation, optimize input data by isolating relevant facial regions. Experimental results demonstrate that this approach outperforms current methods in identifying subtle deepfake artifacts, underscoring the need for robust detection mechanisms to protect the credibility of digital media. Future work will explore improved scalability and real-time applications of this technique.
Keywords: DeepFake Detection; Machine Learning; Deep Learning; Image Classification; Face Recognition; LSTM; Convolutional Neural Networks

Jiajia Li, Ziyi Pan, Teng Xiao, Ping Wang, Qibiao Hu, Jingrui Hou,
EmoSense: A multimodal sentiment-aware framework for music short video AI-generated content detection,
Information Processing & Management,
Volume 63, Issue 2, Part B,
2026,
104473,
ISSN 0306-4573,
https://doi.org/10.1016/j.ipm.2025.104473.
(https://www.sciencedirect.com/science/article/pii/S0306457325004145)
Abstract: The rapid spread of AI-generated content (AIGC) music short videos on social media has introduced new challenges for information authenticity and public trust. Although existing studies have explored multimodal detection techniques, they often fail to model the nuanced emotional and semantic interplay between modalities—particularly the alignment between musical affect and visual-textual content. Such limitations significantly hinder detection accuracy in complex, sentiment-rich AIGC scenarios. To address these challenges, we propose EmoSense, a sentiment-aware framework tailored for music short video AIGC detection. EmoSense comprises two key modules: a Sentiment Alignment Module that models emotional-semantic coherence across text, audio, and visuals via cross-modal embedding, and a Trace Analysis Module that detects spatial–temporal inconsistencies characteristic of synthetic content. Additionally, A deep fusion strategy further enhances cross-modal complementarity, improving both robustness and generalization. To support evaluation, we introduce MSV-AIGC, a real-world, human-annotated multimodal dataset containing 2912 labeled samples (1562 authentic and 1350 AI-generated), covering aligned those modalities. Experimental results show that EmoSense outperforms state-of-the-art baseline on this dataset, achieving 2.27% gains in accuracy and surpassing GPT-4V by 10.7%, highlighting its robustness in detecting synthetic music short videos.
Keywords: Multimodal; AI-generated content detection; Sentiment analysis; Music short video; Feature fusion

Mubarak Alrashoud,
Deepfake video detection methods, approaches, and challenges,
Alexandria Engineering Journal,
Volume 125,
2025,
Pages 265-277,
ISSN 1110-0168,
https://doi.org/10.1016/j.aej.2025.04.007.
(https://www.sciencedirect.com/science/article/pii/S111001682500465X)
Abstract: Deepfake technology creates highly realistic manipulated videos using deep learning models, which makes distinguishing between authentic and fake content extremely difficult. This technology can negatively affect society by breaching privacy and spreading misinformation. This paper presents a comprehensive survey of the recent deepfake video detection approaches and methods. Each deepfake video method is analyzed according to its ability to generalize diverse deepfake fabrication techniques and real-world scenes. We reviewed around 103 articles which eventually shrunk down to 73 based on the screening criteria like abstract/title/irrelevant focus/duplication. The study primarily covers audio-based, visual-based, and multi-modal detection methods. Also, it discusses the usage of Convolutional Neural Networks (CNNs), frequency-domain analysis, and audio-visual synchronization in deepfake video detection and evaluates the strengths and shortcomings of these techniques. Moreover, the study explores major issues such as low resolution, video compression, and adversarial attacks, which prove to be a barrier to making deepfake video detection processes robust. By connecting findings from numerous studies, this research draws attention to the development of standard benchmarking SOPs and multi-modal detection techniques to improve detection performance.
Keywords: Deepfake video detection; CNNs; Frequency-domain analysis; Multi-modal detection; Adversarial attacks; Video compression; Audio-visual synchronization

Akanbi Bolakale AbdulQudus, Oluwatosin Ahmed Amodu, Umar Ali Bukar, Raja Azlina Raja Mahmood, Anies Faziehan Zakaria, Saki-Ogah Queen, Zurina Mohd Hanapi,
A Contemporary and Comprehensive Bibliometric Exposition on Deepfake Research and Trends,
Computers, Materials and Continua,
Volume 84, Issue 1,
2025,
Pages 153-236,
ISSN 1546-2218,
https://doi.org/10.32604/cmc.2025.061427.
(https://www.sciencedirect.com/science/article/pii/S1546221825005521)
Abstract: This paper provides a comprehensive bibliometric exposition on deepfake research, exploring the intersection of artificial intelligence and deepfakes as well as international collaborations, prominent researchers, organizations, institutions, publications, and key themes. We performed a search on the Web of Science (WoS) database, focusing on Artificial Intelligence and Deepfakes, and filtered the results across 21 research areas, yielding 1412 articles. Using VOSviewer visualization tool, we analyzed this WoS data through keyword co-occurrence graphs, emphasizing on four prominent research themes. Compared with existing bibliometric papers on deepfakes, this paper proceeds to identify and discuss some of the highly cited papers within these themes: deepfake detection, feature extraction, face recognition, and forensics. The discussion highlights key challenges and advancements in deepfake research. Furthermore, this paper also discusses pressing issues surrounding deepfakes such as security, regulation, and datasets. We also provide an analysis of another exhaustive search on Scopus database focusing solely on Deepfakes (while not excluding AI) revealing deep learning as the predominant keyword, underscoring AI’s central role in deepfake research. This comprehensive analysis, encompassing over 500 keywords from 8790 articles, uncovered a wide range of methods, implications, applications, concerns, requirements, challenges, models, tools, datasets, and modalities related to deepfakes. Finally, a discussion on recommendations for policymakers, researchers, and other stakeholders is also provided.
Keywords: Deepfake; bibliometric; deepfake detection; deep learning; recommendations

Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Dung Tien Nguyen, Duc Thanh Nguyen, Thien Huynh-The, Saeid Nahavandi, Thanh Tam Nguyen, Quoc-Viet Pham, Cuong M. Nguyen,
Deep learning for deepfakes creation and detection: A survey,
Computer Vision and Image Understanding,
Volume 223,
2022,
103525,
ISSN 1077-3142,
https://doi.org/10.1016/j.cviu.2022.103525.
(https://www.sciencedirect.com/science/article/pii/S1077314222001114)
Abstract: Deep learning has been successfully applied to solve various complex problems ranging from big data analytics to computer vision and human-level control. Deep learning advances however have also been employed to create software that can cause threats to privacy, democracy and national security. One of those deep learning-powered applications recently emerged is deepfake. Deepfake algorithms can create fake images and videos that humans cannot distinguish them from authentic ones. The proposal of technologies that can automatically detect and assess the integrity of digital visual media is therefore indispensable. This paper presents a survey of algorithms used to create deepfakes and, more importantly, methods proposed to detect deepfakes in the literature to date. We present extensive discussions on challenges, research trends and directions related to deepfake technologies. By reviewing the background of deepfakes and state-of-the-art deepfake detection methods, this study provides a comprehensive overview of deepfake techniques and facilitates the development of new and more robust methods to deal with the increasingly challenging deepfakes.
Keywords: Deepfakes; Face manipulation; Artificial intelligence; Deep learning; Autoencoders; GAN; Forensics; Survey

Amritha Devi N, Philomina Simon,
DeepGuardNet: A Novel CNN Architecture for DeepFake Image Detection,
Procedia Computer Science,
Volume 258,
2025,
Pages 811-818,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.04.313.
(https://www.sciencedirect.com/science/article/pii/S1877050925014152)
Abstract: We are in the world where information is the ubiquitous but the authentication of that information is cumbersome. Deepfake technology have achieved a tremendous growth in the digital era. Deepfake is a synthetic media audio, video or images that appear to be realistic though they are fake or fabricated. Deepfake are created by Generative AI techniques that understand the probabilistic distribution of the data. Appropriate detection systems are necessary to prevent the dissemination of misleading information and guarantee the authenticity and integrity of data. Detecting deepfake contents in this digital era is very challenging due to the realistic nature of fake images. In this paper, we present an enhanced CNN architecture, DeepGuardNet, a deepfake detection model that is simple and effective for determining whether images are real or fake. DeepGuardNet is a straightforward, sequential, and robust network designed for deepfake recognition and detection. Additionally, our network has an enhanced ability to detect tampered content with fewer parameters due to the use of separable convolution. In this work, we utilize depthwise separable convolution to efficiently extract deepfake features. The DeepGuardNet architecture effectively captures deepfake image features in both the spatial and depth dimensions. Experimental study on Celeb-DF dataset demonstrated the competence of the proposed method with an accuracy of 91% when compared with conventional methods. The proposed DeepGuardNet architecture is productive in terms of the better feature extraction and reduced computational complexity.
Keywords: Deepfake; DeepGuardNet; MesoNet; Separable Convolution; Convolutional Neural Network; Deepfake Detection; Deep Learning

Gueltoum Bendiab, Houda Haiouni, Isidoros Moulas, Stavros Shiaeles,
Deepfakes in digital media forensics: Generation, AI-based detection and challenges,
Journal of Information Security and Applications,
Volume 88,
2025,
103935,
ISSN 2214-2126,
https://doi.org/10.1016/j.jisa.2024.103935.
(https://www.sciencedirect.com/science/article/pii/S2214212624002370)
Abstract: Deepfake technology presents significant challenges for digital media forensics. As deepfakes become increasingly sophisticated, the ability to detect and attribute manipulated media becomes more difficult. The main challenge lies in the realistic and convincing nature of deepfakes, which can deceive human perception and traditional forensic techniques. Furthermore, the widespread availability of open-source deepfake tools and increasing computational power contribute to the ease with which malicious actors can create and disseminate deepfakes. The challenges posed by deepfakes for digital media forensics are multifaceted. Therefore, the development of sophisticated detection algorithms, the creation of comprehensive datasets, and the establishment of legal frameworks are crucial in addressing these challenges. This paper provides a comprehensive analysis of current methods for deepfake generation and the issues surrounding their detection. It also explores the potential of modern AI-based detection techniques in combating the proliferation of deepfakes. This analysis aims to contribute to advancing deepfake detection by highlighting the limits of current detection techniques, the most relevant issues, the upcoming challenges, and suggesting future directions for research.
Keywords: Deepfake; Artificial intelligence; Digital media forensics; Security; Deepfake detection

Mateusz Kazimierczak, Nuzaira Habib, Jonathan H. Chan, Thanyathorn Thanapattheerakul,
Impact of AI on the Cyber Kill Chain: A Systematic Review,
Heliyon,
Volume 10, Issue 24,
2024,
e40699,
ISSN 2405-8440,
https://doi.org/10.1016/j.heliyon.2024.e40699.
(https://www.sciencedirect.com/science/article/pii/S2405844024167308)
Abstract: The Cyber Kill Chain (CKC) defense model aims to assist subject matter experts in planning, identifying, and executing against cyber intrusion activity, by outlining seven stages required for adversaries to execute an attack. Recent advancements in Artificial Intelligence (AI) have empowered adversaries to execute sophisticated attacks to exploit system vulnerabilities. As a result, it is essential to consider how AI-based tools change the cyber threat landscape and affect the current standard CKC model. Thus, this study examines and categorizes how attackers use AI-based tools, and offers potential defense mechanisms. We conducted a systematic literature review of 62 papers published between 2013 and 2023 from the Web of Science and Google Scholar databases. Our findings indicate that AI-based tools are used most effectively in the initial stages of cyberattacks. However, we find that current defense tools are not designed to counter these sophisticated attacks during these stages. Thus, we provide insights to 1) highlight the changing threat landscape due to AI and 2) to guide the development of cyber defense mechanisms.
Keywords: Cybersecurity; Cyber attacks; Artificial intelligence in cybersecurity; Cyber kill chain; Adversarial AI; AI-based cyber attacks; Systems security; Intrusion/anomaly detection and malware mitigation; Software and application security

Kyung-Jong Kim, Chan-Hwi Lee, So-Eun Bae, Ju-Hyun Choi, Wook Kang,
Digital forensics in law enforcement: A case study of LLM-driven evidence analysis,
Forensic Science International: Digital Investigation,
Volume 54,
2025,
301939,
ISSN 2666-2817,
https://doi.org/10.1016/j.fsidi.2025.301939.
(https://www.sciencedirect.com/science/article/pii/S2666281725000782)
Abstract: The advent of digital technology and the ubiquity of mobile devices in today's society has led to a significant increase in the importance of mobile forensics in criminal investigations. Responding to the escalating volume and complexity of data due to enhanced smartphone capabilities and pervasive messaging apps, law enforcement agencies face challenges in data analysis. This study explores improving investigative efficiency through LLM-driven analysis of text from mobile messenger communications. We have conducted experiments on anonymized data collected from real crime scenes by employing three state-of-the-art LLM models, namely GPT-4o, Gemini 1.5 and Claude 3.5. The study focuses on optimizing model performance by employing prompt engineering, interpreting expressions embedded with hidden meanings such as slang, and contextually inferring ambiguous word usage. Finally, model performance is quantitatively evaluated using metrics such as precision, recall, F1 score, and hallucination rate.
Keywords: Artificial intelligence; Digital forensics; Instant messenger; Large language model (LLM)

Joshita Malla, Harshini Vemuri, SreeDivya Nagalli, S Abhishek, T Anjali,
The Analysis of Neural Network Models to Distinguish AI generated faces from Real faces,
Procedia Computer Science,
Volume 233,
2024,
Pages 295-306,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2024.03.219.
(https://www.sciencedirect.com/science/article/pii/S1877050924005787)
Abstract: The rise of Artificial Intelligence (AI)-generated faces that are identical to actual ones is both a technological innovation and a major concern. While considering the implications, some of the existing security systems are unable to differentiate between a high-quality deep-fake and an actual intruder's face. For instance, the risks are quite high at an airport security checkpoint, where facial recognition is the first line of security against unauthorized entry. The primary concern here is how trustworthy the computer programs and algorithms will be in recognizing counterfeits among a plethora of actual and artificially generated faces. This necessitates the need to introduce machine learning approaches to differentiate between actual and fraudulent faces, particularly when AI-generated faces are involved. Effective artificial intelligence systems must be adaptive and change quickly in the face of more complex threats rather than simply recognizing them. AI-generated faces are becoming more convincing by the day, increasing the risk of their exploitation. This poses the need to ensure that the technology on which people rely should be robust and trustworthy in critical situations, whether it is a security checkpoint or an e-commerce site. In this perspective, this study has attempted to develop and implement Artificial Intelligence-powered solutions to detect the artificial faces while ensuring reliability for critical functions in an age where reality constantly blends with fiction.
Keywords: Artificial Intelligence; Deep Fake; Deep Learning; Facial Recognition; Privacy; Security

Abdullah Yousafzai, Muhammad Mohsan Sheeraz, Ganna Pogrebna, Jon Crowcroft, Ibrar Yaqoob,
Blockchain for the metaverse: Recent advances, taxonomy, and future challenges,
Journal of Network and Computer Applications,
Volume 244,
2025,
104355,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2025.104355.
(https://www.sciencedirect.com/science/article/pii/S1084804525002528)
Abstract: The metaverse is a shared virtual 3D space that combines immersive experiences with applications in gaming, social interactions, commerce, and more. It is rapidly becoming a reality, driven by advances in virtual reality, augmented reality, artificial intelligence, blockchain, and other emerging technologies. Among these, blockchain technology enables secure and decentralized ownership as well as seamless interoperability of virtual assets. Non-fungible tokens ensure verifiable ownership and fraud prevention, while smart contracts facilitate automated peer-to-peer transactions. Blockchain’s security and transparency promote trust and innovation, laying the foundation for a connected and user-driven metaverse ecosystem. In this paper, we explore the role of blockchain technology as a key enabler for the metaverse, providing solutions for decentralization, governance through decentralized autonomous organizations, interoperable mechanisms, digital asset ownership, traceability, auditing, and identity management. We present the key difference between traditional virtual worlds and the metaverse, and why blockchain is preferred over other decentralized technologies for the metaverse. We comprehensively review recent advances in metaverse system architectures, focusing on state-of-the-art solutions and lessons learned. We compare the existing literature based on key parameters; namely, contributions, advantages, limitations, and applications. We present key challenges, including deepfake threats, identity theft and brand infringement risks, mental health risks, digital safety and gambling risks, virtual world laws and regulations, and privacy and data security concerns. We outline future recommendations for enabling a sustainable and user-friendly metaverse ecosystem.
Keywords: Metaverse; Blockchain; Virtual worlds; NFTs; Security; Decentralization

 Furizal, Alfian Ma'arif, Hari Maghfiroh, Iswanto Suwarno, Denis Prayogi,  Kariyamin, Syahrani Lonang, Abdel-Nasser Sharkawy,
Social, legal, and ethical implications of AI-Generated deepfake pornography on digital platforms: A systematic literature review,
Social Sciences & Humanities Open,
Volume 12,
2025,
101882,
ISSN 2590-2911,
https://doi.org/10.1016/j.ssaho.2025.101882.
(https://www.sciencedirect.com/science/article/pii/S2590291125006102)
Abstract: The rapid development of AI has fuelled the spread of deepfake pornography synthetic content that realistically fakes an individual's identity without their consent. This phenomenon has complex social, legal, and ethical implications, particularly related to privacy violations, sexual exploitation, and legal vulnerabilities. This study aims to analyze the social impacts of deepfake pornography, identify existing legal gaps, and evaluate the ethical and regulatory responses that have emerged globally. Using the SLR approach, this study adopts the PICOS framework and PRISMA methodology in the screening and selection of scientific publications. The study finds that the majority of victims, especially women and vulnerable groups, experience psychological, social, and professional harm. Barriers to access to justice are exacerbated by weak legal frameworks, limited capacity of law enforcement officers, and gender bias in legal protection. The absence of a specific legal definition widens the scope for exploitation and exacerbates social inequality. The study recommends comprehensive legal reforms, including criminalization of non-consensual deepfake content, obligations for digital platforms in content moderation, and adoption of technologies such as watermarking (visible and invisible), C2PA standards-based metadata labelling, and advanced AI detection to track synthetic media. Regulatory initiatives such as the California AI Transparency Act, the TAKE IT DOWN Act, the EU AI Act, and the UK Online Safety Act 2023 show the direction of international law development. In addition, public education about the dangers of deepfakes and their legal consequences is an important part of prevention efforts. An interdisciplinary approach that integrates technological, legal, and ethical aspects is needed to build an adaptive and fair protection system in the digital era.
Keywords: Generative artificial intelligence; Deepfake pornography; Social impact; Ethical implication; Legal reform; Privacy

Ashish Kumar, Divya Singh, Rachna Jain, Deepak Kumar Jain, Chenquan Gan, Xudong Zhao,
Advances in DeepFake detection algorithms: Exploring fusion techniques in single and multi-modal approach,
Information Fusion,
Volume 118,
2025,
102993,
ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2025.102993.
(https://www.sciencedirect.com/science/article/pii/S1566253525000661)
Abstract: In recent years, generative artificial intelligence has gained momentum and created extremely realistic synthetic multimedia content that can spread misinformation and mislead society. Deepfake detection is a technique consisting of frameworks, algorithms and approaches to predict manipulated contents namely, image, audio and video. To this end, we have analyzed and explored various deepfake detection frameworks by categorizing them as single-modal or multi-modal approaches. To provide better understanding and clarity, single-modal approaches are further categorized as conventional and advanced techniques. Conventional techniques extract complementary handcrafted features and classify them using machine-learning-based algorithms. On the other hand, advanced techniques adopt deep learning and hybrid algorithms to detect deepfakes. Multi-modal techniques utilize a mixture of two or more modalities for feature extraction and fuse them to obtain the final classification scores. These techniques are also categorized either as deep learning or hybrid techniques. The complementary features, multiple modalities, and deep learning models are fused adaptively using score-level or feature-level fusion. The advantages, features, practical applications, and limitations under each category are highlighted to address the challenges and determine future trends to counter deepfakes. In addition, recommendations are also elaborated to evaluate the potential of artificial intelligence in deepfake detection for providing a safer and more reliable digital world.
Keywords: DeepFake; Artificial intelligence; Generative adversarial network; Fusion algorithms; Transformer; Detection

Jamin Rahman Jim, Md Apon Riaz Talukder, Partha Malakar, Md Mohsin Kabir, Kamruddin Nur, M.F. Mridha,
Recent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review,
Natural Language Processing Journal,
Volume 6,
2024,
100059,
ISSN 2949-7191,
https://doi.org/10.1016/j.nlp.2024.100059.
(https://www.sciencedirect.com/science/article/pii/S2949719124000074)
Abstract: Sentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. Scrutinizing words and phrases categorizes them into positive, negative, or neutral sentiments. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative. Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. We then delved into prevalent pre-processing techniques, datasets, and evaluation metrics to enhance comprehension. We also explored Machine Learning, Deep Learning, Large Language Models and Pre-trained models in sentiment analysis, providing insights into their advantages and drawbacks. Subsequently, we precisely reviewed the experimental results and limitations of recent state-of-the-art articles. Finally, we discussed the diverse challenges encountered in sentiment analysis and proposed future research directions to mitigate these concerns. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.
Keywords: Sentiment classification; Text classification; Natural language processing; Emotion detection; Sentiment analysis

Mark S. Nixon, Alberto S. Aguado,
Chapter 5 - Low-level feature extraction (including edge detection)∗,
Editor(s): Mark S. Nixon, Alberto S. Aguado,
Feature Extraction and Image Processing for Computer Vision (Fifth Edition),
Academic Press,
2026,
Pages 213-304,
ISBN 9780443366864,
https://doi.org/10.1016/B978-0-443-36686-4.00013-4.
(https://www.sciencedirect.com/science/article/pii/B9780443366864000134)
Abstract: We shall define low-level features to be those basic features that can be extracted automatically from an image without any shape information (information about spatial relationships). As such, thresholding is actually a form of low-level feature extraction performed as a point operation. Naturally, all of these approaches can be used in high-level feature extraction, where we find shapes in images. There are very basic techniques and more advanced ones, and we shall look at some of the most popular approaches. The first-order detectors are equivalent to first-order differentiation and, naturally, the second-order edge detection operators are equivalent to a one-higher level of differentiation. An alternative form of edge detection is called phase congruency, and we shall again see the frequency domain used to aid analysis; this time for low-level feature extraction. We shall also consider corner detection, which can be thought of as detecting those points where lines bend very sharply with high curvature and saliency, which are important points. These are the other low-level features that again can be extracted automatically from the image. Finally, we shall investigate techniques that describe motion, called optical flow.
Keywords: Canny; Context aware saliency; Corner detection; Correlation; Curvature; DeepFlow; Differential approach; Edge detection; FAST; First and second order operators; Harris; Implementation; Laplacian of Gaussian; Marr-Hildreth; Optical flow; ORB; Phase congruency; Prewitt; Roberts; Saliency; SIFT; Sobel; SURF; Velocity and acceleration; Window size; Zero-crossing detection

Maged Nasser, Noreen Izza Arshad, Abdulalem Ali, Hitham Alhussian, Faisal Saeed, Aminu Da'u, Ibtehal Nafea,
A systematic review of multimodal fake news detection on social media using deep learning models,
Results in Engineering,
Volume 26,
2025,
104752,
ISSN 2590-1230,
https://doi.org/10.1016/j.rineng.2025.104752.
(https://www.sciencedirect.com/science/article/pii/S2590123025008291)
Abstract: The volume of data circulating from online sources is growing rapidly and comprises both reliable and unreliable information published through many different sources. Researchers are making plausible efforts to develop reliable methods for detecting and eliminating fake web news. Deep learning (DL) methods play a vital role in addressing various fake news detection problems and are found to perform better compared to conventional approaches, making them state-of-the-art in this field. This paper provides a comprehensive review and analysis of existent DL-based models for multimodal fake news detection, focusing on diverse aspects, including user profiles, news content, images, videos, and audio data. This study considered the latest articles within the last seven years, starting from 2018 to 2025, and about 963 quality articles were obtained from the journals and conferences selected for this study. Subsequently, 121 studies were chosen for our SLR after careful screening of the abstract and the full-text eligibility analysis. The findings showed that the Transformer models and Recurrent Neural Networks (RNNs) are the most popular deep learning techniques for detecting multimodal fake news, followed by the Convolutional Neural Networks (CNNs) techniques. The Twitter and Weibo datasets are the two most frequently used standard datasets, and the most frequently used metrics to evaluate the performance of these models are the accuracy, precision, recall, and F-scores. In conclusion, the limitations of the current methods were summarized and some exciting possibilities for future research were highlighted, including designing robust multilingual fake news detection systems, hybridization of deep learning models to enhance detection accuracy, integration of explainable AI (XAI), and facilitating real-time fake news detection models.
Keywords: Multimodal fake news detection; Deep learning models; Transformers; Recurrent neural network (RNN); Convolutional neural networks (CNNs); Autoencoder (AE)

Jabbar Hussain, Magnus Båth, Jonas Ivarsson,
Generative adversarial networks in medical image reconstruction: A systematic literature review,
Computers in Biology and Medicine,
Volume 191,
2025,
110094,
ISSN 0010-4825,
https://doi.org/10.1016/j.compbiomed.2025.110094.
(https://www.sciencedirect.com/science/article/pii/S0010482525004457)
Abstract: Purpose
Recent advancements in generative adversarial networks (GANs) have demonstrated substantial potential in medical image processing. Despite this progress, reconstructing images from incomplete data remains a challenge, impacting image quality. This systematic literature review explores the use of GANs in enhancing and reconstructing medical imaging data.
Method
A document survey of computing literature was conducted using the ACM Digital Library to identify relevant articles from journals and conference proceedings using keyword combinations, such as “generative adversarial networks or generative adversarial network,” “medical image or medical imaging,” and “image reconstruction.”
Results
Across the reviewed articles, there were 122 datasets used in 175 instances, 89 top metrics employed 335 times, 10 different tasks with a total count of 173, 31 distinct organs featured in 119 instances, and 18 modalities utilized in 121 instances, collectively depicting significant utilization of GANs in medical imaging. The adaptability and efficacy of GANs were showcased across diverse medical tasks, organs, and modalities, utilizing top public as well as private/synthetic datasets for disease diagnosis, including the identification of conditions like cancer in different anatomical regions. The study emphasized GAN's increasing integration and adaptability in diverse radiology modalities, showcasing their transformative impact on diagnostic techniques, including cross-modality tasks. The intricate interplay between network size, batch size, and loss function refinement significantly impacts GAN's performance, although challenges in training persist.
Conclusions
The study underscores GANs as dynamic tools shaping medical imaging, contributing significantly to image quality, training methodologies, and overall medical advancements, positioning them as substantial components driving medical advancements.
Keywords: Generative adversarial networks; Image reconstruction; Medical image processing; Deep learning; Modality

Lazarus Kwao, Jing Ma, Sophyani Banaamwini Yussif, Matthew Quayson,
MCDF: Multimodal information fusion and causal analysis for election misinformation detection,
Information Fusion,
Volume 125,
2026,
103470,
ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2025.103470.
(https://www.sciencedirect.com/science/article/pii/S1566253525005433)
Abstract: The rapid spread of election-related misinformation on social media poses a serious threat to public trust, democratic decision-making, and social stability. This form of misinformation is particularly persuasive and difficult to detect as it uses different types of content (modalities), including text, images, captions, and social interactions. These challenges undermine efforts to ensure trustworthy elections and enable timely intervention by policymakers and fact-checkers. However, existing detection approaches struggle with feature misalignment, cross-modal inconsistencies, and noisy social data, thereby limiting their ability to accurately classify misinformation and explain its propagation. To address these challenges, we propose MCDF, a Multimodal Causal Detection Framework, integrating fusion-driven misinformation detection with causal analysis. Our framework consists of three key components: (1) a multimodal rumor detection module, which employs Graph Convolutional Networks (GCNs) for social interaction modeling, Vision Transformers (ViTs) for visual feature extraction, and RoBERTa for text-caption encoding, dynamically aligned via Tensor Fusion Networks (TFNs); (2) a Noise-Gating Mechanism, which refines feature alignment by filtering misleading or redundant inputs, ensuring robust misinformation classification; and (3) DEMATEL, a causal inference module that quantifies misinformation drivers, bridging misinformation classification with explainability. We evaluate our model on Twitter (X), FakeNewsNet (GossipCO and PolitiFact), and a curated Ghana-specific election dataset, demonstrating state-of-the-art performance in both classification and causal inference. MCDF offers a practical and interpretable framework for combating misinformation in real-world political communication, providing actionable insights for electoral stakeholders, fact-checkers, and social media analysts.
Keywords: Multimodal rumor detection; Causal analysis; DEMATEL; African election misinformation; Noise-Gating Mechanisms; Tensor Fusion Networks; Ghana elections

Tahereh Saheb, Mouwafac Sidaoui, Bill Schmarzo,
Convergence of artificial intelligence with social media: A bibliometric & qualitative analysis,
Telematics and Informatics Reports,
Volume 14,
2024,
100146,
ISSN 2772-5030,
https://doi.org/10.1016/j.teler.2024.100146.
(https://www.sciencedirect.com/science/article/pii/S277250302400032X)
Abstract: The integration of artificial intelligence (AI) and social media has provided numerous benefits to businesses, including improved audience analysis and content optimization. However, AI has facilitated the spread of misinformation, emphasizing the importance of taking a balanced approach that considers both the technology's positive applications and its ethical risks. This paper looks at the intersection of AI and social media. The researchers use a mixed-method approach to analyze 1540 scholarly documents, combining bibliometric and systematic literature review techniques. The goal of this research is to identify the most important topics and trends, as well as potential business values and implications, in the AI Social Media domain. The first stage of the research involved a quantitative keyword co-occurrence analysis, which resulted in the identification of ten dominant themes. These include Conversational Agents & User Experience, Human Emotion and Content Recommendation & Moderation, Collective Intelligence in Emergency Management, Algorithmic Activism on social media, Deep Fakes and Fake News, Generative Artificial Intelligence, Algorithmic Bias in Content Moderation Systems, Deep Sentiment Analysis, Metaverse Technologies, and NLP & Mental Health Detection. Each identified theme is then subjected to a qualitative thematic literature review, which provides a more in-depth, context-specific understanding of the associated findings. Because of this comprehensive approach, the study provides a broad overview of the current state of AI social media, shedding light on the potential applications and far-reaching implications of this interdisciplinary nexus. The study's findings have the potential to shape strategic decision-making, policy development, and future research directions in this rapidly changing field.
Keywords: Social media; Artificial intelligence; Bibliometric analysis; Qualitative research

Grzegorz Chmielarz,
Deepfakes, a blessing in/or disguise – challenges of applying Generative AI tools,
Procedia Computer Science,
Volume 270,
2025,
Pages 6278-6287,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.10.098.
(https://www.sciencedirect.com/science/article/pii/S1877050925034313)
Abstract: The primary goal of the article is to analyse the Generative Artificial Intelligence (Gen AI tools) with respect to their potential utility level for legitimate purposes as well as malicious use in the cybersecurity area. With this end in view, the author summarises the benefits and drawbacks of the deepfake technology in an attempt to determine whether the overall evaluation of the phenomenon should be positive or negative. The paper presents potential areas for legitimate use of deepfakes confronted with examples of the negative impact of the manipulated content on the area of cybersecurity. In addition, the author also classifies deepfakes into two categories depending on their type and utility level. In the part devoted to cybersecurity threats risks for the organisational cybersecurity area have been summarised, social engineering techniques used in deepfake campaigns presented and remedial actions to curb the dissemination of deepfakes proposed. The paper’s main objective is to answer the two research questions formulated: RQ 1: Can online users benefit from the deepfake technology? and RQ 2: What actions are required to mitigate the harmful impact of deepfakes on the cybersecurity area? The paper finishes with the conclusion and indication of potential further research avenues.
Keywords: cyberthreats; cybersecurity; Generative Artificial Intelligence; Deepfake

Soundarya B C, Gururaj H L, Naveen Kumar C M,
A Framework for Deepfake Detection using Convolutional Neural Network and Deep Features,
Procedia Computer Science,
Volume 258,
2025,
Pages 3640-3648,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.04.619.
(https://www.sciencedirect.com/science/article/pii/S1877050925017235)
Abstract: With the advancement of Artificial Intelligence, facial recognition has become a crucial biometric feature. Deepfake technology leverages AI and can create hyper-realistic digitally manipulated videos of people appearing to say or do things that never occurred. The emergence of Generative Adversarial Networks (GANs) has further enabled the creation of fake visual content with astonishing realism. This technology has diverse applications, such as in the film industry, where it allows for video recreation without reshooting, creating awareness videos, restoring the voices of those who have lost them, and updating movie scenes at low cost. However, this rapid advancement also presents significant challenges. The proliferation of synthetic images raises severe concerns about their societal impact, particularly in terms of potential misuse for harassment and blackmail. Therefore, developing robust deepfake detection models is imperative. This study evaluates the performance of a proposed ResNet34 model in deepfake detection. We utilize the FaceForensics++ dataset to train and assess the model, incorporating images generated by four popular deepfake techniques. Our experimental results demonstrate that integrating linear ternary patterns (LTP) and edge detection-based features with the modified ResNet34 model achieves superior performance, attaining 97.5% accuracy and surpassing other approaches.
Keywords: Deepfake; Artificial intelligence; Deep learning

Shubham Sharma, Arvind Selwal,
Potential of artificial intelligence in deepfake media: From generation to detection mechanisms, state-of-the-art, and challenges,
Computer Science Review,
Volume 60,
2026,
100866,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2025.100866.
(https://www.sciencedirect.com/science/article/pii/S157401372500142X)
Abstract: Artificial intelligence (AI) plays an important role in the generation of deepfakes by leveraging advanced machine learning models to create hyper-realistic synthetic media across visual, audio, and multimodal formats. The rapid evolution of deepfake technologies, alongside the exponential growth of digital media, demands a comprehensive and critical examination of current capabilities and challenges. Although the concept of media manipulation is not new, the sophistication and accessibility of AI-driven deepfakes present significant threats of misinformation to society and hence cause societal manipulation. This manuscript presents a systematic review of deepfake generation and detection techniques from 2017 to 2025, highlighting the progression of generative models and evaluating detection strategies. The main focus of this work is on the state-of-the-art (SOTA) techniques using adversarial networks, vision transformers (ViTs), attention mechanisms, hybrid learning frameworks, and ensemble models. The study thoroughly examines the benefits and drawbacks of existing methods. It also points out how vulnerable detection systems are to adversarial attacks and compares modern methods with traditional forensic and heuristic approaches. The paper critically analyzes the strengths and limitations of existing models, underscores the susceptibility of detection systems to adversarial attacks, and contrasts contemporary approaches with traditional forensic and heuristic-based methods. In addition to technical insights, the review puts a major focus on practical concerns such as scalability, regulatory frameworks, and the broader societal impact of the deepfake technology. By including benchmark datasets, standard tools, performance evaluation metrics, and relevant policy discussions, the manuscript presents a forward-looking perspective on the ongoing arms race between deepfake generation and detection. The study ends by highlighting the need for strong, flexible, and understandable detection systems, backed by effective policy measures, to reduce the growing risks posed by deepfakes.
Keywords: Artificial intelligence; Deep learning; Deepfakes; Deepfake detection; Generative adversarial networks (GAN); Autoencoders; Vision transformers (ViT); Explainable AI

Domna Bilika, Nikoletta Michopoulou, Efthimios Alepis, Constantinos Patsakis,
Hello me, meet the real me: Voice synthesis attacks on voice assistants,
Computers & Security,
Volume 137,
2024,
103617,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103617.
(https://www.sciencedirect.com/science/article/pii/S0167404823005278)
Abstract: The radical advances in telecommunications and computer science have enabled a myriad of applications and novel seamless interactions with computing interfaces. Voice Assistants (VAs) have become the norm for smartphones, and millions of VAs incorporated in smart devices are used to control these devices in the smart home context. Previous research has shown that they are prone to attacks, leading vendors to implement countermeasures. One of these measures is to allow only a specific individual, the device's owner, to perform potentially dangerous tasks that may disclose personal information, involve monetary transactions, etc. To understand the extent to which VAs provide the necessary protection to their users, we experimented with two of the most widely used VAs, which the participants trained. We then utilised voice synthesis, using samples provided by participants, to synthesise commands that were used to trigger the corresponding VA and perform a dangerous task. Our extensive results showed that more than 30% of our audio synthesis attacks were successful and at least one successful attack for more than half of the participants. Moreover, they illustrate statistically significant variation among vendors and, in one case, even gender bias. The outcomes are rather alarming and require the deployment of further countermeasures to prevent exploitation, as the number of VAs in use is currently comparable to the world population.
Keywords: Voice assistants; Voice synthesis; Android; IOS; Security; Synthesised voice

Gaurav Kumar, Chhavi Dhiman,
Decoding fake news fabrications and trends: A comprehensive survey,
Neurocomputing,
Volume 653,
2025,
131118,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2025.131118.
(https://www.sciencedirect.com/science/article/pii/S0925231225017904)
Abstract: Increased internet access has led to a surge in online content across blogs, websites, news portals, and social media, where people actively share personal ideas, opinions, ideologies while seeking information of their interest. However, relying on individual sources can lead to information overload and the spread of unverified data, often shaped by personal biases. This lack of fact-based reliability fuelled the generation and spread of fake news, undermining trust in digital information ecosystems. To tackle these challenges, Fake News Detection (FND) has become a crucial research area, drawing significant attention of experts to develop solutions to combat misinformation and restore trust in online information. This paper provides a comprehensive review of the changing patterns of fake news trends over time, tracing its shift from text to visual and eventually hybrid formats over the past decade. It reviews the generation and propagation of fake news, explores detection methods and highlights the challenges for efficient detection, including how human and algorithmic bias unknowingly contributes to its spread. The paper discusses key research questions and their implications, emphasizing why multimodal sentiment analysis outperforms other methods for detecting complex, malicious intent. It also provides an overview of popular datasets and resources, along with a bibliometric analysis highlighting key authors and leading institutes in the research area. Finally, it discusses the future direction of fake news detection, underscoring the need for continuous advancements in this rapidly evolving domain.
Keywords: News fabrication; Fake news generation (FNG); Fake news propagation (FNP); Fake news detection (FND); Early detection; Changing trends; Social-media; Misinformation; Disinformation; Malinformation; LLMs; XAI; DeepFake

Rene J. Herrera, Ralph Garcia-Bertrand,
Chapter 10 - Artificial intelligence,
Editor(s): Rene J. Herrera, Ralph Garcia-Bertrand,
The Future of Human Evolution,
Academic Press,
2026,
Pages 407-441,
ISBN 9780443289194,
https://doi.org/10.1016/B978-0-443-28919-4.00002-7.
(https://www.sciencedirect.com/science/article/pii/B9780443289194000027)
Abstract: In many cases, humans no longer live within the confines of natural selection, and instead under artificial selection, by changing their environment through technology. Human evolution occurs slowly, and changes have small or imperceptible effects on the performance of the population, while the evolution of science and technology occurs on a faster time scale with change evident within generations. Artificial intelligence (AI) has become an integral part of human lives and has grown to be a significant force in every sector of the world including politics, science, economics, agriculture, healthcare, entertainment, education, and research. AI advances could create new opportunities and enhance people’s lives; however, for various reasons, unregulated AI is becoming a concern. Human interactions with AI are changing the way we interact socially, with nature, and the way we interact with other technology. Because the use of AI has become a common cultural practice, it could play a significant role in the future of human gene-culture coevolution. The role of AI culture could influence evolution in a variety of ways including AI-associated stress and behavior influencing epigenetics, AI human enhancement, use of robotics, AI’s effect on autonomy, security and equality, AI human symbiosis, and AI’s effect on mate selection. As AI continues to advance, regulation will be needed to ensure the maximum benefits and the avoidance of harm.
Keywords: Artificial intelligence; Machine learning; Deep learning; Human enhancement; Epigenetics; Mate selection

Ehsan Nowroozi, Ali Dehghantanha, Reza M. Parizi, Kim-Kwang Raymond Choo,
A survey of machine learning techniques in adversarial image forensics,
Computers & Security,
Volume 100,
2021,
102092,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2020.102092.
(https://www.sciencedirect.com/science/article/pii/S0167404820303655)
Abstract: Image forensic plays a crucial role in both criminal investigations (e.g., dissemination of fake images to spread racial hate or false narratives about specific ethnicity groups or political campaigns) and civil litigation (e.g., defamation). Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches (e.g., how to detect adversarial (image) examples), and there are associated real-world consequences (e.g., inadmissible evidence, or wrongful conviction). Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios.
Keywords: Image forensics; Adversarial machine learning; Adversarial learning; Adversarial setting; Image manipulation detection; Cyber security

Souvik Chowdhury, Badal Soni,
ENVQA: Improving Visual Question Answering model by enriching the visual feature,
Engineering Applications of Artificial Intelligence,
Volume 142,
2025,
109948,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2024.109948.
(https://www.sciencedirect.com/science/article/pii/S0952197624021079)
Abstract: Visual Question Answering (VQA) is pivotal in various industries, including medicine. Current approaches typically rely on identifying patterns between image regions and questions, using attention-learning techniques to highlight essential information and suppress noise. However, existing VQA systems often overlook crucial foreground and background-related features in images, limiting their ability to tackle complex questions effectively. Most VQA models employ either spatial or channel attention mechanisms. Spatial attention localizes the region of interest (ROI) but may overlook global semantic relationships between salient objects. Conversely, channel attention enhances feature representation but disregards spatial dynamics within images. To address these limitations, we propose ”ENVQA” (Enriching V in VQA), a novel VQA model that integrates enriched visual features by leveraging both spatial and object-level features, alongside spatial and channel attention networks. Our model aims to enhance understanding by capturing both local and global contexts within images. Experimental evaluations on benchmark datasets such as VQA 2.0, TDIUC, and GQA demonstrate that ENVQA outperforms state-of-the-art (SOTA) models utilizing attention mechanisms.
Keywords: Visual Question Answering; Computer vision; Natural language processing; Visual and language

Mohd Tahir Irfan, Bhavna Arora, Neha Sandotra, Abrar Ahmed Raza,
On Machine Learning and Deep Learning based Deepfake Generation and Detection,
Procedia Computer Science,
Volume 259,
2025,
Pages 1927-1936,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.04.148.
(https://www.sciencedirect.com/science/article/pii/S1877050925012505)
Abstract: With the advancement of artificial intelligence, deepfakes have evolved into a potent tool that allows the developer to manipulate images or audios that can lead to defamation or any other kind of security threat. It is a cutting-edge technology that uses deep learning and machine learning techniques which gives the user enormous power to create deep fake media for both entertainment and malicious purposes that may result in high impact in real life scenarios. Hence, recently the research communities have been increasingly interested in the development of approaches for detecting deepfakes as the trust on the media available online comes under dilemma. In this paper, a comprehensive overview of deepfake technology with its pros and cons, followed by deepfake generation methods like Encoder-Decoder and GAN is discussed. The benchmark datasets with the open-source tools for deepfake generation have also been discussed in detail. How the face manipulation techniques like Face-Swap, Face-Synthesis, Face- Attribute-Manipulation and Face-Re-enactment are used is also a part of this study. Additionally, it offers a comparison of past research on the identification of deepfake images and videos which are applying deep-learning and machine-learning algorithms. The research gaps of this technology and how this can be implemented for further research, perspective, and insights of the same have also been given. An evaluation on the machine-learning and deep-learning based detection models for fake images, videos, audios, and multimodal content has also been explored and presented in this paper.
Keywords: Deepfake Generation; Deepfake Detection; Machine Learning; Deep Learning

Usha Kosarkar, Gopal Sarkarkar, Shilpa Gedam,
Revealing and Classification of Deepfakes Video's Images using a Customize Convolution Neural Network Model,
Procedia Computer Science,
Volume 218,
2023,
Pages 2636-2652,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2023.01.237.
(https://www.sciencedirect.com/science/article/pii/S1877050923002375)
Abstract: Deepfake has been exploited in recent years despite its widespread usage in a variety of areas to create dangerous material such as fake movies, rumors, and false news by changing or substituting the face information of the sources and so poses enormous security concerns to society. Research on active detection & prevention technologies is critical as deepfake continues to evolve. Deepfake has been a blessing, but we've taken advantage of it by utilizing it to swap faces. Deepfake is a new subdomain of Artificial Intelligence (AI) technology in which one person's face is layered over another person's face, which is becoming more and more popular on social networking sites. Deepfake pictures and videos can now be created much more quickly and cheaply due to ML (Machine Learning), which is a primary component of deepfakes. Despite negative connotations attached to the term "deepfakes," technology is increasingly being used in commercial & individual contexts. New technical advancements have made it more difficult to distinguish between deepfakes and images that have been digitally manipulated. The rise of deepfake technologies has sparked a growing sense of unease. The primary goal of this project is to properly distinguish deepfake pictures from real images using deep learning techniques. In this study, we implemented a customized CNN algorithm to identify deepfake pictures from a video dataset and conducted a comparative analysis with two other methods to determine which way was superior. The Kaggle dataset was used to train & test our model. Convolutional neural networks (CNNs) have been used in this research to distinguish authentic & deepfake images by training three distinct CNN models. A customized CNN model, which includes several additional layers such as a dense layer, MaxPooling, as well as a dropout layer, has also been developed and implemented. This method follows the frames extraction, face feature extraction, data preprocessing, and classification phases in determining whether Real or Fake images in the video reflect the objectives. Accuracy, loss, and the area under the receiver operating characteristic (ROC) curve were used to characterize the data. Customized CNN outperformed all other models, achieving 91.4% accuracy, a reduced loss value of 0.342, as well as an AUC of 0.92. Besides, we obtained 85.2% testing accuracy from the CNN and 95.5% testing accuracy from the MLP-CNN model.
Keywords: Deepfake detection; Deep learning; Customize CNN; Deepfake Detection Challenge Dataset; Classification

Hsin-Hsuan Chung, Jiangping Chen,
Misinformation detection: datasets, models and performance,
Online Information Review,
Volume 49, Issue 3,
2024,
Pages 570-584,
ISSN 1468-4527,
https://doi.org/10.1108/OIR-06-2024-0388.
(https://www.sciencedirect.com/science/article/pii/S146845272400012X)
Abstract: Purpose
This paper aims to understand the characteristics of current misinformation detection studies, including the datasets used by researchers, the computational models or algorithms being developed or applied, and the performance of misinformation detection models or algorithms.
Design/methodology/approach
We first identified articles from the Scopus database with inclusion and exclusion criteria. Then a coding scheme was derived from the articles based on research questions. Next, datasets, models, and performance were coded. The paper concluded with answers to research questions and future research directions.
Findings
From 115 relevant articles published during 2019–2023 on misinformation detection. We found that most studies used previously existing datasets. Twitter (now X) has been the most widely used source for collecting social media misinformation data. The ten most frequently used datasets are identified. Most studies (96.1%) developed or applied machine learning, especially deep learning models. The most advanced current misinformation detection models could achieve pretty high performance. For example, among 104 studies reporting performance with accuracy, 44.2% achieved an accuracy of 0.95 or higher, and 24.0% achieved 0.90–0.94 on accuracy.
Research limitations/implications
Our study only reviewed English articles from 2019–2023 that are included in the Scopus database. Articles that are not included in the Scopus database are not reviewed.
Practical implications
The high performance of misinformation detection indicates that social media should be able to detect most misinformation if they are willing to do it. However, no system or algorithm could achieve 100% misinformation on performance. Due to the complexity of misinformation, users of social media still need to improve their capabilities of evaluating information on the Internet.
Social implications
This study provides evidence to policymakers that social media platforms have the capability of detecting most misinformation posted. These platforms are responsible for alerting to suspicious postings with misinformation.
Originality/value
This study identifies datasets, computer models, and performance of models from current misinformation detection research. The findings will help social media companies, computer scientists, and information system designers improve their misinformation detection systems. It will also help students in information science and computer science to study the latest models and algorithms. Information professionals may work with computer scientists to improve datasets used for misinformation detection.
Keywords: Misinformation detection; Datasets; Machine learning; Deep learning; Algorithm evaluation

Amina Adadi, Mohammed Lahmer, Samia Nasiri,
Artificial Intelligence and COVID-19: A Systematic umbrella review and roads ahead,
Journal of King Saud University - Computer and Information Sciences,
Volume 34, Issue 8, Part B,
2022,
Pages 5898-5920,
ISSN 1319-1578,
https://doi.org/10.1016/j.jksuci.2021.07.010.
(https://www.sciencedirect.com/science/article/pii/S1319157821001774)
Abstract: Artificial Intelligence (AI) has played a substantial role in the response to the challenges posed by the current pandemic. The growing interest in using AI to handle Covid-19 issues has accelerated the pace of AI research and resulted in an exponential increase in articles and review studies within a very short period of time. Hence, it is becoming challenging to explore the large corpus of academic publications dedicated to the global health crisis. Even with the presence of systematic review studies, given their number and diversity, identifying trends and research avenues beyond the pandemic should be an arduous task. We conclude therefore that after the one-year mark of the declaration of Covid-19 as a pandemic, the accumulated scientific contribution lacks two fundamental aspects: Knowledge synthesis and Future projections. In contribution to fill this void, this paper is a (i) synthesis study and (ii) foresight exercise. The synthesis study aims to provide the scholars a consolidation of findings and a knowledge synthesis through a systematic review of the reviews (umbrella review) studying AI applications against Covid-19. Following the PRISMA guidelines, we systematically searched PubMed, Scopus, and other preprint sources from 1st December 2019 to 1st June 2021 for eligible reviews. The literature search and screening process resulted in 45 included reviews. Our findings reveal patterns, relationships, and trends in the AI research community response to the pandemic. We found that in the space of few months, the research objectives of the literature have developed rapidly from identifying potential AI applications to evaluating current uses of intelligent systems. Only few reviews have adopted the meta-analysis as a study design. Moreover, a clear dominance of the medical theme and the DNN methods has been observed in the reported AI applications. Based on its constructive systematic umbrella review, this work conducts a foresight exercise that tries to envision the post-Covid-19 research landscape of the AI field. We see seven key themes of research that may be an outcome of the present crisis and which advocate a more sustainable and responsible form of intelligent systems. We set accordingly a post-pandemic research agenda articulated around these seven drivers. The results of this study can be useful for the AI research community to obtain a holistic view of the current literature and to help prioritize research needs as we are heading toward the new normal.
Keywords: Artificial Intelligence; Covid-19; Machine learning; Deep learning; Robotic; Umbrella review; Foresight analysis

Mekhail Mustak, Joni Salminen, Matti Mäntymäki, Arafat Rahman, Yogesh K. Dwivedi,
Deepfakes: Deceptions, mitigations, and opportunities,
Journal of Business Research,
Volume 154,
2023,
113368,
ISSN 0148-2963,
https://doi.org/10.1016/j.jbusres.2022.113368.
(https://www.sciencedirect.com/science/article/pii/S0148296322008335)
Abstract: Deepfakes—artificial but hyper-realistic video, audio, and images created by algorithms—are one of the latest technological developments in artificial intelligence. Amplified by the speed and scope of social media, they can quickly reach millions of people and result in a wide range of marketplace deceptions. However, extant understandings of deepfakes’ implications in the marketplace are limited and fragmented. Against this background, we develop insights into the significance of deepfakes for firms and consumers—the threats they pose, how to mitigate those threats, and the opportunities they present. Our findings indicate that the main risks to firms include damage to image, reputation, and trustworthiness and the rapid obsolescence of existing technologies. However, consumers may also suffer blackmail, bullying, defamation, harassment, identity theft, intimidation, and revenge porn. We then accumulate and present knowledge on the strategies and mechanisms to safeguard against deepfake-based marketplace deception. Furthermore, we uncover and report the various legitimate opportunities offered by this new technology. Finally, we present an agenda for future research in this emergent and highly critical area.
Keywords: Deepfake; Fake photo; Fake video; Artificial intelligence; Machine learning; Deception; Opportunities; Threats; Challenges; Protection; Marketing

Marco Tanfoni, Elia Giuseppe Ceroni, Niccolò Pancino, Monica Bianchini, Marco Maggini,
Facial Segmentation in Deepfake Classification: a Transfer Learning Approach,
Procedia Computer Science,
Volume 246,
2024,
Pages 4160-4168,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2024.09.255.
(https://www.sciencedirect.com/science/article/pii/S1877050924022749)
Abstract: Artificial Intelligence (AI)–generated images represent a significant threat in various fields, such as security, privacy, media forensics and content moderation. In this paper, a novel approach for the detection of StyleGAN2–generated human faces is presented, leveraging a Transfer Learning strategy to improve the Classification performance of the models. A modified version of the state– of–the–art semantic segmentation model DeepLabV3+, using either a ResNet50 or a MobileNetV3 Large as feature extraction backbones, is used to create both a face segmentation model and the synthetic image detector. To achieve this goal, the models are at first trained for face segmentation in a multi–class Classification task on a widely used semantic segmentation dataset, achieving remarkable results for both configurations. Then, the pre–trained models are retrained on a collection of real and generated images, gathered from different sources to solve a binary Classification task, namely to detect synthetic (i.e. generated) images, thus carrying out two different transfer learning strategies. The results indicate that this targeted methodology significantly improves the detection rates compared to analyzing the face as a whole, and underlines the importance of advanced image recognition technologies when tackling the challenge of detecting generated faces.
Keywords: Fake detection; Image Authentication; Synthetic Image Detection; Image segmentation; Transfer learning; DeepLabV3+; MobileNetV3; ResNet; Digital Forensics; Machine Learning; Computer Vision

Diya Garg, Rupali Gill,
Unmasking Deepfakes: A Review of Current Datasets, Tools, and Detection Features,
Procedia Computer Science,
Volume 259,
2025,
Pages 1737-1748,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.04.129.
(https://www.sciencedirect.com/science/article/pii/S1877050925012311)
Abstract: Deepfake technology is a new way to alter digital content and create videos that look very real. The responsible use of deepfake technology is essential, as its inappropriate application can lead to significant consequences, from harming individual’s reputations to influencing public opinion. Nowadays, this technology is being misused for spreading false information or deceiving people as well, making it crucial to develop an effective method for the detection of synthetic media. The current research focuses on various aspects such as datasets, features, tools, and techniques used in field of deepfake detection. Further investigation of gaps like lack of multi-modal approach, less work on hybrid models, and unseen datasets associated with current research work has also been done. The existing deep learning models being used for deepfake detection faces several challenges. There is no such model that works well with the different types of datasets. Also, the methods used to create deepfakes are changing quickly, making it even more difficult for existing detection models to obtain better performance. In order to overcome the challenges, it is proposed to design a hybrid learning framework for deepfake detection using a multi-modal approach.
Keywords: Deepfake; Detection; Deep learning; Fake; Video forgery; Image forgery

Abdul Rehman Javed, Zunera Jalil, Wisha Zehra, Thippa Reddy Gadekallu, Doug Young Suh, Md. Jalil Piran,
A comprehensive survey on digital video forensics: Taxonomy, challenges, and future directions,
Engineering Applications of Artificial Intelligence,
Volume 106,
2021,
104456,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2021.104456.
(https://www.sciencedirect.com/science/article/pii/S0952197621003043)
Abstract: With the explosive advancements in smartphone technology, video uploading/downloading has become a routine part of digital social networking. Video contents contain valuable information as more incidents are being recorded now than ever before. In this paper, we present a comprehensive survey on information extraction from video contents and forgery detection. In this context, we review various modern techniques such as computer vision and different machine learning (ML) algorithms including deep learning (DL) proposed for video forgery detection. Furthermore, we discuss the persistent general, resource, legal, and technical challenges, as well as challenges in using DL for the problem at hand, such as the theory behind DL, CV, limited datasets, real-time processing, and the challenges with the emergence of ML techniques used with the Internet of Things (IoT)-based heterogeneous devices. Moreover, this survey presents prominent video analysis products used for video forensics investigation and analysis. In summary, this survey provides a detailed and broader investigation about information extraction and forgery detection in video contents under one umbrella, which was not presented yet to the best of our knowledge.
Keywords: Digital forensics; Anti-forensics; Machine learning (ML); Deep learning (DL); Computer vision (CV); Video forensics; Video forgery; Evidence extraction; Forgery detection; Legal aspects

Shubham Sharma, Arvind Selwal,
Improved Deepfake Detection with Optimized Preprocessing for Low-Quality Images,
Procedia Computer Science,
Volume 258,
2025,
Pages 507-516,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.04.286.
(https://www.sciencedirect.com/science/article/pii/S1877050925013882)
Abstract: Deepfake detection has been one of the fastest-growing areas of research due to the increasing threat manipulated media is posing to the world. One of the major challenges still underlying this problem, it remains poorly researched how to detect deepfakes in low-quality images[1]. In this paper, a framework is proposed to enhance the Mesonet model in the detection of deepfakes using the implementation of a preprocessing optimization layer. This layer refines the quality of input images through an enhanced preprocessing layer. In our experiments, deepfake images were generated from a DCGAN model trained on the CelebA dataset, simulating the real-world scenarios of low-quality deepfakes. Our framework was tested on a dataset comprising 1,000 low-quality real images and 1,000 low-quality deepfake images, also derived from the CelebA dataset. Accuracy was found to increase from 69.40% to 92.50% in the optimized model. Moreover, the area under the ROC curve increased from 0.75—75% to 0.96—96%, also improving the model’s discriminatory power very much. These results shed light on the proportion of preprocessing-related optimizations for performance improvement in deepfake detection. This work has contributed to the deepfake detection methodology with robust ways to improve model performance under very challenging conditions. While the research has significantly enhanced the detection which can be seen when dealing with low quality-wire etc., there are still some issues associated with the complexity of the approach. This extra requirement makes the demand for additional computing more critical. Intuitively, the researchers could direct future works towards seeking more flexible and efficient optimization without sacrificing the desirable accuracy and the ability to use limited resources.
Keywords: Deepfake Detection; Low-Quality Images; Mesonet Model; DCGAN; Image Enhancement; Deep Learning

 Preeti, Manoj Kumar, Hitesh Kumar Sharma,
A GAN-Based Model of Deepfake Detection in Social Media,
Procedia Computer Science,
Volume 218,
2023,
Pages 2153-2162,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2023.01.191.
(https://www.sciencedirect.com/science/article/pii/S1877050923001916)
Abstract: DeepFake uses Generative + Adversarial Network for successfully switching the identities of two people. Large public databases and deep learning methods are now rapidly available because of the proliferation of easily accessible tools online. It has resulted in the emergence of very real appealing fake content that produced a bad impact and challenges for society to deal. Pre-trained generative adversarial networks (GANs) that can flawlessly substitute one person's face in a video or image for that other are proving supportive for implementing deepfake. This paper primarily presented a study of methods used to implement deepfake. Also, discuss the main deepfake's manipulation and detection techniques, and the implementation and detection of deepfake using Deep Convolution-based GAN models. A study of Comparative analyses of proposed GAN with other exiting GAN models using parameters Inception Score “IS” and Fréchet Inception Distance “FID” is also embedded. Along with the abovementioned, the paper discusses open issues and future trends that should be considered to advance in the field.
Keywords: Digital Forensics; Image Vision; Deep Learning; Generative adversarial network; Deep Fakes; Media Forensics; Face Manipulation; Face Recognition

Tanveer Khan, Antonis Michalas, Adnan Akhunzada,
Fake news outbreak 2021: Can we stop the viral spread?,
Journal of Network and Computer Applications,
Volume 190,
2021,
103112,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103112.
(https://www.sciencedirect.com/science/article/pii/S1084804521001326)
Abstract: Social Networks' omnipresence and ease of use has revolutionized the generation and distribution of information in today's world. However, easy access to information does not equal an increased level of public knowledge. Unlike traditional media channels, social networks also facilitate faster and wider spread of disinformation and misinformation. Viral spread of false information has serious implications on the behaviours, attitudes and beliefs of the public, and ultimately can seriously endanger the democratic processes. Limiting false information's negative impact through early detection and control of extensive spread presents the main challenge facing researchers today. In this survey paper, we extensively analyze a wide range of different solutions for the early detection of fake news in the existing literature. More precisely, we examine Machine Learning (ML) models for the identification and classification of fake news, online fake news detection competitions, statistical outputs as well as the advantages and disadvantages of some of the available data sets. Finally, we evaluate the online web browsing tools available for detecting and mitigating fake news and present some open research challenges.
Keywords: Fake news; Fact checking; Machine learning; Tools; Datasets

Sakib Shahriar, Rozita Dara, Rajen Akalu,
A comprehensive review of current trends, challenges, and opportunities in text data privacy,
Computers & Security,
Volume 151,
2025,
104358,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2025.104358.
(https://www.sciencedirect.com/science/article/pii/S0167404825000471)
Abstract: The emergence of smartphones and internet accessibility around the globe have enabled billions of people to be connected to the digital world. Due to the popularity of instant messaging applications and social media, a large quantity of personal data is in text format, and processing text data in a privacy-preserving manner poses unique challenges. While existing reviews focus on privacy concerns from specific algorithmic perspectives or target only a particular domain, such as healthcare or smart metering, they fail to provide a comprehensive view that addresses the multi-layered privacy risks inherent to text data processing. Existing works often limit their scope to specialized solutions like differential privacy, anonymization, or federated learning, neglecting a broader spectrum of challenges. To fill this gap, we present a comprehensive review of privacy-enhancing solutions for text data processing in the present literature and classify the works into six categories of privacy risks: (i) unintentional memorability, (ii) membership inference, (iii) exposure and re-identification, (iv) language models and word embeddings, (v) authorship attribution, and (vi) collaborative processing. We then analyze existing privacy-enhancing solutions for text data by considering the aforementioned privacy risks. Finally, we identified several research gaps, including the need for comprehensive privacy metrics, explainable algorithms, and privacy in social media analytics.
Keywords: Privacy enhancing solutions; Text data; Natural language processing; Artificial intelligence; Machine learning; Privacy risk

Pengpeng Yang, Chen Zhou, Dasara Shullani, Lanxi Liu, Daniele Baracchi,
A Comprehensive Review on File Containers-Based Image and Video Forensics,
Computers, Materials and Continua,
Volume 85, Issue 2,
2025,
Pages 2487-2526,
ISSN 1546-2218,
https://doi.org/10.32604/cmc.2025.069129.
(https://www.sciencedirect.com/science/article/pii/S1546221825009105)
Abstract: Images and videos play an increasingly vital role in daily life and are widely utilized as key evidentiary sources in judicial investigations and forensic analysis. Simultaneously, advancements in image and video processing technologies have facilitated the widespread availability of powerful editing tools, such as Deepfakes, enabling anyone to easily create manipulated or fake visual content, which poses an enormous threat to social security and public trust. To verify the authenticity and integrity of images and videos, numerous approaches have been proposed, which are primarily based on content analysis and their effectiveness is susceptible to interference from various image or video post-processing operations. Recent research has highlighted the potential of file containers analysis as a promising forensic approach that offers efficient and interpretable results. However, there is still a lack of review articles on this kind of approach. In order to fill this gap, we present a comprehensive review of file containers-based image and video forensics in this paper. Specifically, we categorize the existing methods into two distinct stages, qualitative analysis and quantitative analysis. In addition, an overall framework is proposed to organize the exiting approaches. Then, the advantages and disadvantages of the schemes used across different forensic tasks are provided. Finally, we outline the trends in this research area, aiming to provide valuable insights and technical guidance for future research.
Keywords: Image and video forensics; file containers analysis; content analysis; Deepfakes

Sonam Singh, Amol Dhumane,
Unmasking digital deceptions: An integrative review of deepfake detection, multimedia forensics, and cybersecurity challenges,
MethodsX,
Volume 15,
2025,
103632,
ISSN 2215-0161,
https://doi.org/10.1016/j.mex.2025.103632.
(https://www.sciencedirect.com/science/article/pii/S2215016125004765)
Abstract: Deepfakes, which are driven by developments in generative AI, seriously jeopardize public trust, cybersecurity, and the veracity of information. This study offers a comprehensive analysis of the most recent methods for creating and detecting deepfakes in image, video, and audio modalities. With a focus on their advantages and disadvantages in cross-dataset and real-world scenarios, we compile the latest developments in transformer-based detection models, multimodal biometric defenses, and Generative Adversarial Networks (GANs). We provide implementation-level information such as pseudocode workflows, hyperparameter settings, and preprocessing pipelines for popular detection frameworks to improve reproducibility. We also examine the implications of cybersecurity, including identity theft and biometric spoofing, as well as policy-oriented solutions that incorporate federated learning, explainable AI, and ethical protections. By enriching technical insights with interdisciplinary perspectives, this review charts a roadmap for building robust, scalable, and trustworthy deepfake detection systems.
Keywords: Deepfake detection; Generative adversarial networks (GANs); Synthetic media, biometric spoofing; Cyber security threats; Multimedia forensics; AI policy frameworks; Explainable AI; Federated learning; Digital deception; Face synthesis; Speech cloning; Identity theft; Cross-dataset evaluation; Ethical AI

Walter Matli,
Extending the theory of information poverty to deepfake technology,
International Journal of Information Management Data Insights,
Volume 4, Issue 2,
2024,
100286,
ISSN 2667-0968,
https://doi.org/10.1016/j.jjimei.2024.100286.
(https://www.sciencedirect.com/science/article/pii/S2667096824000752)
Abstract: The advent of deepfake technology has introduced complex challenges to the information technology landscape, simultaneously presenting benefits and novel risks and ethical considerations. This paper delves into the evolution of deepfakes through the prism of information poverty theory, scrutinising how deepfakes may contribute to a growing information access/use inequality. The research focuses on the risks of misinformation and the ensuing expansion of digital divides, particularly when manipulative media could delude individuals lacking access to legitimate information sources. The study outlines the potential exacerbation of information asymmetries and examines the societal implications across various demographics. By integrating an analytical discussion on the risks associated with deepfakes, the study aligns the observed trends with the theoretical underpinnings of information poverty. As part of its contribution, the paper offers actionable policy-making recommendations and educational strategies to combat the proliferation of harmful deepfake content. The article aims to ensure a more equitable distribution of authentic information and foster media literacy. Through a multifaceted approach, this study endeavours to provide a foundational understanding for stakeholders to navigate the ethical minefield posed by deepfakes and to instil a framework for information equity in the digital era. The article provides critical insights into the discourse on deepfake technology and its relation to information poverty, underscoring the urgent need for equitable access to informed digital spaces. As deepfake technology evolves and more data emerges, a societal demand exists for comprehensive knowledge about deepfakes to promote discernment, decision-making and awareness. Policymakers are tasked with recognising the significance of widening access to sophisticated information technologies whilst addressing their negative repercussions. Their efforts will be particularly crucial for disseminating knowledge about deepfakes to those with limited or non-existent information and communication awareness and infrastructures. Learning from past successes and failures becomes pivotal in shaping effective strategies to address the challenges posed by deepfakes and fostering accessible, informed digital communities.
Keywords: Deepfake technology; Information poverty theory; Artificial intelligence (AI); Synthetic media; Societal implications; Technological advancements

Ahmad Alobaid, Talal Bonny, Maher Alrahhal,
Disruptive attacks on artificial neural networks: A systematic review of attack techniques, detection methods, and protection strategies,
Intelligent Systems with Applications,
Volume 26,
2025,
200529,
ISSN 2667-3053,
https://doi.org/10.1016/j.iswa.2025.200529.
(https://www.sciencedirect.com/science/article/pii/S2667305325000559)
Abstract: This paper provides a systematic review of disruptive attacks on artificial neural networks (ANNs). As neural networks become increasingly integral to critical applications, their vulnerability to various forms of attack poses significant security challenges. This review categorizes and analyzes recent advancements in attack techniques, detection methods, and protection strategies for ANNs. It explores various attacks, including adversarial attacks, data poisoning, fault injections, membership inference, model inversion, timing, and watermarking attacks, examining their methodologies, limitations, impacts, and potential improvements. Key findings reveal that while detection and protection mechanisms such as adversarial training, noise injection, and hardware-based defenses have advanced significantly, many existing solutions remain vulnerable to adaptive attack strategies and scalability challenges. Additionally, fault injection attacks at the hardware level pose an emerging threat with limited countermeasures. The review identifies critical gaps in defense strategies, particularly in balancing robustness, computational efficiency, and real-world applicability. Future research should focus on scalable defense solutions to ensure effective deployment across diverse ANN architectures and critical applications, such as autonomous systems. Furthermore, integrating emerging technologies, including generative AI models and hybrid architectures, should be prioritized to better understand and mitigate their vulnerabilities.
Keywords: Fault injection attacks; Adversarial attacks; Deep neural network; Machine learning; Security analysis

Md Tanvir Islam, Ik Hyun Lee, Ahmed Ibrahim Alzahrani, Khan Muhammad,
MEXFIC: A meta ensemble eXplainable approach for AI-synthesized fake image classification,
Alexandria Engineering Journal,
Volume 116,
2025,
Pages 351-363,
ISSN 1110-0168,
https://doi.org/10.1016/j.aej.2024.12.031.
(https://www.sciencedirect.com/science/article/pii/S111001682401617X)
Abstract: In the evolving landscape of artificial intelligence (AI), differentiating between authentic and artificially generated images poses a significant challenge, primarily due to the rapidly enhancing quality of AI-generated images. This paper systematically evaluates state-of-the-art classification models to distinguish authentic images from those synthetically produced using the CIFAKE dataset. We introduce FakeGPT and PFake, two new test datasets featuring genuine and AI-generated synthetic images with specific keywords paralleling the generation of the CIFAKE dataset. We use the transfer learning technique to train the state-of-the-art classification models on the CIFAKE training set, followed by rigorous evaluation against the CIFAKE, FakeGPT, and PFake test datasets. Further, we explore ensemble approaches, including stacking, voting, bagging, and meta-ensemble learning. The culmination of our extensive research efforts is the Meta Ensemble eXplainable Fake Image Classifier (MEXFIC), which stands out with a notable accuracy of 94% and 96.61% against the Stable Diffusion generated CIFAKE and PFake datasets, respectively. This is a significant improvement over the ConvNextLarge model, achieving the highest accuracy of 92.54% among the state-of-the-art models. Our study showcases the competitive edge of MEXFIC that highlights the necessity for more robust models capable of identifying AI-synthesized images, as evidenced by the performance on the challenging FakeGPT dataset.
Keywords: Fake image classification; AI-synthesized image classification; AI-generated image classification; Image authenticity verification; Smart surveillance; CIFAKE

Tuba Arif, David Camacho, Jong Hyuk Park,
Unveiling cybersecurity mysteries: A comprehensive survey on digital forensics trends, threats, and solutions in network security,
Journal of Network and Computer Applications,
Volume 243,
2025,
104296,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2025.104296.
(https://www.sciencedirect.com/science/article/pii/S1084804525001936)
Abstract: The field of digital forensics is undergoing a paradigm shift because security breaches are now occurring outside of conventional domains such as mobile devices, databases, networks, multimedia, cloud platforms, and the Internet of Things (IoT) all require a complete approach. This study report reveals a high level of ambiguities and process redundancies within the subdomains of digital forensics through the completion of a Systematic Literature Review (SLR). To address this, we suggest a high-level theoretical metamodel that unifies tasks, operations, procedures, and methods of research across many subdomains that will help forensic investigators during digital investigations to organize and integrate evidence. The study also discusses the necessity of global perspectives in research on digital forensics and provides a qualitative evaluation of past surveys, highlighting similar difficulties, obstacles, and key issues across domains, whereas earlier surveys concentrated on domains. The findings through examination offer a multidimensional knowledge of the difficulties in digital forensics and suggested metamodels help to create a more cohesive and integrated approach to digital investigations, establishing an environment for further study and collaborations in this crucial domain.
Keywords: Cyber security; Digital forensics; Network security; Artificial intelligence; Data privacy

Michał Choraś, Konstantinos Demestichas, Agata Giełczyk, Álvaro Herrero, Paweł Ksieniewicz, Konstantina Remoundou, Daniel Urda, Michał Woźniak,
Advanced Machine Learning techniques for fake news (online disinformation) detection: A systematic mapping study,
Applied Soft Computing,
Volume 101,
2021,
107050,
ISSN 1568-4946,
https://doi.org/10.1016/j.asoc.2020.107050.
(https://www.sciencedirect.com/science/article/pii/S1568494620309881)
Abstract: Fake news has now grown into a big problem for societies and also a major challenge for people fighting disinformation. This phenomenon plagues democratic elections, reputations of individual persons or organizations, and has negatively impacted citizens, (e.g., during the COVID-19 pandemic in the US or Brazil). Hence, developing effective tools to fight this phenomenon by employing advanced Machine Learning (ML) methods poses a significant challenge. The following paper displays the present body of knowledge on the application of such intelligent tools in the fight against disinformation. It starts by showing the historical perspective and the current role of fake news in the information war. Proposed solutions based solely on the work of experts are analysed and the most important directions of the application of intelligent systems in the detection of misinformation sources are pointed out. Additionally, the paper presents some useful resources (mainly datasets useful when assessing ML solutions for fake news detection) and provides a short overview of the most important R&D projects related to this subject. The main purpose of this work is to analyse the current state of knowledge in detecting fake news; on the one hand to show possible solutions, and on the other hand to identify the main challenges and methodological gaps to motivate future research.
Keywords: Fake news; Machine Learning; Social media; Media content manipulation; Disinformation detection

Ramcharan Ramanaharan, Deepani B. Guruge, Johnson I. Agbinya,
DeepFake video detection: Insights into model generalisation — A Systematic review,
Data and Information Management,
Volume 9, Issue 4,
2025,
100099,
ISSN 2543-9251,
https://doi.org/10.1016/j.dim.2025.100099.
(https://www.sciencedirect.com/science/article/pii/S2543925125000075)
Abstract: Deep learning generative models have progressed to a stage where distinguishing fake images and videos has become difficult, posing risks to personal integrity, potentially leading to social instability, and disrupting government functioning. Existing reviews have mainly focused on the approaches used to detect DeepFakes, and the data sets used for those approaches. However, challenges persist when attempting to generalise detection techniques to identify previously unseen datasets. The purpose of this systematic review is to explore state-of-the-art frameworks for DeepFake detection and provide readers with an understanding of the strengths and weaknesses of current approaches, as well as the generalisability of existing detection techniques. The study indicates that generalising DeepFake detection remains a challenge that requires further research. Moreover, 46.3% of the selected publications agreed that DeepFake detection techniques could be generalised to identify various types of DeepFakes. A key limitation in achieving generalisation is the tendency of models to overfit to available data datasets, reducing their effectiveness in adapting to new or unseen types of DeepFakes. This review emphasises the need for the development of extensive and diverse datasets that more accurately reflect the wide range of DeepFake manipulations encountered in real-world applications. Lastly, the paper explores potential advancements that could pave the way to the next generation of solutions against DeepFakes.
Keywords: DeepFake; Detection; Generalisability; Systematic review; Machine learning

Helena Liz-López, Mamadou Keita, Abdelmalik Taleb-Ahmed, Abdenour Hadid, Javier Huertas-Tato, David Camacho,
Generation and detection of manipulated multimodal audiovisual content: Advances, trends and open challenges,
Information Fusion,
Volume 103,
2024,
102103,
ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2023.102103.
(https://www.sciencedirect.com/science/article/pii/S1566253523004190)
Abstract: Generative deep learning techniques have invaded the public discourse recently. Despite the advantages, the applications to disinformation are concerning as the counter-measures advance slowly. As the manipulation of multimedia content becomes easier, faster, and more credible, developing effective forensics becomes invaluable. Other works have identified this need but neglect that disinformation is inherently multimodal. Overall in this survey, we exhaustively describe modern manipulation and forensic techniques from the lens of video, audio and their multimodal fusion. For manipulation techniques, we give a classification of the most commonly applied manipulations. Generative techniques can be exploited to generate datasets; we provide a list of current datasets useful for forensics. We have reviewed forensic techniques from 2018 to 2023, examined the usage of datasets, and given a comparative analysis of each modality. Finally, we give another comparison of end-to-end forensics tools for end-users. From our analysis clear trends are found with diffusion models, dataset granularity, explainability techniques, synchronisation improvements, and learning task diversity. We find a roadmap of deep challenges ahead, including multilinguality, multimodality, improving data quality (and variety), all in an adversarial ever-changing environment.
Keywords: Multimedia data manipulation generation; Multimedia data forensics; Deep Learning; Video; Audio; Multimodal

Ankit Yadav, Dinesh Kumar Vishwakarma,
Datasets, clues and state-of-the-arts for multimedia forensics: An extensive review,
Expert Systems with Applications,
Volume 249, Part C,
2024,
123756,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2024.123756.
(https://www.sciencedirect.com/science/article/pii/S0957417424006225)
Abstract: With the large chunks of social media data being created daily and the parallel rise of realistic multimedia tampering methods, detecting and localising tampering in images and videos has become essential. This survey focusses on approaches for tampering detection in multimedia data using deep learning models. Specifically, it presents a detailed analysis of publicly available benchmark datasets for malicious manipulation detection. It also offers a comprehensive list of tampering clues and commonly used deep learning architectures. Next, it discusses the current state-of-the-art tampering detection methods, categorizing them into meaningful types such as deepfake detection methods, splice tampering detection methods, copy-move tampering detection methods, etc. and discussing their strengths and weaknesses. Top results achieved on benchmark datasets, comparison of deep learning approaches against traditional methods and critical insights from the recent tampering detection methods are also discussed. Lastly, the research gaps, future direction and conclusion are discussed to provide an in-depth understanding of the tampering detection research arena.
Keywords: Tampering detection; Localization; Forgery; Manipulation; Deep learning; Convolutional neural networks

Mirko Casu, Luca Guarnera, Pasquale Caponnetto, Sebastiano Battiato,
GenAI mirage: The impostor bias and the deepfake detection challenge in the era of artificial illusions,
Forensic Science International: Digital Investigation,
Volume 50,
2024,
301795,
ISSN 2666-2817,
https://doi.org/10.1016/j.fsidi.2024.301795.
(https://www.sciencedirect.com/science/article/pii/S2666281724001197)
Abstract: This paper examines the impact of cognitive biases on decision-making in forensics and digital forensics, exploring biases such as confirmation bias, anchoring bias, and hindsight bias. It assesses existing methods to mitigate biases and improve decision-making, introducing the novel “Impostor Bias”, which arises as a systematic tendency to question the authenticity of multimedia content, such as audio, images, and videos, often assuming they are generated by AI tools. This bias goes beyond evaluators' knowledge levels, as it can lead to erroneous judgments and false accusations, undermining the reliability and credibility of forensic evidence. Impostor Bias stems from an a priori assumption rather than an objective content assessment, and its impact is expected to grow with the increasing realism of AI-generated multimedia products. The paper discusses the potential causes and consequences of Impostor Bias, suggesting strategies for prevention and counteraction. By addressing these topics, this paper aims to provide valuable insights, enhance the objectivity and validity of forensic investigations, and offer recommendations for future research and practical applications to ensure the integrity and reliability of forensic practices.
Keywords: Forensic sciences; Cognitive biases; Cognitive psychology; Digital forensics; Synthetic data; Impostor bias; Generative AI; GAN; Diffusion models; Deepfake detection

Sarina Aminizadeh, Arash Heidari, Shiva Toumaj, Mehdi Darbandi, Nima Jafari Navimipour, Mahsa Rezaei, Samira Talebi, Poupak Azad, Mehmet Unal,
The applications of machine learning techniques in medical data processing based on distributed computing and the Internet of Things,
Computer Methods and Programs in Biomedicine,
Volume 241,
2023,
107745,
ISSN 0169-2607,
https://doi.org/10.1016/j.cmpb.2023.107745.
(https://www.sciencedirect.com/science/article/pii/S016926072300411X)
Abstract: Medical data processing has grown into a prominent topic in the latest decades with the primary goal of maintaining patient data via new information technologies, including the Internet of Things (IoT) and sensor technologies, which generate patient indexes in hospital data networks. Innovations like distributed computing, Machine Learning (ML), blockchain, chatbots, wearables, and pattern recognition can adequately enable the collection and processing of medical data for decision-making in the healthcare era. Particularly, to assist experts in the disease diagnostic process, distributed computing is beneficial by digesting huge volumes of data swiftly and producing personalized smart suggestions. On the other side, the current globe is confronting an outbreak of COVID-19, so an early diagnosis technique is crucial to lowering the fatality rate. ML systems are beneficial in aiding radiologists in examining the incredible amount of medical images. Nevertheless, they demand a huge quantity of training data that must be unified for processing. Hence, developing Deep Learning (DL) confronts multiple issues, such as conventional data collection, quality assurance, knowledge exchange, privacy preservation, administrative laws, and ethical considerations. In this research, we intend to convey an inclusive analysis of the most recent studies in distributed computing platform applications based on five categorized platforms, including cloud computing, edge, fog, IoT, and hybrid platforms. So, we evaluated 27 articles regarding the usage of the proposed framework, deployed methods, and applications, noting the advantages, drawbacks, and the applied dataset and screening the security mechanism and the presence of the Transfer Learning (TL) method. As a result, it was proved that most recent research (about 43%) used the IoT platform as the environment for the proposed architecture, and most of the studies (about 46%) were done in 2021. In addition, the most popular utilized DL algorithm was the Convolutional Neural Network (CNN), with a percentage of 19.4%. Hence, despite how technology changes, delivering appropriate therapy for patients is the primary aim of healthcare-associated departments. Therefore, further studies are recommended to develop more functional architectures based on DL and distributed environments and better evaluate the present healthcare data analysis models.
Keywords: Medical data processing; Healthcare data analysis; Deep learning; Distributed computing

Krishnashree Achuthan, Sasangan Ramanathan, Raghu Raman,
Securing the metaverse: Machine learning–based perspectives on risk, trust, and governance,
International Journal of Information Management Data Insights,
Volume 5, Issue 2,
2025,
100356,
ISSN 2667-0968,
https://doi.org/10.1016/j.jjimei.2025.100356.
(https://www.sciencedirect.com/science/article/pii/S2667096825000382)
Abstract: The rapid expansion of the metaverse presents significant cybersecurity and privacy challenges, requiring structured, data-driven analysis. This study applies the ADO-TCM framework and BERTopic modeling to examine drivers of cybersecurity risk, theoretical responses, and interdisciplinary research gaps. Using PRISMA guidelines, 86 peer-reviewed studies were analyzed to identify key antecedents—technological vulnerabilities, user behavior, regulatory fragmentation, economic incentives, and cultural factors—shaping decisions in compliance, deployment, and education. These, in turn, influence outcomes like trust, threat mitigation, and scalability. The review identifies five latent themes: secure identity, privacy, trust, governance, and AI’s role in shaping risk. The study maps diverse theoretical lenses—cognitive, behavioral, strategic, and technological—used to interpret immersive threats and decision-making in metaverse contexts. Contributing a novel, empirically grounded synthesis, this research advances the information management literature and proposes a forward-looking agenda focused on adaptive security, ethical AI, interoperability, regulatory convergence, and intelligent, user-centric architecture for immersive ecosystems.
Keywords: Metaverse security; Cybersecurity governance; Decentralized identity; Privacy protection; User behavior; ADO-TCM framework; Threat detection; Regulatory compliance; BERTopic modeling

Vidya K, Praveen Ramesh, Hrithik Viknesh, Sanjay Devanand,
Compressed Deepfake Detection using Spatio-Temporal Approach with Model Pruning,
Procedia Computer Science,
Volume 230,
2023,
Pages 436-444,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2023.12.099.
(https://www.sciencedirect.com/science/article/pii/S187705092302104X)
Abstract: Deepfake is a deep learning technology that replaces source person face photos with the target person face photos in a movie to create a video of the target by executing the actions performed by source person. Due to the limited storage capacity and network bandwidth constraints, compressed media is now commonly employed in social networks. The goal of this research study is to determine whether or not a given compressed video is a deepfake. To do this, both the spatial and temporal components of the video should be considered, and the findings will be integrated by using an appropriate fusion approach. Hence, the deep learning model used in the spatial approach is pruned using the Network Pruning technique to achieve a better performance. The combined prediction of the spatial and temporal approaches indicates whether the given video is deepfake or not.
Keywords: Compressed; Fusion; Pruning; Spatio-Temporal features

Pramukh Nanjundaswamy Vasist, Satish Krishnan,
Engaging with deepfakes: a meta-synthesis from the perspective of social shaping of technology theory,
Internet Research,
Volume 33, Issue 5,
2023,
Pages 1670-1726,
ISSN 1066-2243,
https://doi.org/10.1108/INTR-06-2022-0465.
(https://www.sciencedirect.com/science/article/pii/S1066224323000357)
Abstract: Purpose
This study aims to establish a comprehensive understanding of the intricacies of how individuals engage with deepfakes, focusing on limiting adverse effects and capitalizing on their benefits.
Design/methodology/approach
This study conducted a meta-synthesis of qualitative studies on deepfakes, incorporating study-specific analysis followed by a cross-study synthesis.
Findings
Based on the meta-synthesis, the study developed an integrated conceptual framework based on the perspectives from the social shaping of technology theory embedding deepfake-related assertions, motivations, the subtleties of digital platforms, and deepfake-related repercussions.
Research limitations/implications
The study offers crucial insights into the evolving nature of deepfakes as a socio-technical phenomenon and the significance of platform dynamics in deepfake production. It enables researchers to comprehend the cascading effects of deepfakes and positions them to evaluate deepfake-related risks and associated mitigation mechanisms.
Practical implications
The framework that emerges from the study illustrates the influence of platforms on the evolution of deepfakes and assists platform stakeholders in introducing effective platform governance structures to combat the relentless proliferation of deepfakes and their consequences, as well as providing guidance for governments and policymakers to collaborate with platform leaders to set guardrails for deepfake engagement.
Originality/value
Deepfakes have been extensively contested for both their beneficial and negative applications and have been accused of heralding an imminent epistemic threat that has been downplayed by some quarters. This diversity of viewpoints necessitates a comprehensive understanding of the phenomenon. In responding to this call, this is one of the first to establish a comprehensive, theoretically informed perspective on how individuals produce, process, and engage with deepfakes through a meta-synthesis of qualitative literature on deepfakes.
Keywords: Deepfake; Synthetic media; Fake news; Meta-synthesis; Qualitative study

Battula Thirumaleshwari Devi, Rajkumar Rajasekaran,
Deepfake Video Detection Using Ada-Boosting on the DFDC Dataset,
Procedia Computer Science,
Volume 258,
2025,
Pages 1091-1101,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.04.344.
(https://www.sciencedirect.com/science/article/pii/S1877050925014462)
Abstract: The rapid proliferation of deepfake videos, generated using advanced machine learning techniques to create highly realistic but misleading content, poses significant challenges across various sectors, including cybersecurity, media integrity, and personal privacy. Detecting these deepfakes has become essential for maintaining trust in digital media and preventing malicious exploitation. This paper presents a novel approach to deepfake video detection by employing the AdaBoost algorithm, a powerful ensemble learning method recognized for its ability to improve classification performance by focusing on difficult-to-classify instances. Using the Deepfake Detection Challenge (DFDC) dataset, our study demonstrates that the AdaBoost classifier, when coupled with a carefully designed feature set, achieves competitive accuracy in detecting deepfake videos. Our results show that this approach provides an effective solution for deepfake detection, with strong recall performance, making it a viable method for real-world applications.
Keywords: Deepfake detection; AdaBoost; DFDC dataset; ensemble learning; machine learning; video forensics

He Huang, Nan Sun, Massimiliano Tani, Yu Zhang, Jiaojiao Jiang, Sanjay Jha,
Can LLM-generated misinformation be detected: A study on Cyber Threat Intelligence,
Future Generation Computer Systems,
Volume 173,
2025,
107877,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2025.107877.
(https://www.sciencedirect.com/science/article/pii/S0167739X25001724)
Abstract: Given the increasing number and severity of cyber attacks, there has been a surge in cybersecurity information across various mediums such as posts, news articles, reports, and other resources. Cyber Threat Intelligence (CTI) involves processing data from these cybersecurity sources, enabling professionals and organizations to gain valuable insights. However, with the rapid dissemination of cybersecurity information, the inclusion of fake CTI can lead to severe consequences, including data poisoning attacks. To address this challenge, we have implemented a three-step strategy: generating synthetic CTI, evaluating the quality of the generated CTI, and detecting fake CTI. Unlike other subdomains, such as fake COVID news detection, there is currently no publicly available dataset specifically tailored for fake CTI detection research. To address this gap, we first establish a reliable groundtruth dataset by utilizing domain-specific cybersecurity data to fine-tune a Large Language Model (LLM) for synthetic CTI generation. We then employ crowdsourcing techniques and advanced synthetic data verification methods to evaluate the quality of the generated dataset, introducing a novel evaluation methodology that combines quantitative and qualitative approaches. Our comprehensive evaluation reveals that the generated CTI cannot be distinguished from genuine CTI by human annotators, regardless of their computer science background, demonstrating the effectiveness of our generation approach. We benchmark various misinformation detection techniques against our groundtruth dataset to establish baseline performance metrics for identifying fake CTI. By leveraging existing techniques and adapting them to the context of fake CTI detection, we provide a foundation for future research in this critical field. To facilitate further research, we make our code, dataset, and experimental results publicly available on GitHub.
Keywords: Cyber security; Artificial intelligence; Human-centric

Dmitry Gura, Bo Dong, Duaa Mehiar, Nidal Al Said,
Customized Convolutional Neural Network for Accurate Detection of Deep Fake Images in Video Collections,
Computers, Materials and Continua,
Volume 79, Issue 2,
2024,
Pages 1995-2014,
ISSN 1546-2218,
https://doi.org/10.32604/cmc.2024.048238.
(https://www.sciencedirect.com/science/article/pii/S1546221824002698)
Abstract: The motivation for this study is that the quality of deep fakes is constantly improving, which leads to the need to develop new methods for their detection. The proposed Customized Convolutional Neural Network method involves extracting structured data from video frames using facial landmark detection, which is then used as input to the CNN. The customized Convolutional Neural Network method is the date augmented-based CNN model to generate ‘fake data’ or ‘fake images’. This study was carried out using Python and its libraries. We used 242 films from the dataset gathered by the Deep Fake Detection Challenge, of which 199 were made up and the remaining 53 were real. Ten seconds were allotted for each video. There were 318 videos used in all, 199 of which were fake and 119 of which were real. Our proposed method achieved a testing accuracy of 91.47%, loss of 0.342, and AUC score of 0.92, outperforming two alternative approaches, CNN and MLP-CNN. Furthermore, our method succeeded in greater accuracy than contemporary models such as XceptionNet, Meso-4, EfficientNet-BO, MesoInception-4, VGG-16, and DST-Net. The novelty of this investigation is the development of a new Convolutional Neural Network (CNN) learning model that can accurately detect deep fake face photos.
Keywords: Deep fake detection video analysis; convolutional neural network; machine learning; video dataset collection; facial landmark prediction; accuracy; models

Kamran Gholamizadeh, Esmaeil Zarei, Luca Gualtieri, Matteo De Marchi,
Advancing occupational and system safety in Industry 5.0: Effective HAZID, risk analysis frameworks, and human-AI interaction management,
Safety Science,
Volume 184,
2025,
106770,
ISSN 0925-7535,
https://doi.org/10.1016/j.ssci.2024.106770.
(https://www.sciencedirect.com/science/article/pii/S0925753524003606)
Abstract: As Industry 5.0 evolves, this study addresses critical aspects of system and occupational safety through hazard identification (HAZID), risk analysis, and the interaction between humans and artificial intelligence (HI and AI). This review investigates existing literature to answer three key research questions: (1) How is HAZID conducted to meet the safety demands of Industry 5.0? (2) What constitutes an effective risk analysis framework for this era? (3) How can conflicts between HI and AI be managed to enhance system reliability? The findings demonstrate that AI-driven approaches significantly improve HAZID processes and underscore the necessity of tailored risk analysis frameworks for effective safety management. Additionally, strategies to mitigate HI-AI conflicts are discussed, highlighting the importance of policies that foster decision-making and reliability in human-AI collaboration. The study concludes with practical recommendations for advancing safety management in complex socio-technical systems within Industry 5.0.
Keywords: Safety 5.0; Human-AI Collaboration; Intelligent Systems; Human-Centered Design; AI integration; System Safety

Mahmoud Ragab, Bandar M. Alghamdi, Rayed Alakhtar, Huda Alsobhi, Louai A. Maghrabi, Ghadah Alghamdi, Sameer Nooh, Abdullah AL-Malaise AL-Ghamdi,
Enhancing cybersecurity in higher education institutions using optimal deep learning-based biometric verification,
Alexandria Engineering Journal,
Volume 117,
2025,
Pages 340-351,
ISSN 1110-0168,
https://doi.org/10.1016/j.aej.2025.01.012.
(https://www.sciencedirect.com/science/article/pii/S1110016825000213)
Abstract: Cybersecurity is an increasingly significant issue in higher education institutions, and biometric technology offers an effective solution to enhance security measures. Biometrics mentions utilizing biological features, like facial detection or fingerprint, to prove the identity of individuals. In higher education institutions, biometrics are used to improve access control and authentication models. For instance, biometric verification was utilized for secure access to computer labs, buildings, and other sensitive areas on campus. It supports preventing unauthorized access and decreasing the risk of being or other security breaches. It could be an effective tool to improve cybersecurity in higher education institutions. However, it can be vital to implement robust security protocols and privacy protection to ensure that biometric data can be utilized securely and responsibly. So, this research paper proposes the hunter-prey optimizer with deep learning-enabled biometric verification for cybersecurity (HPODL-BVCS) techniques in higher education institutions. The HPODL-BVCS technique utilizes the DL model to accomplish biometric verification in higher education institutions. To complete this, the HPODL-BVCS technique employs bilateral filtering (BF) based noise elimination to preprocess the biometric imageries. Besides, the HPODL-BVCS technique employs the ShuffleNetv2.3 model for feature extraction purposes. Additionally, the HPO model is used for the hyperparameter tuning process. The HPODL-BVCS technique utilizes a convolutional autoencoder (CAE) model with root mean square propagation optimizer (RMSProp) for classification. The simulation result of the HPODL-BVCS approach is performed on the biometric dataset. The experimental validation of the HPODL-BVCS approach portrayed a superior accuracy value of 99.81 % over existing techniques in terms of diverse performance metrics.
Keywords: Higher education institutions; Deep learning; Cybersecurity; Hunter prey optimizer; Biometrics

Simon Fahle, Christopher Prinz, Bernd Kuhlenkötter,
Systematic review on machine learning (ML) methods for manufacturing processes – Identifying artificial intelligence (AI) methods for field application,
Procedia CIRP,
Volume 93,
2020,
Pages 413-418,
ISSN 2212-8271,
https://doi.org/10.1016/j.procir.2020.04.109.
(https://www.sciencedirect.com/science/article/pii/S2212827120307435)
Abstract: Artificial Intelligence (AI) and especially machine learning (ML) become increasingly more frequently applicable in factory operations. This paper presents a systematic review of today’s applications of ML techniques in the factory environment. The utilization of ML methods related to manufacturing process planning and control, predictive maintenance, quality control, in situ process control and optimization, logistics, robotics, assistance and learning systems for shopfloor employees are being analyzed. Moreover, an overview of ML training concepts in learning factories is given. Furthermore, these concepts will be analyzed regarding the implemented ML method. Finally, research gaps are identified.
Keywords: Artificial Intelligence; machine learning; production systems; factory operation

Uzma Zaheen, Allah Rakha, Qudsia Hassan, Muhammad Farhan Khan, Sareen Akhtar, Anam Munawar,
A systematic review about the evolving role of artificial intelligence in various fields of forensic medicine,
Journal of Forensic and Legal Medicine,
Volume 117,
2026,
103043,
ISSN 1752-928X,
https://doi.org/10.1016/j.jflm.2025.103043.
(https://www.sciencedirect.com/science/article/pii/S1752928X25002446)
Abstract: Objective
This study systematically reviews the applications and impact of artificial intelligence (AI) in forensic medicine, focusing on its role in mimicking human cognitive processes, enhancing diagnostic accuracy, pattern recognition, and operational efficiency across forensic domains.
Methodology
A systematic search was conducted in PubMed and Google Scholar using keywords including "AI in forensics" and "machine learning forensic analysis," covering publications from 2014 to 2024. A total of approximately 1000 articles were initially identified, of which 100 met the inclusion criteria after screening for relevance, study design, and quality.
Results
AI applications in forensic medicine were categorized into key domains: personal identification, forensic pathology, radiology and imaging, digital forensics, toxicology, and forensic anthropology. Machine learning, deep learning, and neural network models demonstrated improvements in accuracy, reproducibility, and efficiency compared with conventional approaches. For example, AI-assisted imaging techniques reduced inter-observer variability in postmortem fracture detection, while predictive models for postmortem interval estimation showed mean error reductions of up to 15 %. Despite these advances, challenges such as small, non-representative datasets, limited external validation, and ethical concerns remain.
Conclusion
AI has significantly enhanced multiple areas of forensic practice by improving diagnostic capabilities, streamlining workflows, and supporting decision-making. However, wider adoption requires rigorous validation, standardization, and ethical oversight. Future research should focus on integrating multimodal data, expanding dataset diversity, and addressing legal and ethical implications to maximize the utility of AI in real-world forensic investigations.
Keywords: Artificial intelligence (AI); Forensics medicine; Diagnosis; Forensic pathology; Deep learning; Neural network

Fakhar Abbas, Araz Taeihagh,
Unmasking deepfakes: A systematic review of deepfake detection and generation techniques using artificial intelligence,
Expert Systems with Applications,
Volume 252, Part B,
2024,
124260,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2024.124260.
(https://www.sciencedirect.com/science/article/pii/S0957417424011266)
Abstract: Due to the fast spread of data through digital media, individuals and societies must assess the reliability of information. Deepfakes are not a novel idea but they are now a widespread phenomenon. The impact of deepfakes and disinformation can range from infuriating individuals to affecting and misleading entire societies and even nations. There are several ways to detect and generate deepfakes online. By conducting a systematic literature analysis, in this study we explore automatic key detection and generation methods, frameworks, algorithms, and tools for identifying deepfakes (audio, images, and videos), and how these approaches can be employed within different situations to counter the spread of deepfakes and the generation of disinformation. Moreover, we explore state-of-the-art frameworks related to deepfakes to understand how emerging machine learning and deep learning approaches affect online disinformation. We also highlight practical challenges and trends in implementing policies to counter deepfakes. Finally, we provide policy recommendations based on analyzing how emerging artificial intelligence (AI) techniques can be employed to detect and generate deepfakes online. This study benefits the community and readers by providing a better understanding of recent developments in deepfake detection and generation frameworks. The study also sheds a light on the potential of AI in relation to deepfakes.
Keywords: Deep learning; Deepfakes; Detection and generation; Artificial Intelligence (AI); Policy recommendations; Literature review

MD Sarfaraz Momin, Abu Sufian, Debaditya Barman, Marco Leo, Cosimo Distante, Naser Damer,
Explainable deepfake detection across different modalities: An overview of methods and challenges,
Image and Vision Computing,
Volume 163,
2025,
105738,
ISSN 0262-8856,
https://doi.org/10.1016/j.imavis.2025.105738.
(https://www.sciencedirect.com/science/article/pii/S0262885625003269)
Abstract: The increasing use of deepfake technology enables the creation of realistic and deceptive content, raising concerns about several serious issues, including biometric authentication, misinformation, politics, privacy, and trust. Many Deepfake Detection (DD) models are entering the market to combat the misuse of deepfakes. With these developments, one primary issue occurs in ensuring the explainability of the proposed detection models to understand the rationale of the decision. This paper aims to investigate the state-of-the-art explainable DD models across multiple modalities, including image, video, audio, and text. Unlike existing surveys that focus on detection methodologies with minimal attention to explainability and limited modality coverage, this paper directly focuses on these gaps. It offers a comprehensive analysis of advanced explainability techniques, including Grad-CAM, LIME, SHAP, LRP, Saliency Maps, and Anchors, for detecting deceptive content across the modalities. It identifies the strengths and limitations of existing models and outlines research directions to enhance explainability and interpretability in future works. By exploring these models, we aim to enhance transparency, provide deeper insights into model decisions, and bridge the gap between detection accuracy with explainability in DD models.
Keywords: Machine learning; Deep learning; Generative AI; Deepfake; Explainable AI

Akshay Agarwal, Nalini Ratha,
Chapter 8 - Manipulating faces for identity theft via morphing and deepfake: Digital privacy,
Editor(s): Venu Govindaraju, Arni S.R. Srinivasa Rao, C.R. Rao,
Handbook of Statistics,
Elsevier,
Volume 48,
2023,
Pages 223-241,
ISSN 0169-7161,
ISBN 9780443184307,
https://doi.org/10.1016/bs.host.2022.12.003.
(https://www.sciencedirect.com/science/article/pii/S016971612200058X)
Abstract: Digital face images can be easily manipulated for obfuscating or impersonating an identity. Several techniques are used for face manipulation, both traditional computer vision based such as morphing, and modern deep learning based such as deepfake. Morphing and deepfake techniques became advanced enough in creating photorealistic face images. Due to that, these techniques pose a serious threat to identity theft and can significantly harm at a personal level such as the risk of reputation and money, and the national level such as interference in the election. In this chapter, we review (i) different stealthy ways of identity threat generation techniques, (ii) popular databases used in this research direction, and (iii) defense algorithms build to detect these manipulated images. We further provide key open challenges which need to be addressed to make the defense algorithms robust, generalized, and to handle the adaptive nature of the attacks.
Keywords: Deepfake; Identity swap; Digital threats; Vulnerability of deep face recognition; Privacy and security

Jenifer Loovens, Hasan Tinmaz,
A systematic literature review of deepfakes in forensic science,
Forensic Imaging,
Volume 43,
2025,
200647,
ISSN 2666-2256,
https://doi.org/10.1016/j.fri.2025.200647.
(https://www.sciencedirect.com/science/article/pii/S2666225625000259)
Abstract: This research explores the complex implications of deepfakes, a controversial application of Artificial Intelligence (AI) and deep learning in forensic science. It highlights the ethical dilemmas and technological challenges associated with their use, emphasizing the growing risk deepfakes pose to the integrity of digital evidence. The study also addresses the ongoing ‘arms race’ between the development of increasingly sophisticated deepfakes content and the progress in detection tools. Additionally, it investigates the psychological and legal aspects of deepfakes, advocating for critical technological advancements and ethical frameworks to mitigate the associated risks. A systematic literature review of 36 selected research articles published between 2021 and 2024 across seven academic databases was conducted. The analysis identifies key research trends, categorizes essential keywords, and examines the various forensic approaches employed in deepfakes research. The findings reveal that, while progress has been made in deepfakes detection and forensic analysis, interdisciplinary collaboration is urgently needed to establish standardized methods and frameworks to combat digital manipulation. Continuous advancements in detection techniques, alongside the integration of ethical considerations into forensic practices, will be crucial to preserving the integrity of digital evidence amidst the rapid evolution of deepfakes technology.
Keywords: Deepfakes; Forensic science; Misinformation; Media manipulation; Digital evidence; Systematic review

Christopher Hargreaves, Frank Breitinger, Liz Dowthwaite, Helena Webb, Mark Scanlon,
DFPulse: The 2024 digital forensic practitioner survey,
Forensic Science International: Digital Investigation,
Volume 51,
2024,
301844,
ISSN 2666-2817,
https://doi.org/10.1016/j.fsidi.2024.301844.
(https://www.sciencedirect.com/science/article/pii/S2666281724001719)
Abstract: This paper reports on the largest survey of digital forensic practitioners to date (DFPulse) conducted from March to May 2024 resulting in 122 responses. The survey collected information about practitioners' operating environments, the technologies they encounter, investigative techniques they use, the challenges they face, the degree to which academic research is accessed and useful to the practitioner community, and their suggested future research directions. The paper includes quantitative and qualitative results from the survey and a discussion of the implications for academia, the improvements that can be made, and future research directions.
Keywords: Digital forensics; Practitioner survey; Challenges; Future directions; Artificial intelligence

Fransiskus Triyanto Winata, Nicholas Justin Tanuwijaya, Reina Setiawan, Reinert Yosua Rumagit,
Comparison of deepfake detection using CNN and hybrid models,
Procedia Computer Science,
Volume 269,
2025,
Pages 1556-1564,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.09.097.
(https://www.sciencedirect.com/science/article/pii/S1877050925027656)
Abstract: In today’s digital era, manipulated videos known as deepfakes are becoming more common and harder to detect. These deepfake contents can be misused by people to spread false information, damage someone’s reputation, or even harm many people. To address this issue, we conducted a study to compare the performance of three deep learning models Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and a hybrid CNN-LSTM model to detect deepfake content. In this study, we used the DeepFake Detection Challenge (DFDC) dataset, which includes a variety of real and fake videos, then we will evaluate each model using accuracy, precision, recall, F1-score, AUC, and loss values. The results of our research show that the RNN model provides the best overall performance with an accuracy of 77.5%, F1-score of 77.5%, and AUC of 0.928, while CNN followed closely with 75.5% accuracy and AUC of 0.930. The CNN-LSTM model showed lower performance with 67.5% accuracy and AUC of 0.886. The findings demonstrate the strength of RNNs in modeling temporal patterns in video data, which is crucial for effective deepfake detection. Then CNN model also have good performance and shows strong results in detecting spatial features in individual frames. Meanwhile, CNN-LSTM model does not perform as well as the other models. This can be happen due to hybrid model having more complex structure, which makes it more difficult to train effectively. We hope this research helps in building better tools to detect deepfakes in real-world situations.
Keywords: Deepfake; DFDC; CNN; LSTM; CNN-LSTM

Preeti Sharma, Manoj Kumar, Hitesh Kumar Sharma,
GAN-CNN Ensemble: A Robust Deepfake Detection Model of Social Media Images Using Minimized Catastrophic Forgetting and Generative Replay Technique,
Procedia Computer Science,
Volume 235,
2024,
Pages 948-960,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2024.04.090.
(https://www.sciencedirect.com/science/article/pii/S187705092400766X)
Abstract: Deep-fake photographs are difficult to discern from real ones, especially when utilized in social media platforms. Anyone can willfully create disinformation about public personalities, politicians, and celebrities using these deep fake photographs. So, it is an important need of society to work for an effective model for its detection. The models for deep fake detection commonly use CNN-based detectors. These detectors experience a drop in performance when used for transfer learning or continual learning techniques. A significant limitation in this process is CNN's catastrophe forgetting defect. For the solution of this problem, a Generative replay technique in the form of a GAN-CNN model is implemented that works to minimize this catastrophe forgetting issue that further helps for better detection. It involves generating and storing samples from previous tasks and then replaying them during the training of new tasks which makes CNN more robust to identify deep fakes. The GAN model used in this work is traditional DCGAN improved with necessary adjustments to achieve training stability. It is observed that the model attained a good accuracy of 98.67%(training),70.08% (testing) and minimum loss with a value of 0. 0337 for 100 epochs. Also, it acquired good precision values of 68% and 72%, Recall values are 74% and 66%, and F1 scores of 71% and 69% for classes 0 and 1 respectively. The model outcome is found stable and reliable in deep fake detection under dynamic training conditions. Optimum values of evaluation parameters ensure the model's capacity to learn new tasks preserving the existing task-learning knowledge.
Keywords: Deep Learning; CNN;GAN;Catastrophe forgetting;CNN continual learning;Lifelong learning; GAN-CNN deep fake detector

Andry Chowanda, Mohamed Imran Bin Mohamed Ariff,
CNN-swarm intelligence hybrid model for facial expression recognition,
Procedia Computer Science,
Volume 269,
2025,
Pages 844-852,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.09.027.
(https://www.sciencedirect.com/science/article/pii/S1877050925026882)
Abstract: Emotion is essential to our social interactions, as it allows us to communicate with others effectively. Emotion is expressed through facial expressions, voice prosody, and body language. As technology advances rapidly to facilitate human connection, the capac- ity to discern emotions through facial expressions, voice prosody, and body language has markedly improved. Facial expressions serve as non-verbal indicators for interpreting emotions. Recent research indicates that models utilising extensive datasets and deep learning architectures, such as Convolutional Neural Networks (CNN) and Vision Transformers, have strong performance on benchmarks for face expression datasets. Nevertheless, significant constraints exist when representing emotions through facial expressions (i.e., image or video-based). The variance and illumination significantly impact the model’s performance. This study seeks to examine the effectiveness of combining a conventional optimisation technique (i.e., gradient-based optimisation) with a metaheuristic search approach (i.e., swarm intelligence) to improve model performance. An inception-based architecture is utilised to model emotion recognition from facial cues. A hybrid optimisation approach that integrates gradient-based and swarm intelli- gence techniques is employed to improve the architectures. All the offered models demonstrate significantly enhanced performance relative to the baseline. Model B (20-5) attained a training accuracy of 99.15%, a validation accuracy of 100%, a training loss of 0.0934, and a validation loss of 0.0402.
Keywords: Facial Expression Recognition; Particle Swarm Optimisation; Convolutional Neural Networks; Swarm Intelligence; Emotions Recognition

Jeongeun Park, Changhoon Oh, Ha Young Kim,
AI vs. human-generated content and accounts on Instagram: User preferences, evaluations, and ethical considerations,
Technology in Society,
Volume 79,
2024,
102705,
ISSN 0160-791X,
https://doi.org/10.1016/j.techsoc.2024.102705.
(https://www.sciencedirect.com/science/article/pii/S0160791X24002537)
Abstract: As content generated by artificial intelligence (AI) has become more accessible and higher quality, social debates have intensified. Therefore, this study investigates how people perceive and evaluate AI-generated content and accounts compared to human-created accounts and content. We created Instagram accounts representing AI, influencers, and the public and conducted a user study with 43 participants. The participants had difficulty distinguishing AI accounts from human ones. Moreover, there were significant differences in user perceptions concerning the three account types. Participants perceived the AI and influencer accounts as more attractive than the public account, and they rated the quality of AI-generated content as highly as that created by influencers. These findings suggest that the advancement of generative AI could alter the social media landscape and contribute to discussions on the characteristics and ethical problems of AI-generated content.
Keywords: Artificial intelligence (AI) generated content; AI artists; Social networking services; User experience; Generative models

Anton Firc, Kamil Malinka, Petr Hanáček,
Deepfakes as a threat to a speaker and facial recognition: An overview of tools and attack vectors,
Heliyon,
Volume 9, Issue 4,
2023,
e15090,
ISSN 2405-8440,
https://doi.org/10.1016/j.heliyon.2023.e15090.
(https://www.sciencedirect.com/science/article/pii/S2405844023022971)
Abstract: Deepfakes present an emerging threat in cyberspace. Recent developments in machine learning make deepfakes highly believable, and very difficult to differentiate between what is real and what is fake. Not only humans but also machines struggle to identify deepfakes. Current speaker and facial recognition systems might be easily fooled by carefully prepared synthetic media – deepfakes. We provide a detailed overview of the state-of-the-art deepfake creation and detection methods for selected visual and audio domains. In contrast to other deepfake surveys, we focus on the threats that deepfakes represent to biometrics systems (e.g., spoofing). We discuss both facial and speech deepfakes, and for each domain, we define deepfake categories and their differences. For each deepfake category, we provide an overview of available tools for creation, datasets, and detection methods. Our main contribution is a definition of attack vectors concerning the differences between categories and reported real-world attacks to evaluate each category's threats to selected categories of biometrics systems.
Keywords: Face deepfakes; Speech deepfakes; Biometrics systems; Facial recognition; Speaker recognition; Deepfake detection; Cybersecurity

Muhammad Zubair, Saqib Hakak,
Exploring the Landscape of Compressed DeepFakes: Generation, Dataset and Detection,
Neurocomputing,
Volume 619,
2025,
129116,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2024.129116.
(https://www.sciencedirect.com/science/article/pii/S0925231224018873)
Abstract: In today’s era of social media, where information spreads rapidly through platforms like YouTube, Facebook, and Twitter, the development of generative models have given rise to a phenomenon called DeepFakes. This survey aims to provide a comprehensive overview of compressed DeepFakes research, covering various detection and generation techniques and datasets. It presents the details of detection methods, including experimental settings such as datasets, algorithms, feature selection, and results. The survey also highlights the existing challenges and future directions.
Keywords: Fake news; DeepFakes; Social Engineering; MultiMedia Forensics; Forgery detection; GenerativeAI; Compression

Sakshini Hangloo, Dr. Bhavna Arora,
Feature fusion for multimodal fake news detection,
Procedia Computer Science,
Volume 259,
2025,
Pages 1144-1153,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.04.069.
(https://www.sciencedirect.com/science/article/pii/S1877050925011676)
Abstract: With the rise of social media, people increasingly rely on these platforms to stay informed about global events. However, the credibility of these sources remains uncertain. The accessibility and speed that make social media appealing have also contributed to a growing issue of fake news. But, the very nature of the social media where due to the lack of fact check on the news article along with fabricated visuals, creates a sense of confusion among people. To address this problem, this paper presents feature fusion techniques for multimodal fake news detection problem. The framework uses the VGG-19 pre-trained model to preprocess and extract visual features, and GloVE is used for generating text embeddings, these features are fused using three different fusion techniques (1) concatenation, (2) addition and (3) weighted addition. This paper presents a comparative analysis of these feature fusion techniques on FakeNewsNet dataset. The results shows the supremacy of weighted addition algorithm over the other two algorithms over various performance metrics.
Keywords: fusion; multimodal; fake news; rumor

Krity Duhan, Abhishek Kajal,
A Comparative Analysis of Deep Learning Based Approaches for DeepFake Identification,
Procedia Computer Science,
Volume 259,
2025,
Pages 482-493,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.03.350.
(https://www.sciencedirect.com/science/article/pii/S1877050925010944)
Abstract: One of the major concerns in the present global environment is the creation and utilization of deepfakes. In this work, we have examined the issues and challenges that are created by the deepfake in the security and surveillance mechanisms. Deepfakes are used for by passing the facial identification or biometrics system that are normally used as surveillance mechanisms. Previously the only issue was to find the authentic person through the photographs that they have put on their identity proofs but nowadays it is extended to find the forged pics that are manipulated with the usage of AI based methods like deepfakes. Deepfakes are used in the current environments for bypassing the security by impersonation and providing false information and thus become a source of danger especially in the politics arena and entertainment landscape. Deep learning has the tendency to mitigate the impact of deepfakes to considerable extent by incorporating the same in the techniques develop for the identification of deepfakes. This paper provides the comprehensive review of the deepfake related work that has been done by researchers with the usage of deep learning approaches which could identify the fake images, videos to significant extent in various contexts along with the mechanism used for deepfake creation and identification in general. Moreover, the comparative analysis of the existing techniques has been done considering a range of factors like dataset used, technique used, accuracy, AUC, data type used for deepfake identification etc. It has been inferred that majority of the researchers have used the FaceForensics++, Celeb-DF, DFDC dataset and have utilized CNN technique for the deepfake identification primarily on images.
Keywords: Deepfake; CNN; GAN; Accuracy; Deep learning; Dataset; AUC

S Yougesh Kumar, Amruth Ganesh J M, Tejasvin Vimal, S Abhishek, Anjali T,
Beyond X-ray: Deep Learning Solutions for Gastrointestinal Bleeding,
Procedia Computer Science,
Volume 259,
2025,
Pages 1306-1315,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.04.085.
(https://www.sciencedirect.com/science/article/pii/S187705092501186X)
Abstract: Gastrointestinal (GI) bleeding is a critical medical condition that has a substantial impact and poses significant challenges for the global healthcare system due to its varying severity. GI bleeding can occur ranging from mild, self-limiting cases to severe, poten- tially fatal events, which emphasizes the need for timely and accurate diagnosis. Traditional feature extraction techniques, such as color thresholding and Scale-Invariant Feature Transform (SIFT), combined with classifiers like Support Vector Machine (SVM), often fail to detect subtle patterns within complex gastrointestinal images, especially those images captured from wireless capsule endoscopy (WCE).This work demonstrates the distinct benefits of deep learning techniques over conventional methods, especially their capacity to identify minute bleeding patterns in intricate gastrointestinal pictures. Better performance is shown by models like EfficientNetB0, which are computationally efficient, and the custom CNN, which is designed for binary classification. These discoveries aid in the creation of automated, trustworthy diagnostic instruments for use in real-time healthcare settings. Especially, In this research, we introduce the most sought-after deep learning methods namely Custom Convolutional Neural Network (CNN), EfficientNetB0, ResNet50, and InceptionV3. These models and their working have been thoroughly understood and evaluated across various parameters such as precision, accuracy, recall F1-score, etc., These algorithms have proven capability in identifying patterns that traditional methods tend to miss out. The results prove the potential of the models, particularly EfficientNetB0 and custom CNN, which offered superior performance, efficiency and reliability in tasks involving the classification of medical images. Further study of this work can be carried out, optimizing model accuracy, robustness, and computational efficiency by including varied and more diversified datasets. Parallely, exploring novel architectures that can handle more complicated and obscure medical images opens up new areas of interest for upcoming works.
Keywords: Custom Convolutional Neural Network; Deep Learning; Efficient Convolutional Neural Network (EfficientNetB0); InceptionV3; Residual Network50 (ResNet50); Wireless Capsule Endoscopy
