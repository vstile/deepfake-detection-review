"Authors","Author full names","Author(s) ID","Title","Year","Link","Abstract","Author Keywords"
"Rajesh, T.; Maruthupermal, S.","Rajesh, Tirupathi (60055683100); Maruthupermal, S. (60054258400)","60055683100; 60054258400","Generalizable deepfake detection framework using hybrid convolution-based EfficientNetB7 with attention mechanism","2026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013740842&partnerID=40&md5=87fa8f1e12c720047e82ef8f0c904675","The highly realistic fake videos have been created by the deepfake technology in recent years. The political and societal consequences occurred because of the misinformation disseminated by the fake videos. To prevent these issues, automated approaches for deep fake detection are necessary. The deep fake video is detected in this work using hybrid deep learning. The main contribution of the proposed research is to determine deepfakes from videos, which aims to avoid the spread of malicious rumours. The differences between the real and deepfake videos are effectively identified through this developed approach, as they utilize the different features from the facial landmarks along with the eye blinks for determining the sophisticated fakes from the authenticated visual contents. The required videos are collected from the public databases. The frames are extracted from the collected videos. The attained video frames are given to the facial landmark detection process, where the coordinates of the eyes, lips, and nose are extracted. From these detected facial landmarks, the texture features, shape features, and the number of eye blinks are obtained for better detection purposes. The extracted features are integrated with the optimized weights for attaining the weighted fused feature, where the Updated Random Parameter-aided Fennec Fox Optimization (URP-FFO) is used for tuning the weights optimally. Each frame of 1-dimensional data forms 2-dimensional original video frames, which are taken as feature set 1 from the weighted fused features. The 3-dimensional data from the facial images is considered as feature set 2. The attained features are given to the Hybrid 2D-3D Convolution-based EfficientNetB7 with Attention Mechanism (HC-EB7AM) for differentiating the real and fake videos. The manipulated contents are easily identified by this approach. Finally, several measures are used for validating the performance. The potential strength of the proposed model is validated by the experimentation with the accuracy of 93.66% using dataset 1 and 94.38% using dataset 2, respectively. © 2025 Elsevier Ltd","Deepfake detection; EfficientNetB7 with attention mechanism; Facial landmark extraction; Hybrid convolution network; Updated random parameter-based Fennec fox optimization; Video frames"
"Wang, J.; Jin, X.; Wang, H.; Jiang, L.","Wang, Jinyu (58078265000); Jin, Xin (56984939900); Wang, Huaye (59156347800); Jiang, Longteng (58759665200)","58078265000; 56984939900; 59156347800; 58759665200","VAD-Lip: Visual and Audio Deepfake Detection via Lip Features","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021655688&partnerID=40&md5=7d472314470bbba496a78230fe249747","With the widespread adoption of internet technology and smart devices, video has become a key medium for information dissemination, permeating nearly every aspect of social life. However, the misuse of deepfake technology has raised several social issues, such as the creation of fake videos for manipulating public opinion, financial fraud, and social engineering attacks, severely disrupting social order. Existing studies primarily focus on pure visual feature analysis and multimodal detection methods, but detection of audio-video desynchronization forgery remains relatively underexplored. This study proposes a new large-scale audiovisual forgery dataset, the Audio-Video Lip-sync Forged Dataset (AVLiFD), and introduces a novel multimodal deepfake detection method, Visual and Audio Deepfake Detection via Lip Features (VAD-Lip). The method combines visual and audio features specifically for detecting deepfake videos forged through audio, video, and audiovisual joint manipulation leading to audio-visual desynchronization. Experimental results demonstrate that the proposed method achieves high detection accuracy for audio-video joint forgery, providing new data support and technical solutions for research in this area. On the AVLiFD dataset, our model achieved an accuracy of 92.50% and an AUC of 0.9518, outperforming other models. On the Celeb-DF, FF++, and DFDC datasets, it also achieved high accuracy rates of 94.67%, 93.67%, and 92.80%, respectively. © 2025 Copyright held by the owner/author(s).","Deepfake Detection; Lip Features; Multi-Modal; Visual-Audio"
"Gao, Q.; Zhang, B.; Wu, J.; Luo, W.; Teng, Z.; Fan, J.","Gao, Qi (59680862500); Zhang, Baopeng (57211546343); Wu, Jianghao (58260652500); Luo, Wenxin (58684184300); Teng, Zhu (36553668200); Fan, Jianping (7402795255)","59680862500; 57211546343; 58260652500; 58684184300; 36553668200; 7402795255","Leveraging facial landmarks improves generalization ability for deepfake detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000788682&partnerID=40&md5=77411aebf9ee5caf6aa8528e59da5c65","Recently, facial forgery technology has become increasingly sophisticated and published datasets aim to cover a wide range of data variations. Existing deepfake detection models have benefited from the powerful feature embedding of deep networks and carefully designed fine-tuning modules, resulting in an excellent performance on in-dataset evaluations. However, the performance declines in cross-dataset evaluations due to various forgery methods and dataset shifts. In this study, we concentrate on the generalization issue of deepfake detection and find that forgery traces appear to gather around the facial interest points even manipulated by different forgery methods. To facilitate this, we propose a Trail Tracing Network (TTNet) to capture the generalized feature representation, which leverages facial landmarks to eliminate redundant information and expand the forged traces in the feature space. We conduct extensive experiments on the widely employed benchmarks, including FaceForensics++, DFDCp, and Celeb-DF. Experimental results demonstrate the outstanding generalization ability of our method against existing state-of-the-art methods by a large margin. In addition, the proposed method also exhibits excellent performance on the in-dataset evaluation. © 2025 Elsevier Ltd","Deepfake detection; Facial landmarks; Generalization"
"Sun, Z.; Ruan, N.","Sun, Zekun (57223991663); Ruan, Na (54684943100)","57223991663; 54684943100","GANK: Dynamic Geometric and Appearance Features for Efficient and Robust Detection of Face Forgery","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003735268&partnerID=40&md5=c3dd6615003268518e2bc1c01798279d","Deepfakes refers to various deep-learning-based techniques that manipulate the face in videos. Maliciously manufactured face forgeries could result in serious problems such as portrait infringement, information confusion, or even public panic. Previous countermeasures focused mainly on promoting detection accuracy while relatively overlooking robustness and computational overhead. In this work, we propose an efficient and robust framework named GANK, which discriminates Deepfake videos through temporal modeling on decoupled geometric and appearance features. A temporal denoising technique featuring landmark tracking and Kalman filtering is introduced to optimize the feature sequences, and multi-stream Recurrent Neural Networks (RNN) are constructed for sufficient exploitation of dynamic features. Besides, we introduce two optimizations to alleviate overfitting and enhance the utilization of temporal information, including channel-wise dropout and temporal random cropping. Our framework achieves outstanding robustness using very lightweight network backbones, reaching state-of-the-art performance on multiple benchmarks. © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Deepfake detection; Face forgery; Facial landmarks; Multimodal learning; Video analysis"
"Sohail, S.; Sajjad, S.M.; Zafar, A.; Iqbal, Z.; Muhammad, Z.; Kazim, M.","Sohail, Saud (59754568500); Sajjad, Syed Muhammad (56342241200); Zafar, Adeel (55602568100); Iqbal, Zafar (57770211300); Muhammad, Zia (56387287500); Kazim, Muhammad (59064523100)","59754568500; 56342241200; 55602568100; 57770211300; 56387287500; 59064523100","Deepfake Image Forensics for Privacy Protection and Authenticity Using Deep Learning","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003539462&partnerID=40&md5=6c03e4247ecd7df7857ec8f7e05e6b6f","This research focuses on the detection of deepfake images and videos for forensic analysis using deep learning techniques. It highlights the importance of preserving privacy and authenticity in digital media. The background of the study emphasizes the growing threat of deepfakes, which pose significant challenges in various domains, including social media, politics, and entertainment. Current methodologies primarily rely on visual features that are specific to the dataset and fail to generalize well across varying manipulation techniques. However, these techniques focus on either spatial or temporal features individually and lack robustness in handling complex deepfake artifacts that involve fused facial regions such as eyes, nose, and mouth. Key approaches include the use of CNNs, RNNs, and hybrid models like CNN-LSTM, CNN-GRU, and temporal convolutional networks (TCNs) to capture both spatial and temporal features during the detection of deepfake videos and images. The research incorporates data augmentation with GANs to enhance model performance and proposes an innovative fusion of artifact inspection and facial landmark detection for improved accuracy. The experimental results show near-perfect detection accuracy across diverse datasets, demonstrating the effectiveness of these models. However, challenges remain, such as the difficulty of detecting deepfakes in compressed video formats, the need for handling noise and addressing dataset imbalances. The research presents an enhanced hybrid model that improves detection accuracy while maintaining performance across various datasets. Future work includes improving model generalization to detect emerging deepfake techniques better. The experimental results reveal a near-perfect accuracy of over 99% across different architectures, highlighting their effectiveness in forensic investigations. © 2025 by the authors.","AI-generated content detection; cybersecurity and privacy protection; deep learning; deepfake detection; digital media authenticity; facial landmark detection; image forensics; manipulated media detection; real-time deepfake detection; video forensics"
"Zhang, Y.; Niu, R.; Zhang, X.; Chen, S.; Wang, M.; Li, X.","Zhang, Yue (59069118800); Niu, Run (57224592978); Zhang, Xianlin Zhang Z. (57195488768); Chen, Siqi (58176980000); Wang, Mingdao (57779848000); Li, Xueming (56031046700)","59069118800; 57224592978; 57195488768; 58176980000; 57779848000; 56031046700","Exploring coordinated motion patterns of facial landmarks for deepfake video detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000031996&partnerID=40&md5=7f34599c870d1590ca86028c7d989958","Due to the rich geometric and motion information they contain, recent studies indicate that facial landmark clues have significant potential for deepfake video detection. In this paper, we make a key observation that there exist coordinated motions among different facial landmarks for real individuals. While the forgery methods focus more on appearance realism, thus likely to disrupt the underlying coordinated motion patterns. Inspired by this observation, this paper explores how to leverage coordinated motion patterns among facial landmarks to enhance deepfake detection. First, we introduce a coordinated motion landmarks mining strategy (CMLMS), to effectively identify correlated landmarks. Utilizing these correlations, we propose a landmark temporal dynamic relation module (LTDRM), which focuses on the coordinated motion patterns between landmarks while extracting their spatiotemporal features. Specifically, LTDRM constructs an adjacency matrix based on the correlated landmarks and uses graph convolution to selectively aggregate information between correlated landmarks. Additionally, LTDRM is a plug-and-play module and can boost the performance of existing deepfake detection methods with minimal computational overhead. Experimental results validate the effectiveness and generalizability of our method. © 2025 Elsevier B.V.","Coordinated motion; Deepfake detection; Facial landmark"
"El-Taj, H.; Alammari, F.; Alkhowaiter, J.; Bogari, L.; Essa, R.","El-Taj, Homam (35317497100); Alammari, Fatima (59561799300); Alkhowaiter, Joud (59560797100); Bogari, Layal (59561366400); Essa, Renad (59561799400)","35317497100; 59561799300; 59560797100; 59561366400; 59561799400","Deepfake Detection Based on Visual Lip-sync Match and Blink Rate","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218749969&partnerID=40&md5=9fb6b9c809efbe37274893104a918cfd","Deepfake technology has emerged as a significant challenge to the authenticity of digital media, necessitating innovative detection methods. This paper introduces TrueSync, an advanced application for detecting deepfake videos by integrating two critical detection features: lip-sync analysis and blink rate monitoring. Leveraging a hybrid approach combining CNN-LSTM and SyncNet models, TrueSync processes visual and temporal features to identify anomalies in lip movement synchronization and eye blinking patterns. The application utilizes a modular pipeline to analyse these features independently and then fuses the results for a comprehensive detection score. This approach enhances detection accuracy and provides users with reliable tools to combat sophisticated manipulations. By proposing this scalable solution, TrueSync addresses the increasing difficulty in distinguishing authentic videos from manipulated content, fostering trust in digital media. © IJCESEN.","Artificial Intelligence; Blink Rate; Deepfake Detection; visual lip-sync matching"
"Badoni, P.; DIldar, M.S.; Ahmed, M.N.; Zamani, A.S.; Hussain, M.R.; Wadhwa, M.","Badoni, Parveen (58257583600); DIldar, Muhammad Shahid (57194213565); Ahmed, Mohammad Nadeem (57135903900); Zamani, Abu Sarwar (57295189700); Hussain, Mohammad Rashid (57205669651); Wadhwa, Manoj (26421405100)","58257583600; 57194213565; 57135903900; 57295189700; 57205669651; 26421405100","The Growing Need for Deepfakes Detection in the Age of AI-Generated Media","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017418157&partnerID=40&md5=150c72973a2ed043333c6845b6dba2e1","Deepfakes detection is now a necessity because deep learning methods for producing highly realistic synthetic media are advancing at an alarming rate. These doctored photos, videos, and audio files can mislead their audience, disseminate disinformation, and represent a major security risk. Detection techniques utilize convolutional neural networks, recurrent neural networks, and transformer models to detect minor artifacts, inconsistencies, and unnatural patterns in multimedia content. Methods such as frequency analysis, facial landmark tracking, and adversarial training are used to improve detection accuracy. The ongoing development of generative models requires constant research to create resilient and flexible detection systems that can counter the threats from synthetic media manipulation. © 2025 IEEE.","adversarial training; convolutional neural networks; Deepfakes detection; digital security; facial landmark tracking; generative models; misinformation; multimedia forensics; synthetic media"
"Ming, L.; He, P.; Li, H.; Wang, S.; Jiang, X.","Ming, Liyue (57209884407); He, Peisong (57075586900); Li, Haoliang (56593801800); Wang, Shiqi (55364979300); Jiang, Xinghao (13607511800)","57209884407; 57075586900; 56593801800; 55364979300; 13607511800","Critical Contour Prior-Guided Graph Learning With Pose Calibration for Identity-Aware Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017131421&partnerID=40&md5=a2f547ae16b7f52481cbb04ef318f3da","Deepfake has recently raised severe public concerns about security issues, such as creating fake news of celebrities. As countermeasures, identity-aware detection methods leverage identity information to expose forged videos by measuring identity consistency between the suspicious input and its reference samples. However, the performance of existing methods suffers from notable degradation due to undesired variations of head poses and capturing environments. In this work, we first conduct a statistical analysis to illustrate the influence of different facial regions for forensic purposes, which infers more reliable identity information is located in critical face regions. Motivated by this analysis, we propose a graph learning-based identity-aware deepfake detection framework considering critical contour prior as guidance. First, feature sampling based on contour landmarks is applied to construct the graph data as the input of our critical contour prior-guided graph attention network (CP-GAT), where a node position prediction task is constructed as auxiliary supervision to explore rich relationships between nodes. To enhance pose-invariant ability, a rotation compensation block is integrated into CP-GAT and trained using a pose-calibrated contrastive learning to extract identity features, which takes high-quality front faces as the calibration goal with a progressively updating selection. Besides, an adversarial node masking-based training strategy is proposed as feature augmentation to further enhance the reliability. During the inference stage, the similarity between identity features of the input sample and its reference samples extracted by the trained CP-GAT is used to obtain the detection result. Extensive experiments are conducted on various face forgery datasets and state-of-the-art methods are compared to verify the superiority of the proposed method in terms of detection capability and robustness. © 1999-2012 IEEE.","contrastive learning; Deepfake detection; graph learning; video forensics"
"Sheelavantmath, K.K.; Isloor, V.; Sheetal, B.; Bhuvisha, N.; Sandesh, B.J.","Sheelavantmath, Khushi Kiran (60098002600); Isloor, Vaibhav (60097910400); Sheetal, B. V. (58069100200); Bhuvisha, N. (60097910500); Sandesh, Balasaraswati J. (57194794854)","60098002600; 60097910400; 58069100200; 60097910500; 57194794854","Localization of Deepfake Facial Images Through U-NET Architecture","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015790368&partnerID=40&md5=b96159862bbae2c9f51061dfa8a23492","Traditional deep fake detection methods predominantly rely on binary classification, categorizing images as either real or fake, which often limits their transparency and interpretability for real-world applications. To address this, we propose a novel approach that integrates Histogram of Oriented Gradients (HOG) features into a U-Net architecture, enabling the capture of both fine-grained texture patterns and high-level contextual information in deep fake regions. For enhanced interpretability, Class Activation Maps (CAM) are transformed into the HSV color space, facilitating intuitive color-based analysis. Contour detection is then employed to localize manipulated regions, and bounding boxes are generated using Dlib to precisely identify these features. By iterating through 68 facial landmarks, textual descriptions are generated to highlight alterations in facial features, providing a detailed understanding of the modifications. To further quantify the extent of manipulation, the resulting feature map undergoes a quantitative assessment by calculating the percentage of contour-detected areas. This method offers a comprehensive framework for identifying and analyzing subtle differences between genuine and modified images, advancing the transparency and accuracy of deep fake detection. ©2025 IEEE.","Class Activation Maps (CAM); Contour Detection; Deepfake Detection; Dlib; Facial Landmark Analysis; Histogram of Oriented Gradients (HOG); HSV Color Space; U-Net Architecture"
"Karunasena, G.M.K.B.; Herath, H.M.K.K.M.B.; Rathnayake, R.M.P.M.D.; Priyankara, H.D.N.S.; Madhusanka, B.G.D.A.","Karunasena, G. M.K.B. (57224305971); Herath, H. M.K.K.M.B. (57224317946); Rathnayake, R. M.P.M.D. (59563363900); Priyankara, H. D.N.S. (57215341629); Madhusanka, B. G.D.A. (57151317900)","57224305971; 57224317946; 59563363900; 57215341629; 57151317900","Detecting Deepfakes Through the Classification of Facial Active and Passive Features Using Machine Learning","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003055125&partnerID=40&md5=b5095c7b9e1e6c95484f907e06af4941","Recently, there has been a much public concern over fake photos and films that contain facial information created by digital modifications, especially with deepfakes techniques. The widely used term “Deepfakes” describes a deep learning technique that swaps faces to produce fake identities. The distinction between actual and deepfake videos has become crucial due to their detrimental effects on the world. An intriguing vulnerability within live deepfakes has largely gone unnoticed by the security research community thus far. This research may be attributed to real-time deepfakes during video calls and conferences, which have recently become a notable cause for concern. Developing and evaluating existing deepfake detection methods requires large-scale datasets. Still, the deepfake datasets available today have inadequate image quality and do not appear like the deepfake videos that have been making the rounds on the Internet. This research presents a novel way for recognizing synthetic media produced using deepfake: an active and passive facial features based end-to-end deep neural network classifier model (AFPF-DNNC model). The facial landmark detector recognizes and extracts the area from each individual video frame that contains the human face, then extracted facial regions used to extract active features in the faces. Simultaneously, a convolutional neural network (CNN) extracts passive features from these faces. The retrieved features are inputs into the end-to-end deep neural network classifier (DNNC), which is the last recognition step in the detection system. The proposed algorithm achieves 90.32% of an area under the receiver operating characteristic curve (AUC), 90.6 % accuracy, 90 % specificity, 91.3 % recall, 87.5 % precision, and 89.3 % F-score on the Celeb-DF (v2) a new deepfake forensics dataset. Finally, the experimental investigation validates the suggested method’s effectiveness over the most advanced techniques. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Active facial features; AFPF-DNNC model; AI; Deepfake detection; End-to-end deep neural networks; Facial landmark detection; Passive facial features"
"Wang, T.; Huang, M.; Cheng, H.; Zhang, X.; Shen, Z.","Wang, Tianyi (57211200251); Huang, Mengxiao (58712721500); Cheng, Harry (57226475825); Zhang, Xiao (59419465000); Shen, Zhiqi (57211681855)","57211200251; 58712721500; 57226475825; 59419465000; 57211681855","LampMark: Proactive Deepfake Detection via Training-Free Landmark Perceptual Watermarks","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209806841&partnerID=40&md5=3cfaf37e9a761ffa5e92f3e117c1f46c","Deepfake facial manipulation has garnered significant public attention due to its impacts on enhancing human experiences and posing privacy threats. Despite numerous passive algorithms that have been attempted to thwart malicious Deepfake attacks, they mostly struggle with the generalizability challenge when confronted with hyper-realistic synthetic facial images. To tackle the problem, this paper proposes a proactive Deepfake detection approach by introducing a novel training-free landmark perceptual watermark, LampMark for short. We first analyze the structure-sensitive characteristics of Deepfake manipulations and devise a secure and confidential transformation pipeline from the structural representations, i.e. facial landmarks, to binary landmark perceptual watermarks. Subsequently, we present an end-to-end watermarking framework that imperceptibly and robustly embeds and extracts watermarks concerning the images to be protected. Relying on promising watermark recovery accuracies, Deepfake detection is accomplished by assessing the consistency between the content-matched landmark perceptual watermark and the robustly recovered watermark of the suspect image. Experimental results demonstrate the superior performance of our approach in watermark recovery and Deepfake detection compared to state-of-the-art methods across in-dataset, cross-dataset, and cross-manipulation scenarios. © 2024 ACM.","deepfake detection; digital forensics; landmark perceptual watermark; robust watermarking"
"Saif, S.; Tehseen, S.; Ali, S.S.","Saif, Shahela (57195215570); Tehseen, Samabia (36148756300); Ali, Syed Sohaib (59955271500)","57195215570; 36148756300; 59955271500","Fake news or real? Detecting deepfake videos using geometric facial structure and graph neural network","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195171350&partnerID=40&md5=e7b132d9029fd6ad387c7ed6a02b087b","Deepfake videos are increasingly used in spreading fake news or propaganda having a serious impact on people and society. Traditional deepfake detectors exploit spatial and/or temporal inconsistencies to differentiate between real and fake videos. Owing to the rapidly advancing deepfake creation algorithms, the latest detectors have made use of physiological and biological facial features to create more generic solutions. Our proposed solution uses facial landmarks as the physiological identifiers of a person's face and through them develops a relationship between facial areas in normal speech and tampered speech. By creating a graph structure from the resulting sparse data, we were able to use a spatio-temporal graph convolutional network for classification, which has significantly fewer parameters and a shorter training time than traditional CNNs. We conducted a multitude of experiments on 3 datasets, utilizing spatio-temporal features. The results demonstrate that this technique has better generalization, and high performance compared to latest research in deepfake detection without the reliance on large deep learning models which are tuned to learning image discrepancies more than data patterns. Moreover, our use of facial landmark-based features with a graph structure paves the way for the development of an explainable AI model that can be relied on. © 2024 Elsevier Inc.","Deepfake detection; Deepfake videos; Deepfakes; Forgery detection; Graph convolution network; Video forgery"
"Sultan, D.A.; Ibrahim, L.M.","Sultan, Duha A. (57218379583); Ibrahim, Laheeb Mohammed (36809457500)","57218379583; 36809457500","Deepfake Detection Model Based on VGGFace with Head Pose Estimation Technique","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197739021&partnerID=40&md5=cdeca50ab91ec6215e140441eabd1593","Rapid developments in deepfake technology have produced hyper-realistic fake media such as images, audio, and video. Especially, the fabricated videos, which have gained a large widespread in social media sites. Because of the great harm inflicted by these videos, many researchers have increased their efforts to find a reliable method to identify and distinguish these fake videos from the actual ones. In this work, we suggested a new model to detect fake videos, based on two major techniques. First, the VGGFace model was used to extract the most important facial features combined with, second: the estimation of the head pose angle that represents the relative orientation of the human face in video frames. All these calculations are done on human faces detected and cropped from video frames, where 10, 20, and 30 frames were extracted from each video. FF++ dataset was used to train and test the model, which produced a max test accuracy of 0.885. The code was written using Python version 3.9. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","CNN; Deepfake Detection; Deeplearning; Head Pose; VGGFace"
"Ramadhani, K.N.; Munir, R.; Utama, N.P.","Ramadhani, Kurniawan Nur (56412050200); Munir, Rinaldi (35176324300); Utama, Nugraha Priya (58449636000)","56412050200; 35176324300; 58449636000","Improving Video Vision Transformer for Deepfake Video Detection Using Facial Landmark, Depthwise Separable Convolution and Self Attention","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182930350&partnerID=40&md5=577126d43b07d58f9d9c0277cf7f42a1","In this paper, we present our result of research in video deepfake detection. We built a deepfake detection system to detect whether a video is a deepfake or real. The deepfake detection algorithm still struggle in providing a sufficient accuracy values, especially in challenging deepfake dataset. Our deepfake detection system utilized spatiotemporal feature that extracted using Video Vision Transformer (ViViT). The main contribution of our research is providing a deepfake detection system that based on ViViT architecture and using landmark area images for the input of the system. Our system extracted the feature from a number of spatial features. The spatial feature was extracted using Depthwise Separable Convolution (DSC) block combined with Convolution Block Attention Module (CBAM) from tubelet. The tubelet was a representation of facial landmark area that was extracted from the input video. In our system, we used 25 facial landmark area for an input video. In our experiment we used Celeb-DF version 2 dataset because it is considered to be a challenging deepfake dataset. We conducted augmentation to the dataset, so we obtained 8335 videos for training set, 390 videos for validation set, and 1123 videos for testing set. We trained our deepfake detection system using Adam optimizer, with learning rate of 10-4 and 100 epoch. From the experiment, we obtained the accuracy score of 87.18% and F1 score of 92.52%. We also conducted the ablation study to display the effect of each part of our model to the overall system performance. From this research, we obtained that by using landmark area images, our ViViT based deepfake detection system had a good performance in detecting deepfake videos. © 2013 IEEE.","convolution block attention module; Deepfake detection; depthwise separable convolution; facial landmark; video vision transformer"
"D'Amelio, A.; Lanzarotti, R.; Patania, S.; Grossi, G.; Cuculo, V.; Valota, A.; Boccignone, G.","D'Amelio, Alessandro (57200181244); Lanzarotti, Raffaella (6507361650); Patania, Sabrina (57700644300); Grossi, Giuliano (57211910215); Cuculo, Vittorio (56529589400); Valota, Andrea (58636644400); Boccignone, Giuseppe (7003837461)","57200181244; 6507361650; 57700644300; 57211910215; 56529589400; 58636644400; 7003837461","On Using rPPG Signals for DeepFake Detection: A Cautionary Note","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173583929&partnerID=40&md5=f63460637814a9cf8fe5eea6c5716937","An experimental analysis is proposed concerning the use of physiological signals, specifically remote Photoplethysmography (rPPG), as a potential means for detecting Deepfakes (DF). The study investigates the effects of different variables, such as video compression and face swap quality, on rPPG information extracted from both original and forged videos. The experiments aim to understand the impact of face forgery procedures on remotely-estimated cardiac information, how this effect interacts with other variables, and how rPPG-based DF detection accuracy is affected by these quantities. Preliminary results suggest that cardiac information in some cases (e.g. uncompressed videos) may have a limited role in discriminating real videos from forged ones, but the effects of other physiological signals cannot be discounted. Surprisingly, heart rate related frequencies appear to deliver a significant contribution to the DF detection task in compressed videos. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.","Deepfake detection; Physiological signals; rPPG; Video forensics"
"Li, M.; Liu, B.; Hu, Y.; Zhang, L.; Wang, S.","Li, Meng (57218087423); Liu, Beibei (55544736500); Hu, Yongjian (35766130600); Zhang, Liepiao (57237902400); Wang, Shiqi (55364979300)","57218087423; 55544736500; 35766130600; 57237902400; 55364979300","Deepfake Detection Using Robust Spatial and Temporal Features from Facial Landmarks","2021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113890116&partnerID=40&md5=9d04c2b12f0902853bc25a3e991ffa80","Most current deepfake detectors may suffer the decrease of detection accuracy under common video processing like compression. To deal with this issue, we proposed a new way of using biometric features for robust deepfake detection. Our biometric features are derived from a set of selected facial landmarks. We first presented a metric to select robust facial landmarks, and then constructed facial feature vectors with the selected landmarks. The spatial angles and the temporal rotation angles are introduced to facilitate the construction of the SVM feature vector. In essence, we use the spatial angles and temporal rotation angles to characterize the inherent consistency of facial landmarks at both frame level and video level. Experimental results have demonstrated that our detector has the best robustness compared with 6 current methods. It also has good scores in AUC (Area Under the Receiver Operating Characteristic Curve) and detection accuracy. © 2021 IEEE.","Deepfake detection; Facial landmarks; Robustness; Spatial features; Temporal features"
"Younus, M.A.; Hasan, T.M.","Younus, Mohammed Akram (57217854027); Hasan, Taha Mohammed (54782466800)","57217854027; 54782466800","Abbreviated View of Deepfake Videos Detection Techniques","2020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087722089&partnerID=40&md5=847ed50abbf46f0880d1c925dab256ef","In the era of technological advances and a qualitative breakthrough in the artificial intelligence field and deep neural networks, a new age of hyper-realistic digital videos forgery called DeepFake has been born, with that new technology, it is difficult to distinguish between real videos and fake ones which are uploaded daily on various websites across the Internet. Many open-source DeepFake creation methods have risen, leading to a growing number of synthesized media clips over the internet. There are many efficient fast methods and techniques which have been designed to detect and spot such phenomenon. Background Comparison, Temporal Pattern Analysis, Eye blinking, Facial Artifacts, Mesoscopic Analysis, and Pose Estimation are some of those techniques. Some of these approaches designed to detect and identify the video forgery without any prior enlightenment concerning the videos under analysis. The primary scope of this study is to provide an abbreviate review of these methodologies of a method-comparison study that has been presented to assist the researcher's evaluation of such studies. © 2020 IEEE.","Deepfake Detection techniques; Digital videos forgery; Facial Artifacts; Mesoscopic"
"Yang, X.; Li, Y.; Lyu, S.","Yang, Xin (57212206392); Li, Yuezun (57188647738); Lyu, Siwei (8727557200)","57212206392; 57188647738; 8727557200","Exposing Deep Fakes Using Inconsistent Head Poses","2019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069004552&partnerID=40&md5=dbaf7b398e245b4185fc2c6de97b11a6","In this paper, we propose a new method to expose AI-generated fake face images or videos (commonly known as the Deep Fakes). Our method is based on the observations that Deep Fakes are created by splicing synthesized face region into the original image, and in doing so, introducing errors that can be revealed when 3D head poses are estimated from the face images. We perform experiments to demonstrate this phenomenon and further develop a classification method based on this cue. Using features based on this cue, an SVM classifier is evaluated using a set of real face images and Deep Fakes. © 2019 IEEE.","DeepFake Detection; Head Pose Estimation; Media Forensics"
