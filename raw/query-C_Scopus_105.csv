"Authors","Author full names","Author(s) ID","Title","Year","Link","Abstract","Author Keywords"
"Chen, Z.; Wang, S.; Wang, Z.","Chen, Zhong (57103925800); Wang, Siyang (55805815000); Wang, Zuxi (7410051639)","57103925800; 55805815000; 7410051639","Dual-stream temporal-spatial forgery detection via phase-consistent edge features and 3D visual state-space modeling","2026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015384367&partnerID=40&md5=2041ac6891834f5d8773a64c8ac9317d","Deepfake, as a generative technology, has opened up new avenues for the development of the film, television, and art industries. However, its abusive use has triggered serious social security threats, such as infringement of portrait rights and the spread of misinformation, which has drawn widespread attention to research on deepfake detection techniques. Current deep learning-based face forgery detection methods face critical challenges: 1) insufficient focus on common forgery traces leads to poor generalization performance on datasets generated by unknown forgery methods; 2) traditional spatio-temporal feature fusion mechanisms struggle to balance the representational weights of spatial details and temporal dynamics, and exhibit inadequate robustness against post-processing operations like compression and cropping. To address these issues, this paper first designs a phase consistency edge artifact mining module is designed to extract common forgery traces from edge textures by leveraging the deep-phase information of images, significantly enhancing the model's generalization ability. Second, a multi-frame synthesis strategy is designed to effectively integrate spatial and temporal features while balancing the network's attention to these two feature domains. Third, a visual state-space model based on 3D scanning is designed, which for the first time employs the Mamba model to analyze spatio-temporal forgery patterns, notably improving the robustness of the model against unknown perturbations. Experimental results on standard benchmarks–FaceForensics++, Celeb-DFv2, WildDeepfake and DFDC(Preview)–demonstrate that the proposed method achieves state-of-the-art performance in three core dimensions: detection accuracy, cross-dataset generalization, and robustness against perturbations. © 2025 Elsevier Ltd","Deepfake detection; Multi-frame synthesis; Phase consistency; Visual state-space model"
"T.; Li, G.; Xiao, Y.; Tian, H.; Cao, Y.","; Li, Gen (59736920700); Xiao, Yanhui (36697381400); Tian, Huawei (36615909900); Cao, Yun (55470378800)","60231562700; 59736920700; 36697381400; 36615909900; 55470378800","Enhanced deepfake detection via dynamic data augmentation and spatiotemporal attention","2026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105024347588&partnerID=40&md5=dfbab16a02c2f9e55ed2f36bf6dc384b","Recent advancements in Deepfake technology have raised significant concerns regarding the authenticity of online media. Misuse of Deepfakes poses threats to privacy and economic security. To address the generalization issues faced by existing Deepfake detection methods, we propose a novel framework combining forgery-aware Dynamic Data Augmentation (DDA) and Cross-Frame Multi-Head Attention (CMA). DDA generates forgery heatmaps using spatial features extracted by Swin Transformer, guiding region-level data augmentation dynamically. CMA extracts temporal inconsistencies across frames using spatial features. Spatiotemporal features are fused and fed into a Bidirectional GRU (Bi-GRU) to model sequential dependencies. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods in cross-compression and cross-forgery-type evaluations. Our code and datasets are publicly available at https://github.com/luanmianv/DDA-CMA. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.","Cross-frame multi-head attention; Deepfake detection; Dynamic data augmentation; Spatiotemporal feature fusion"
"Nguyen, N.M.; Thuan, N.D.","Nguyen, Minh Nhut (57212857862); Thuan, Nguyen Dinh (55520800900)","57212857862; 55520800900","MobilKAN-H: Cross-Dataset Deepfake Detection Using a Hybrid MobileNet-LSTM-HMM-KAN Architecture","2026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023638416&partnerID=40&md5=6ebec32c52ff567c641b36fa8ca38ac1","Deepfakes threaten the trustworthiness of online media and public discourse. We present a lightweight yet effective hybrid for facial deepfake detection that couples MobileNet spatial encoders with sequence-level LSTM, probabilistic Hidden Markov Models (HMM) for temporal regularization, and Kolmogorov–Arnold Networks (KAN) for adaptive, interpretable nonlinear modeling. On FaceForensics++ and Celeb-DF v2—two widely used and challenging benchmarks—our best variant achieves strong cross-validated accuracy, outperforming both our baselines and previously reported methods on these datasets. We report precision, recall, F1-score, and AUC, and discuss training choices that yield robust generalization under video compression. Our results suggest that combining LSTM-based sequence modeling with HMM smoothing and KAN adapters is a practical path toward reliable, low-latency deepfake screening in real-world pipelines. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2026.","Celeb-DF v2; Deepfake detection; FaceForensics++; Hidden Markov Model; Kolmogorov–Arnold Network; LSTM; MobileNet"
"Zhang, L.; Liu, B.; Chu, Q.; Yu, N.","Zhang, Li (60200276300); Liu, Bin (56431524800); Chu, Qi (57219372015); Yu, Neng-Hai (7201981769)","60200276300; 56431524800; 57219372015; 7201981769","Multimodal Consistency-Driven Deepfake Detection","2026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022185336&partnerID=40&md5=cf82fedb6880b93b0cf94a189f1253e8","The emergence of multimodal deepfake videos demands detection systems that address synchronized audio-visual manipulations. We present AVMCD, a novel framework combining dual-stream transformer architecture with cross-modal consistency verification. Our solution introduces three key innovations: (1) Joint spatiotemporal modeling using Video Vision Transformers (ViViT) for facial dynamics and Audio Spectrogram Transformers (AST) for spectral speech patterns; (2) A synchronization analysis module employing cross-module features to detect audio-visual temporal mismatches; (3) Hybrid learning integrating one-class classification with multi-task consistency constraints. The framework overcomes critical limitations in existing approaches by explicitly modeling cross-modal interactions while preventing overfitting to single-modality artifacts. Comprehensive evaluations on FakeAVCeleb demonstrate state-of-the-art performance with 96.1% ACC, surpassing leading audio-visual methods by 12.4% ACC improvement. This work establishes a new paradigm for multimodal deepfake detection through systematic integration of transformer-based feature fusion and physiological consistency verification. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2026.","Audio-visual Consistency; Deepfake detection; Transformer networks"
"Ben Jabra, M.; Cheikhrouhou, O.; BenAmor, A.","Ben Jabra, Marwa (57195283226); Cheikhrouhou, Omar (35112771500); BenAmor, Anouar (56912231600)","57195283226; 35112771500; 56912231600","Leveraging temporal attention and bidirectional modeling for robust deepfake video detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021244301&partnerID=40&md5=c9780b50f99ee54d84524e35d6c27c58","The growing sophistication of deepfake videos threatens media authenticity, requiring detectors that are accurate and robust to real-world distortions. We design and compare two deep learning architectures for video-based detection. The baseline fuses ResNeXt50 with a unidirectional LSTM, while the proposed model combines EfficientNet-B0 with a Bidirectional LSTM and temporal attention to capture bidirectional dependencies and emphasize informative frames. Both are trained and evaluated on Celeb-DF v2 under identical settings for fair comparison. Results show the proposed model outperforms the baseline, reaching 95.3% accuracy and a ROC-AUC of 0.981. These findings highlight the value of efficient convolutional encoders with temporal attention for interpretable and competitive deepfake detection, with applications in media forensics and verification. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.","BiLSTM; Celeb-DF v2; Deep learning; Deepfake detection; EfficientNet; LSTM; Media authenticity; ResNeXt; Spatiotemporal modeling; Temporal attention; Video forensics"
"Han, R.; Wang, X.; Bai, N.; Hou, J.; Zhang, W.; Li, J.","Han, Ruidong (58199903300); Wang, Xiaofeng (7501854020); Bai, Ningning (58102462000); Hou, Jianpeng (58861477200); Zhang, Wenshuo (60041028100); Li, Jianghua (55800222700)","58199903300; 7501854020; 58102462000; 58861477200; 60041028100; 55800222700","HSFF-Net: Hierarchical spectral-feature fusion network for deepfake detection and localization","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013114740&partnerID=40&md5=66d399fe80dc48060d8a44e3daa35710","The rapid development of deepfake techniques poses a serious threat to multimedia authenticity, driving increased attention to deepfake detection. However, most existing methods focus solely on classification while overlooking forgery localization, which is essential for understanding manipulation intent. To address this issue, we propose a novel Hierarchical Spectral-Feature Fusion Network (HSFF-Net) for deepfake detection and localization from spatial- and frequency-domain views. Specifically, the Spectral Detail Amplification (SDA) module enhances tampering cues around facial features in the frequency domain. The Dynamic Collaborative Fusion (DCF) unit integrates complementary dual-stream features across multiple hierarchical levels to highlight valuable information. The Adaptive Feature Elevation (AFE) module bridges coarse semantic and fine-grained details in a top-down manner. Furthermore, the Global Guidance Exposure (GGE) module injects localization cues across feature levels to improve forgery localization accuracy. Additionally, we design the contrastive clustering loss for the detection task, which guides features to cluster around their corresponding class centers while simultaneously pushing them away from other class centers, thereby promoting intra-class compactness and inter-class separability. Abundant experiments demonstrate that HSFF-Net achieves superior performance on both detection and localization tasks, with good generalization across diverse datasets and robustness against various perturbations. © 2025","Adaptive feature elevation; Contrastive clustering loss; Deepfake detection and localization; Dynamic collaborative fusion; Global guidance exposure; Spectral detail amplification"
"Xu, W.; Wu, J.; Lu, W.; Luo, X.; Wang, Q.","Xu, Wenbo (57222359967); Wu, Junyan (58937430900); Lu, Wei (57715097700); Luo, Xiangyang (8976166200); Wang, Qian (56856235900)","57222359967; 58937430900; 57715097700; 8976166200; 56856235900","A Multimodal Deviation Perceiving Framework for Weakly-Supervised Temporal Forgery Localization","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105024076843&partnerID=40&md5=38b7f20dc18f1273e71edabd0fc77f1d","Current researches on Deepfake forensics often treat detection as a classification task or temporal forgery localization problem, which are usually restrictive, time-consuming, and challenging to scale for large datasets. To resolve these issues, we present a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization (MDP), which aims to identify temporal partial forged segments using only video-level annotations. The MDP proposes a novel multimodal interaction mechanism (MI) and an extensible deviation perceiving loss to perceive multimodal deviation, which achieves the refined start and end timestamps localization of forged segments. Specifically, MI introduces a temporal property preserving cross-modal attention to measure the relevance between the visual and audio modalities in the probabilistic embedding space. It could identify the inter-modality deviation and construct comprehensive video features for temporal forgery localization. To explore further temporal deviation for weakly-supervised learning, an extensible deviation perceiving loss has been proposed, aiming at enlarging the deviation of adjacent segments of the forged samples and reducing that of genuine samples. Extensive experiments demonstrate the effectiveness of the proposed framework and achieve comparable results to fully-supervised approaches in several evaluation metrics. © 2025 ACM.","deepfake detection; multimodal; temporal forgery localization; weakly-supervised"
"Babu, V.S.; Sathya, M.; Maheshwari, R.U.; Subha, K.J.","Babu, V. Suresh (59995030000); Sathya, Mariappan Arul (57516794700); Maheshwari, R. Uma (59454701500); Subha, K. J. (57503532300)","59995030000; 57516794700; 59454701500; 57503532300","Deepfake Detection Utilizing Enhanced Silicon-Based Physically Unclonable Functions Integrated with Photonic Crystal Fiber Sensor","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010591738&partnerID=40&md5=6d6839e3d34a1ddf89979ac92475145c","This study introduces an advanced silicon-based physically unclonable function (PUF) integrated with Photonic Crystal Fiber (PCF) sensors, aimed at enhancing the robustness and reliability in deepfake detection devices. Leveraging the inherent unpredictability of silicon-based Physical Unclonable Functions (PUFs), combined with the sensitivity of PCF sensors, we propose a novel system for secure, accurate deepfake image detection utilizing hybrid Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks. The proposed architecture demonstrates significant improvement, achieving an average detection accuracy of 98.6%, surpassing existing models by 7.3%. Additionally, our integrated approach exhibits enhanced robustness, reducing false-positive rates by 15% and false negatives by 13.2% compared to conventional methods. Experimental evaluations confirm that the integration of silicon-based PUFs with PCF sensors not only strengthens the resilience against adversarial attacks but also enhances reliability under varying environmental conditions. This work offers a promising pathway toward advanced, secure, and high-performance deepfake detection solutions, suiTable for real-world deployment in cybersecurity applications. © The Author(s), under exclusive licence to Springer Nature B.V. 2025.","Convolutional neural networks (CNN) and long short-term memory (LSTM) networks; Deepfake detection; Photonic crystal fiber (PCF) sensors; Physically unclonable functions (PUFs); Silicon PUFs"
"Zha, R.; Lian, Z.; Li, Q.","Zha, Ruiqi (57222155916); Lian, Zhichao (55822749800); Li, Qianmui Mu (8231309800)","57222155916; 55822749800; 8231309800","Centroid-based Contrastive Consistency Learning for transferable deepfake detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001413488&partnerID=40&md5=f72b66751f969d1229e60a9c69bbae99","Previous research efforts in deepfake detection mainly concentrated on identifying and differentiating artifacts discernible to humans. Those methods have left a bias in learned models, as they tend to concentrate on the disparities between forged and natural regions from the perspective of a single sample while overlooking consistency within categories from the perspective of the entire sample set, which remains crucial across various real-world applications. Therefore, inspired by contrastive learning, we tackle the deepfake detection problem by learning the invariant representations of both categories. Our proposed method, termed Centroid-based Contrastive Consistency Learning (C3L) method, integrates constraints on representations at both the data preprocessing and feature extraction stages. Specifically, during data preprocessing, we consider both temporal relationships within videos and the latent relationships within synthesis data. We introduce a novel Positive Enhancement Module (PEM) designed to characterize natural and forged samples in a facial semantically irrelevant way, thereby guiding a task-oriented positive pair contrasting strategy. In addition, at the feature extraction stage, we introduce the Margin Feature Simulation Module (MFSM), which leverages the centroid of the natural category to simulate marginal features for both categories. Subsequently, we employ the Supervised Contrastive Margin Loss (SCML), utilizing simulated features to emphasize differences at decision boundaries and optimize the learning process. The effectiveness and robustness of the proposed method have been demonstrated through extensive experiments. © 2025 Elsevier B.V.","Contrastive learning; Deep learning; Deepfake detection; Outlier detection"
"Khudhur, R.Z.; Mohammed, M.A.","Khudhur, Raman Z. (60080148000); Mohammed, Marwan Aziz (56494313300)","60080148000; 56494313300","A Spatio-Temporal Deep Learning Approach for Efficient Deepfake Video Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014768493&partnerID=40&md5=ae4cd97a0dd94dd0274a85dd07694c87","—Deepfake videos have grown to be a big concern in the modern digital media landscape as they cause difficulties undermining the legitimacy of channels of information and communication. Humans often find it challenging to tell the difference between a fake and a genuine video due to the increasing realism of facial deepfakes. Identification of these misleading materials is the first step in preventing deepfakes from spreading through social media. This work introduces Spatio-temporal Intelligent Deepfake Detector (STIDD), a deep learning system including enhanced spatial and temporal modeling techniques. By means of a pre-trained EfficientNetV2-B0 model, the proposed framework efficiently extracts spatial characteristics from each frame, subsequently, and Bidirectional Long Short-Term Memory layers help to capture temporal relationships from video sequences. We evaluate STIDD on the FaceForensics++ (FF++) dataset encompassing all five manipulation techniques (DeepFakes, FaceSwap, Face2Face, FaceShifter, and NeuralTextures). The experimental results reveal that STIDD achieved precision, recall, and F1-scores all higher than 0.99 and a final test accuracy of 99.51% on the combined FF++ test set. The results demonstrate that the integration of sophisticated spatial extraction and strong temporal modeling allows STIDD to achieve high detection performance while maintaining computing efficiency at just 0.39 Giga Floating-Point Operations (GFLOPs) per inference. © 2025 Raman Z. Khudhur and Marwan A. Mohammed.","Deepfake detection; EfficientNet; Index Terms—Deep learning; Spatio-temporal modeling"
"Xu, S.; Yang, X.; Xie, Z.","Xu, Shiyou (59993339000); Yang, Xiong (59993222400); Xie, Zhanghuang (58988870600)","59993339000; 59993222400; 58988870600","The future of misinformation control: integrating advanced algorithms and Blockchain for effective Deepfake detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010557399&partnerID=40&md5=10fbcfdb86275fa1658e739ba242f6f8","Fast-forwarding with respect to surface advancements in deepfake generation technologies, the digital media credibility is now under threat; there are serious threats posed to privacy and security upon deepfake video generation. It is hypothesized in the present study that employing advanced algorithms along with blockchain technology will greatly enhance the accuracy and security of deepfake detection systems. To this end, a novel framework for detecting deepfake combinations was designed by conceiving DDO-AGNN with blockchain technologies for federated learning. The present model includes the use of a comprehensive dataset, detailing diverse deepfake videos; these were normalized via pre-processing by min-max normalization. The DDO-AGNN algorithm was implemented in Python and optimized by DDO for improved feature extraction and classification. Blockchain technology was employed for federated learning, ensuring privacy-preserving collaborative model training across multiple nodes. The tool interchangeably used in this framework includes Python for the algorithm implementation, blockchain for federated learning, and several machine learning libraries for model training and evaluation. The results indicate that the proposed DDO-AGNN outperforms all existing methods for deepfake detection with regard to accuracy, precision, recall, and F1-score. More precisely, the model with 99.38% accuracy and 99.26% precision was able to surpass state-of-the-art methods including ResNet-SwishDense54, BlazeFace+DFN + XGBoost, and YIX by 98.67% recall and 98.89% F1-score. Blockchain-enabled federated learning successfully retained data privacy, but due to the high computational cost (10–100 GPU hours), scalability posed a challenge. These findings indicate that the integration of DDO-AGNN with blockchain technology provides an effective and secure solution for deepfake detection, representing a significant enhancement over current techniques. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.","Blockchain technology; Deepfake detection; Dynamic Drosophila optimization (DDO); Federated learning; Graph neural networks (GNN); Privacy-preserving algorithms"
"Vivekananda, G.N.; Mahesh, T.R.; Gupta, M.; Thakur, A.; Sayal, A.","Vivekananda, G. N. (57189247593); Mahesh, T. R. (57638929100); Gupta, Muskan Kumari (59033809500); Thakur, Arastu (58828085700); Sayal, Anu (57204790343)","57189247593; 57638929100; 59033809500; 58828085700; 57204790343","Refining digital security with EfficientNetV2-B2 deepfake detection techniques","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005518686&partnerID=40&md5=24dd609f49b296fea4135504fae60257","The rise in digitally altered images has made research on robust solutions for real image verification across sectors, including media and cybersecurity very essential. Deepfake technology's development compromises digital media's validity and calls for advanced detection to address. With EfficientNetV2-B2, a novel improvement in convolutional neural networks that is considered efficient and effective, the present research proposes a strong method for separating deepfake and real images. To ensure equal ratio, the paper utilized a balanced dataset consisting of 100,000 photos divided equally between real-world and deepfake classes. Methodology involved image preprocessing to the same dimensions, model strength augmentation techniques, and a rigorous training process with parameter optimization for precision. Interestingly, the study employed an independent learning rate adjustment method for enhancing training performance, resulting in better model calibration. Experiment setup results showed a staggering 99.885 % in classification accuracy and a corresponding high F1 score, thereby establishing the capability of the model in deepfake detection. Extensive exploration also confirmed there were evident cases of misclassification, which indicated areas where training model and image processing procedures should be improved. The results illustrate the prospect of applying EfficientNetV2-B2 in situations where high accuracy is needed in photo verification. © 2025","Adaptive Learning; Cybersecurity; Data Augmentation; Deepfake Detection; Digital Forensics; EfficientNetV2-B2; Image Processing Efficiency; Image Verification; Machine Learning Algorithms; Media Integrity"
"Niu, R.; Zhang, Y.","Niu, Run (57224592978); Zhang, Yue (59069118800)","57224592978; 59069118800","3D-CADet: Leveraging 3D Channel Attention for Improving Deepfake Video Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008489588&partnerID=40&md5=b4d3755624de977b6eeebe07f9c884f2","With the continuous advancement of deepfake technology, the threats it poses to social stability and information security are increasingly severe, making research on deepfake detection technology particularly critical. Existing spatiotemporal models (3D CNN) often lack the ability to adaptively emphasize key features, making it challenging to comprehensively capture subtle forgery traces within videos, thereby limiting their detection performance. To address these challenges, we propose a 3D Channel Attention Deepfake Detection Network (3D-CADet), which combines 3D CNN with 3D channel attention module (3DCABlock) to enhance the model's sensitivity to critical features. The 3DCABlock module employs 1D convolution with adaptive kernel size to enable local cross-channel interactions and dynamic weight assignment, allowing the model to effectively capture subtle spatiotemporal inconsistencies in deepfake videos. Experimental results validate the effectiveness of 3D-CADet, demonstrating superior detection performance across multiple challenging datasets. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Channel attention; Deepfake detection; Video"
"Zhou, C.; Li, F.W.B.; Song, C.; Zheng, D.; Yang, B.","Zhou, Changshuang (59540773400); Li, Frederick W.B. (7406057098); Song, Chao (57199789791); Zheng, Dong (58745105000); Yang, Bailin (13105420500)","59540773400; 7406057098; 57199789791; 58745105000; 13105420500","3D data augmentation and dual-branch model for robust face forgery detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216862698&partnerID=40&md5=067bae7db56f20def336a5b09deafefb","We propose Dual-Branch Network (DBNet), a novel deepfake detection framework that addresses key limitations of existing works by jointly modeling 3D-temporal and fine-grained texture representations. Specifically, we aim to investigate how to (1) capture dynamic properties and spatial details in a unified model and (2) identify subtle inconsistencies beyond localized artifacts through temporally consistent modeling. To this end, DBNet extracts 3D landmarks from videos to construct temporal sequences for an RNN branch, while a Vision Transformer analyzes local patches. A Temporal Consistency-aware Loss is introduced to explicitly supervise the RNN. Additionally, a 3D generative model augments training data. Extensive experiments demonstrate our method achieves state-of-the-art performance on benchmarks, and ablation studies validate its effectiveness in generalizing to unseen data under various manipulations and compression. © 2025","3D data augmentation; Deepfake detection; Dual-branch network"
"Usmani, S.; Kumar, S.; Sadhya, D.","Usmani, Shaheen (58368838500); Kumar, Sunil (59563799600); Sadhya, Debanjan (56926822500)","58368838500; 59563799600; 56926822500","Spatio-temporal knowledge distilled video vision transformer (STKD-VViT) for multimodal deepfake detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214225264&partnerID=40&md5=7d077ea5dfba0abe84726b0ab1ed535c","The widespread circulation of manipulated videos using deepfake techniques has raised concerns about the authenticity of multimedia content. In response, deepfake detection techniques have made significant strides in specific scenarios. However, most of the existing methods are unimodal and focus only on extracting traditional spatial features, due to which they struggle to accurately identify modern deepfakes. This work introduces the STKD-VViT model for detecting deepfakes across multiple modalities while employing spatiotemporal features. STKD-VViT combines the strengths of the Video vision transformer and the Vision transformer to process visual and audio streams. The Video vision transformer employs a multi-head attention mechanism and tubelet embedding to extract the video's spatial and temporal features. Alternatively, the vision transformer extracts the salient features from the mel-spectrograms of audio files. Furthermore, STKD-VViT leverages the knowledge distillation technique to reduce the number of FLOPs and the model's parameters. Experimental results on the benchmark FakeAVCeleb dataset demonstrate that STKD-VViT achieves a testing accuracy of 97.49% for video stream data, 98.65% for audio stream data and 96.0% when both streams are combined using score-level fusion, surpassing other state-of-the-art methods. © 2024 Elsevier B.V.","Deepfake detection; Fusion; Knowledge distillation; Video vision transformer; Vision transformer"
"Agrawal, P.; Jha, A.; Bhute, A.","Agrawal, Pratham (59393604200); Jha, Anchalaa (59393326500); Bhute, Avinash N. (36917986600)","59393604200; 59393326500; 36917986600","Comprehensive Exploration of Deepfake Detection Using Deep Learning","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208021779&partnerID=40&md5=f998b7a51f7e5f642415a4e889388342","The proliferation of deepfake technology poses a significant threat to the veracity of multimedia content, necessitating robust countermeasures for detection. This research explores the efficacy of three distinct deep learning architectures—Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Convolutional Neural Network (CNN)—in discerning deepfake manipulations within video data. The study encompasses a diverse dataset encompassing real and manipulated videos, ensuring comprehensive evaluation. Each model is meticulously trained and rigorously tested, with performance assessed in terms of accuracy, precision, recall, and F1-score. The results underscore the CNN’s exceptional prowess in spatial feature extraction, yielding superior accuracy compared to its temporal counterparts. A comprehensive discussion delves into the strengths and limitations of each model, providing valuable insights for future research in this critical domain. This research not only contributes to the arsenal of techniques for deepfake detection but also underscores the dynamic landscape of multimedia forensics, necessitating continuous innovation to safeguard the integrity of visual content in an era of increasingly sophisticated manipulations. The findings herein offer a foundational framework for further advancements in deepfake detection methodologies. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.","CNN; Deep learning; Deepfake detection; GRU; LSTM; Multimedia forensics"
"Winata, F.T.; Tanuwijaya, N.J.; Setiawan, R.; Rumagit, R.Y.","Winata, Federic (58112441500); Tanuwijaya, Nicholas (59940330000); Setiawan, Reina (57191338031); Rumagit, Reinert Yosua (57200214667)","58112441500; 59940330000; 57191338031; 57200214667","Comparison of deepfake detection using CNN and hybrid models","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023988519&partnerID=40&md5=0c48f5b82665f60e63eec80e3d65cf2a","In today's digital era, manipulated videos known as deepfakes are becoming more common and harder to detect. These deepfake contents can be misused by people to spread false information, damage someone's reputation, or even harm many people. To address this issue, we conducted a study to compare the performance of three deep learning models Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and a hybrid CNN-LSTM model to detect deepfake content. In this study, we used the DeepFake Detection Challenge (DFDC) dataset, which includes a variety of real and fake videos, then we will evaluate each model using accuracy, precision, recall, F1-score, AUC, and loss values. The results of our research show that the RNN model provides the best overall performance with an accuracy of 77.5%, F1-score of 77.5%, and AUC of 0.928, while CNN followed closely with 75.5% accuracy and AUC of 0.930. The CNN-LSTM model showed lower performance with 67.5% accuracy and AUC of 0.886. The findings demonstrate the strength of RNNs in modeling temporal patterns in video data, which is crucial for effective deepfake detection. Then CNN model also have good performance and shows strong results in detecting spatial features in individual frames. Meanwhile, CNN-LSTM model does not perform as well as the other models. This can be happen due to hybrid model having more complex structure, which makes it more difficult to train effectively. We hope this research helps in building better tools to detect deepfakes in real-world situations. © 2025 The Authors.","CNN; CNN-LSTM; Deepfake; DFDC; LSTM"
"Wu, B.; Qian, Q.; Ran, L.; Wang, H.","Wu, Bingxiang (60116978800); Qian, Qing (54417789600); Ran, Longwen (60117099100); Wang, Huan (57189987570)","60116978800; 54417789600; 60117099100; 57189987570","GASGM-GFT: Gaussian Attenuation Singing Graph Model and Graph Fourier Transform for Singing Voice Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023984387&partnerID=40&md5=db59e10e4a24b26fb1f20ece6edb1236","Singing voice synthesis and singing voice conversion technologies have raised significant concerns regarding music copyright and authenticity. While extant research has predominantly focused on detecting spoofing speech, much less attention has been given to deepfake singing voice. Moreover, conventional techniques frequently prove incapable of capturing the intricate temporal relationships inherent in vocalisation, thus impeding their capacity to effectively extract both local and global characteristics. To address these challenges, this paper introduces a novel deepfake detection method of singing voice called GASGM-GFT (Gaussian Attenuation Singing Graph Model and Graph Fourier Transform). In this approach, a singing graph model between sampling points of the original singing is creatively constructed using a Gaussian attenuation function. This model captures the complex temporal relationships of the singing signal. Next, the Graph Fourier Transform (GFT) is applied to map the singing signal into the graph frequency domain. This helps capture both local and global frequency features within the singing graph model, revealing hidden deepfake properties of the signal. The one-dimensional graph frequency domain signals are then expanded into two dimensions and processed through residual blocks for feature enhancement. Finally, the graph attention module is utilized to model node relationships, compute attention weights, and aggregate features through graph pooling layers to produce the final discrimination result. The experimental results demonstrate that GASGM-GFT outperforms existing advanced methods for spoofing speech detection and singing voice deepfake detection on the CtrSVDD dataset. It achieves an equal error rate (EER) of only 2.53% on the Vocals test set and surpasses other systems on the Mixture test set. © 2025 IEEE.","Graph Neural Network; Graph Signal Processing; Singing Voice Deepfake Detection"
"Chen, J.; Hu, M.; Zhang, D.; Meng, J.","Chen, Jiaxin (57211874368); Hu, Miao (59538714500); Zhang, Dengyong (55318418900); Meng, Jingyang (59538773100)","57211874368; 59538714500; 55318418900; 59538773100","GC-ConsFlow: Leveraging Optical Flow Residuals and Global Context for Robust Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022616236&partnerID=40&md5=99ee935c7aa14229df890fb29510b959","The rapid development of Deepfake technology has enabled the generation of highly realistic manipulated videos, posing severe social and ethical challenges. Existing Deepfake detection methods primarily focused on either spatial or temporal inconsistencies, often neglecting the interplay between the two or suffering from interference caused by natural facial motions. To address these challenges, we propose the global context consistency flow (GC-ConsFlow), a novel dual-stream framework that effectively integrates spatial and temporal features for robust Deepfake detection. The global grouped context aggregation module (GGCA), integrated into the global context-aware frame flow stream (GCAF), enhances spatial feature extraction by aggregating grouped global context information, enabling the detection of subtle, spatial artifacts within frames. The flow-gradient temporal consistency stream (FGTC), rather than directly modeling the residuals, it is used to improve the robustness of temporal feature extraction against the inconsistency introduced by unnatural facial motion using optical flow residuals and gradient-based features. By combining these two streams, GC-ConsFlow demonstrates the effectiveness and robustness in capturing complementary spatiotemporal forgery traces. Extensive experiments show that GC-ConsFlow outperforms existing state-of-the-art methods in detecting Deepfake videos under various compression scenarios. © 2025 IEEE.","Deepfake Detection; Global Context Analysis; Optical Flow Residuals; Spatiotemporal Features"
"Fang, Z.; Zhao, H.; Wei, T.; Zhou, W.; Wan, M.; Wang, Z.; Zhang, W.; Yu, N.","Fang, Ziyuan (58505872700); Zhao, Hanqing (57226043096); Wei, Tianyi (57221840182); Zhou, Wenbo (57192111936); Wan, Ming (58018076000); Wang, Zhanyi (58018076100); Zhang, Weiming (56459354100); Yu, Neng-Hai (7201981769)","58505872700; 57226043096; 57221840182; 57192111936; 58018076000; 58018076100; 56459354100; 7201981769","UniForensics: Face Forgery Detection via General Facial Representation","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022594664&partnerID=40&md5=1cd3f7f6ae1936de0f2484504cfce669","The rise of deepfakes has significantly heightened concerns for privacy and the authenticity of digital media, bringing widespread attention to face forgery detection. Previous deepfake detection methods mostly depend on low-level textural features vulnerable to perturbations and fall short of detecting unseen forgery methods. In contrast, high-level semantic features are less susceptible to perturbations and not limited to forgery-specific artifacts, thus having stronger generalization. Motivated by this, we propose a detection method that utilizes high-level semantic features of faces to identify inconsistencies in temporal domain. We introduce UniForensics, a novel deepfake detection framework that leverages a transformer-based video classification network, initialized with a meta-functional face encoder for enriched facial representation. In this way, we can take advantage of both the powerful spatio-temporal model and the high-level semantic information of faces. Furthermore, to leverage easily accessible real face data and guide the model in focusing on spatio-temporal features, we design a Dynamic Video Self-Blending (DVSB) method to efficiently generate training samples with diverse spatio-temporal forgery traces using real facial videos. Based on this, we advance our framework with a two-stage training approach: The first stage employs a novel self-supervised contrastive learning, where we encourage the network to focus on forgery traces by impelling videos generated by the same forgery process to have similar representations. On the basis of the representation learned in the first stage, the second stage involves fine-tuning on face forgery detection dataset to build a deepfake detector. Extensive experiments validates that UniForensics outperforms existing face forgery detection methods in generalization ability and robustness. In particular, our method achieves 95.3% and 77.2% cross dataset AUC on the challenging Celeb-DFv2 and DFDC respectively. Code will be made publicly available. © 2004-2012 IEEE.","data synthesis; Deepfake detection; self-supervised contrastive learning"
"Yadav, K.D.K.; Kavati, I.; Kurella, A.S.; Jain, S.; Katare, Y.","Yadav, Koyya Deepthi Krishna (58993087100); Kavati, Ilaiah (56286935100); Kurella, Abhihkeshav Santosh (60201287300); Jain, Savvy (60201287400); Katare, Yashsvini (60201891700)","58993087100; 56286935100; 60201287300; 60201287400; 60201891700","Context-Preserving and Sparsity-Aware Temporal Graph Network for Unified Face Forgery Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022297379&partnerID=40&md5=53cbf7620eada8fc8372b61c27be58f1","Deepfakes and face forgeries continue to evolve, posing significant threats to consumer privacy, reputation, and public security. Existing deep learning approaches often focus on spatial inconsistencies but fail to capture relational context across facial regions and long-term temporal anomalies such as unnatural blinking or lip synchronization errors. We propose EDRL, a lightweight model that effectively captures spatiotemporal relational dependencies for robust face forgery detection. The architecture incorporates a Spatio-Temporal Attention (STA) module built upon a lightweight MC318 3D convolutional backbone, enabling motion-aware feature extraction and region-specific attention mapping. A Sparsity-Aware Edge Dropping Relation Learner (EDRL) constructs adaptive facial graphs by pruning redundant and less informative edges. A Temporal Adaptive Aggregation Network (TAAN) then aggregates frame-level features, ensuring that temporally significant representations are preserved even after edge pruning. Extensive evaluations show that EDRL achieves 98.4% accuracy on CASIA-FASD and reduces HTER by 6.2% on Replay-Attack compared to state-of-the-art baselines, while maintaining competitive results on digital forgery datasets. By enhancing robustness to diverse manipulations while reducing computational overhead, EDRL contributes towards a secure, lightweight, and deployable framework suitable for real-world applications. © 2025 IEEE.","DeepFake detection; Face Forgery; graph convolution network; relation learning; Spatio-Temporal attention"
"Chen, T.; Hu, J.; Yang, S.","Chen, Tiewen (58897683400); Hu, Jing (57192187866); Yang, Shanming (60198383900)","58897683400; 57192187866; 60198383900","Deepfake detection based on Spatio-Temporal Fusion","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022067609&partnerID=40&md5=e2b3a1d8d3b62b722a628fa1b6c6e136","To address the limitations of existing detection algorithms that utilize temporal information but are weak in extracting local information of forged traces, this paper introduces a detection algorithm for forged faces which based on spatiotemporal feature fusion. The algorithm comprises a spatial-frequency domain hybrid feature enhancement module and a frequency-aware dynamic gating unit module. Through the frequency domain, the high-frequency components of forged traces are enhanced to capture the global spatial and local frequency-domain cues. Subsequently, the frequency-aware dynamic gating unit is employed to amplify subtle cues in the temporal dimension that are difficult to discern. These cues from both dimensions are then integrated through an information fusion module to obtain a more comprehensive feature representation. Experimental results demonstrate that the proposed algorithm exhibits superior generalization performance on datasets in the forged domain. © 2025 IEEE.","deepfake detection; Feature Enhancement; Spatiotemporal Feature Fusion"
"Wagh, A.; Rane, R.; Pinjarkar, L.","Wagh, Abhishek (57218176549); Rane, Rashmi A. (57191090200); Pinjarkar, Latika Shyam (57200090983)","57218176549; 57191090200; 57200090983","Enhancing DeepFake Detection through Deep Learning and Explainable AI","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021825201&partnerID=40&md5=95db8f19f485f4f51d85633e39a4b153","DeepFake technologies powered by AI models like GANs and Auto-Encoders, with serious threats to privacy and security. Traditional detections methods struggled against Sophisticated DeepFakes. This paper reviews existing detection techniques, identifies research gaps and proposes a new framework integrating CNNs, Transformers, Bi-LSTM and Explainable AI (XAI) for detecting both images and videos and for better accuracy and interpretability. © 2025 IEEE.","Bi-LSTM; CNN; Deep Learning; DeepFake Detection; Explainable AI; Grad-CAM; Transformer"
"He, Z.; Guo, Z.; Wang, L.; Yang, G.; Diao, Y.; Ma, D.","He, Ziyuan (59925900300); Guo, Zhiqing (57219672095); Wang, Liejun (16833826600); Yang, Gaobo (8647279200); Diao, Yunfeng (57222760580); Ma, Dan (59925783500)","59925900300; 57219672095; 16833826600; 8647279200; 57222760580; 59925783500","WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020986670&partnerID=40&md5=f1e1fa362bf8d11229c7e006907184a4","Deepfake technology has great potential in the field of media and entertainment, but it also brings serious risks, including privacy disclosure and identity fraud. To counter these threats, proactive forensic methods have become a research hotspot by embedding invisible watermark signals to build active protection schemes. However, existing methods are vulnerable to watermark destruction under malicious distortions, which leads to insufficient robustness. Moreover, embedding strong signals may degrade image quality, making it challenging to balance robustness and imperceptibility. Although watermarked images look natural, their underlying structures are often different from the original images, which is ignored by traditional watermarking methods. To address these issues, this paper proposes a proactive watermarking framework called WaveGuard, which explores frequency domain embedding and graph-based structural consistency optimization. In this framework, the watermark is embedded into the high-frequency sub-bands by dual-tree complex wavelet transform (DT-CWT) to enhance the robustness against distortions and deepfake forgeries. By leveraging joint sub-band correlations and selected sub-band combinations, the framework enables robust source tracing and semi-robust deepfake detection. To enhance imperceptibility, we propose a Structural Consistency Graph Neural Network (SC-GNN) that constructs graph representations of the original and watermarked images to ensure structural consistency and reduce perceptual artifacts. Experimental results show that the proposed method performs exceptionally well in face swap and face replay tasks. © 1991-2012 IEEE.","Deepfake Detection; Frequency-domain Embedding; Graph Neural Network (GNN); Source Tracing"
"Castiglione, A.; Cimmino, L.; Loia, V.; Nappi, M.; Sorrentino, C.","Castiglione, Aniello (16027997000); Cimmino, Lucia (57220807983); Loia, Vincenzo (35570326700); Nappi, Michele (6603906020); Sorrentino, Carlo (60151790000)","16027997000; 57220807983; 35570326700; 6603906020; 60151790000","Dynamic Lip Motion Analysis for Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019517360&partnerID=40&md5=b0b9929300f24a3da8085db9b3e91114","The proliferation of deepfake videos poses a significant threat to the integrity of digital media, given their capacity to disseminate misinformation and undermine the reliability of visual content. As synthetic audiovisual forgeries become increasingly realistic, they are being used in coordinated disinformation efforts, contributing to the broader issue of information disorder, which compromises public trust, media authenticity, and the verifiability of digital evidence. This study investigates the effectiveness of spatio-temporal features, extracted using Local Binary Patterns on Three Orthogonal Planes (LBP-TOP), in distinguishing synthetic from authentic video content. The approach focuses on the dynamic characteristics of the labial region, where inconsistencies in facial motion are more likely to occur in deepfake videos. LBP-TOP is employed to capture subtle texture and motion variations across spatial and temporal dimensions. Preliminary empirical evaluations conducted on benchmark deepfake datasets demonstrate the efficacy of the proposed approach, emphasizing the importance of localized temporal analysis in enhancing detection accuracy. The results highlight the potential of region-specific facial modelling as a computationally efficient yet discriminative strategy in the context of video forensics and the mitigation of synthetic media-based disinformation. © 2025 IEEE.","Deepfake Detection; Digital Video Forensics; Local Binary Patterns on Three Orthogonal Planes (LBP-TOP); Misinformation; Spatiotemporal Feature Extraction"
"Kamil, F.; Maharani, E.M.; Rahmania, R.","Kamil, Faishal (58915710000); Maharani, Eleanor Maritsa (60145438100); Rahmania, Rissa (57209028479)","58915710000; 60145438100; 57209028479","ResNeXt-50 and LSTM for Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019048551&partnerID=40&md5=35ff1921b8475d2ec61c5510bd6bde73","Deepfake technology presents a significant threat to digital security due to its increasing sophistication and potential for misuse. This research proposes a Deepfake detection model combining ResNeXt-50 32×4d for spatial feature extraction and Long Short-Term Memory (LSTM) networks for temporal analysis. A video processing pipeline was implemented to isolate and retain facial regions from each frame in the dataset. Videos were decomposed into frames, with faces detected using a pre-trained model of face recognition and frames containing faces were cropped, resized to 256×256 pixels, and saved. Only the first 150 frames per video were processed to maintain temporal order for sequential analysis, with the dataset split into training, validation, and test sets (70%, 20%, 10%). The processed frames were then passed through the ResNeXt-50 for spatial feature extraction and LSTM for temporal dependencies. Evaluated on the Celeb-DF and FaceForensics++ datasets, the model achieved peak accuracies of 99.90% and 99.13%, respectively. Future research will focus on integrating multimodal data and leveraging Explainable AI to further improve detection accuracy and interpretability. © 2025 IEEE.","Celeb-DF; Deepfake Detection; FaceForensics++; LSTM; ResNeXt-5032x4d"
"Rahman, T.; Chakma, R.; Mahmud, T.","Rahman, Taohidur (58982078700); Chakma, Rishita Jyoti (58244844600); Mahmud, Tanjim (57193502601)","58982078700; 58244844600; 57193502601","Beyond Accuracy: Explainable Multimodal Deepfake Detection Through Cross-Modal Feature Analysis and Dynamic Attention Weighting","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019046823&partnerID=40&md5=0084e29cc5fe087e92ff8ebf2c6a51a1","The advent of deepfake technology in recent years has significantly transformed the domain of video synthesis, facilitating the production of convincing synthetic films. The study explains a method for detecting deepfakes that uses both visual and sound analysis with special neural networks and a technique that combines the two types of information. We have a system that uses an EfficientNetB0- based CNN to analyze face images and a bidirectional LSTM to process audio features, showing that combining both types of data makes detection stronger than using just one type. Our research on the AVLips dataset shows that different types of data learn in different ways-visual features help with quick learning at first but are more likely to overfit, while audio features lead to a more consistent learning process. Using explainable AI methods, we find that visual deepfake signs are mainly seen around the eyes and mouth, while specific MFCC coefficients (specifically 2 and 9) offer important distinguishing information in the audio part. The fusion model attains an accuracy of 87.25 %, surpassing the visual-only model at 85.25 % and the audio-only model at 81 %. In addition to performance measurements, our study offers important information regarding feature relevance across modalities and illustrates how attention mechanisms may adjust modality contributions depending on the reliability of individual samples. This study improves the field by highlighting the benefits of understanding multimodal methods and identifying specific patterns across different types of data that characterize deepfake content. © 2025 IEEE.","AVLips; CNN; Deepfake Detection; EfficientNetBO; LSTM; MFCC; Multimodal"
"Cyril, M.A.; Sunkavalli, J.P.; Prasanth, D.","Cyril, Madda Anthony (60136264200); Sunkavalli, Jayaprakash Prakash (57501004600); Prasanth, D. (59155949000)","60136264200; 57501004600; 59155949000","DeepGuard: A Hybrid CNN-LSTM Framework for Robust Deepfake Video Detection with Spatiotemporal Analysis","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018453266&partnerID=40&md5=6cf0cc89888143c2617debdb2abbc7f5","Deepfake technology has developed very quickly, allowing highly realistic artificial media to be created that causes serious risks to privacy, security, and disinformation. This paper introduces a deep learning method for identifying deepfake videos by applying a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model. The proposed model takes advantage of spatial information learned by a CNN (Xception backbone) and temporal relationships learned by an LSTM in order to label videos as real or fake. The model was trained on the FaceForensics++ dataset and evaluated on the Celeb-DF dataset with an accuracy of 82%. A web application was also implemented for real-time deepfake detection. The outcome proves the efficacy of the suggested architecture in detecting manipulated videos with challenges and future enhancements © 2025 IEEE.","Celeb-DF; CNN-LSTM; Deepfake detection; FaceForensics++; Temporal modelin); Xception"
"Sharma, S.; Gupta, K.","Sharma, Soumya (59986415000); Gupta, Keshav (56785003700)","59986415000; 56785003700","DeepShield: AI-Powered Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018297547&partnerID=40&md5=4fdb47cb73fbac218506d90e4e411f5c","The growing pervasiveness of deepfake technology is enormous in its threats to digital integrity, cybersecurity, and the diffusion of misinformation. Public opinion can be manipulated by deepfake media, identity authentication can be challenged, and digital trust can be harmed. To curb such threats, we introduce DeepShield, a new AI-powered deepfake detection system that combines spatial and temporal analysis for strong forgery detection. Our approach integrates a ResNeXt-based Convolutional Neural Network (CNN) for extracting spatial features and a Long Short-Term Memory (LSTM) network for detecting temporal abnormalities between video frames. DeepShield is trained on benchmark datasets such as FaceForensics++ and DFDC and reaches an accuracy of 8 1. 9 7% with 8 3. 4% recall and 8 0. 4% F1-score, all while having good generalization. Though not surpassing state-of-the-art models in bare accuracy, DeepShield prioritizes scalability, robustness against unseen manipulations, and deployability. We also provide future directions such as transformer-based architecture and multimodal detection, thus positioning DeepShield as a step towards robust real-time video authentication in digital forensics and cybersecurity. © 2025 IEEE.","Convolutional Neural Networks (CNN); Deepfake detection; Long Short-Term Memory (LSTM); ResNeXt; Video Authentication"
"Esty, F.T.T.","Esty, Fatema Tuj Tarannom (59940285800)","59940285800","Advancing Deepfake Detection: An Optimized Neural Network Model with Keyframe Analysis","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017956654&partnerID=40&md5=abac20688272f053e8c42be95a750eb6","Artificial intelligence (AI) can rapidly progress and foster AI algorithms to create a new machine-created video called deepfakes. These misleading videos are serious threats to our society and politics, or they can be used as weapons against the viewers. Deepfakes refer to synthetic media created with AI; therefore, such techniques can produce real-looking visual and auditory content. The first element of mitigating the malice of such videos is detecting them on social media platforms. This paper proposes a novel neural network-based method for deepfake video detection, focusing on critical video frame extraction to lessen computational complexity. The proposed method utilizes the architecture of a CNN coupled with a classifier network and a unique algorithm to accurately detect deepfake videos while keeping very low computational needs. Processing key video frames achieves impressive results based on single-keyframe processing for deepfake video detection. The technique is simple and efficient in authenticating video content concerning the social and economic problems of false videos. After evaluation over an entire dataset of 600 videos downloaded from various websites, the model detects deepfakes across different databases with an overall accuracy of 97.3%. Its performance surpasses that of earlier methods with reduced computation time, thus providing an efficient solution for deepfake detection. © 2025 IEEE.","Convolutional neural networks (CNN); Deepfake detection; Digital media forensics; Recurrent Neural Networks; Short-Term Memory (LSTM)"
"Dutta, A.; Kumar das, A.K.; Naskar, R.; Chakra, R.S.","Dutta, Anurag (57820449400); Kumar das, Arnab (58264994300); Naskar, Ruchira (53866980700); Chakra, Rajat Subhra (60126927000)","57820449400; 58264994300; 53866980700; 60126927000","WaveDIF: Wavelet Sub-Band based Deepfake Identification in Frequency Domain","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017846289&partnerID=40&md5=25e3a49c32a6741b8757607c50b2db66","With the more realistic convergence of Deepfakes, its' identification becomes more demanding. Recently, numerous deepfake detection techniques have been proposed, most of which are in the spatio-temporal domain. While these methods have shown promise, many of them neglect convincing artifacts that exhibit different patterns across frequency domains. This research proposes WaveDIF, a strict frequency domain, lightweight deepfake video detection algorithm using wavelet sub-band energies. In WaveDIF, for feature extraction, each video undergoes a Discrete Fourier Transform to filter out high-frequency noisy details (quite evident in deepfakes). These representations are then decomposed into their respective wavelet sub-bands -LL (Low-Low), LH (Low-High), HL (High-Low), and HH (High-High) passing them through a Haar Filter, following which the energy values (particular to each sub-band) are computed. These energy values are then used to learn a linear decision boundary (using regression analysis), which is then used for classification. This enables an interpretable, lightweight deterministic technique for the detection of synthesized videos, besides achieving an accuracy comparable to the state-of-the-art. Experimental results on popular deepfake video datasets shows over 92% accuracy for in-dataset evaluation, and 88% accuracy for cross dataset evaluation. © 2025 IEEE.","discrete fourier transform; discrete wavelet transform; frequency domain analysis; lightweight deepfake detection; wavelet sub-band energies"
"Melouk, M.M.; Shao, M.; Basit, A.; Abouzahir, C.; Zhou, R.; Shafique, M.","Melouk, Mohammed Mouad (60123651000); Shao, Minghao (58908537200); Basit, Abdul (58021649500); Abouzahir, Chaimae (59927346100); Zhou, Ruilin (60123542200); Shafique, Muhammad (17435669500)","60123651000; 58908537200; 58021649500; 59927346100; 60123542200; 17435669500","EVDO: An Enhanced Framework for Deepfake Detection in Videos Through Optical Flow and Temporal-Spatial Analysis","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017549190&partnerID=40&md5=1bb793326b568b604088502cdc1b4c64","As generative AI advances, the realism of synthetic media has sparked serious concerns in security, privacy, and misinformation. This issue is particularly concerning with the rise of deepfake technologies that manipulate facial imagery, undermining media authenticity. While existing research has largely focused on image-based deepfake detection with specialized feature extractors, detecting deepfakes in video, especially with broad generalization across varied manipulation techniques, remains a substantial challenge. This paper introduces EVDO, a novel framework designed to detect deepfake videos by integrating temporal and spatial analysis for enhanced detection accuracy. Our approach leverages optical flow techniques to capture subtle temporal manipulation artifacts between video frames overlooked in spatial analysis. Using a FlowFormer++ model for temporal analysis, frame pairs are sampled to produce cost volumes that highlight essential motion regions and capture manipulation-specific artifacts through optical flow images which encode temporal dependencies. A flow-finetuned detector then extracts flow-level features indicative of deepfake manipulation. Complementing this, Xception-based spatial detectors analyze each frame individually, generating high-dimensional embeddings that capture frame-specific anomalies. Fusing these temporal and spatial embeddings enables comprehensive binary classification of deepfakes. Validated on the FaceForensics++ benchmark, EVDO significantly improves generalization. Our proposed temporal path contributes correct classifications to around 12% of datapoints. EVDO achieves a video AUC of 99.13% (99.43% of end-to-end SOTA methods) while enabling forensically verifiable manipulation detection. © 2025 The Authors.","deep-learning; Deepfake detection; optical flow; temporal-spatial analysis"
"Ambika, S.; Harini, Y.; Sumanth, B.R.","Ambika, S. (57195715689); Harini, Yerramsetty (59402166500); Sumanth, B. R. (59402842000)","57195715689; 59402166500; 59402842000","Real-Time Deepfake Detection using a Hybrid MobileNet-LSTM Model For Image and Video Analysis","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016904565&partnerID=40&md5=693a8140c3f7e4dd4c91760468864053","As deepfake technology has become more advanced, there is an urgent need for strengthening detection mechanisms that can help counter misinformation and online fraud. Deepfake media is manufactured using deep learning techniques. It has recently become increasingly sophisticated and the boundary between genuine and tampered content is often hard to see. This paper introduces a hybrid MobileNet-LSTM model, which is made for spotting deepfakes in both image and video.The deepfake identification system uses MobileNet to efficiently sample spatial features and LSTM to gather temporal dependencies, thereby improving the detection rate of tampered content. Meanwhile, this method is validated on standard datasets and that the accuracy is better, in terms of computational efficiency and that it is more robust than other types of manipulation. © 2025 IEEE.","Computer Vision; Deepfake Detection; Hybrid MobileNet-LSTM Model; LSTM; Machine Learning; Media Authentication; MobileNet; Real-time Analysis"
"Rout, J.; Mishra, M.","Rout, Jayanti (57712825900); Mishra, Minati (55340242800)","57712825900; 55340242800","Cascaded ResNet50-LSTM Architecture for Deepfake Video Identification","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016863229&partnerID=40&md5=493b8cb3dd69b4f756e53ef95da80c0a","With the increasing sophistication of AI-generated content, detecting fake videos has become a critical challenge in digital media forensics. Such fake videos can cause loss to individuals and organizations in terms of fiscal matters, goodwill, societal status, etc. Traditional detection methods such as frame-by-frame analysis, motion inconsistencies, etc., were timeconsuming, subjective, and prone to human bias. This research presents a hybrid deep learning framework that combines Convolutional Neural Networks (CNNs) for spatial feature extraction with Long-Short-Term Memory (LSTM) networks for modeling temporal relationships. The proposed model is designed to efficiently differentiate between authentic and AI-generated videos by leveraging both spatial and sequential patterns within video frames. The model is trained and evaluated on the Celeb-DF (v2) dataset. The preprocessing phase involves frame extraction, resizing, and normalization to enhance feature representation. The experimental findings indicate that the proposed model delivers robust performance in the test set: 94. 98% accuracy, 98. 06% precision, 96. 08% recall, 97. 06% F1 score and 98. 32% AUC-ROC. Further evaluation using a confusion matrix and loss accuracy curves highlights the robustness of the model in detecting AI-generated content. This research contributes to improving deepfake detection techniques, ensuring improved security and reliability in multimedia authentication systems. © 2025 IEEE.","AI-Generated Video Detection; CNNs; Deepfake Detection; Fake Video Identification; Forgery Detection; LSTM"
"Colaco, B.; Patil, B.; Gharat, S.; Gawande, S.","Colaco, Brinal (57190174454); Patil, Bramheti (60103825100); Gharat, Swara (60104216000); Gawande, Srushti (60104086200)","57190174454; 60103825100; 60104216000; 60104086200","Blockchain Based Deepfake Video Detection Using Deep Learning","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016321996&partnerID=40&md5=0c6574d6868093ee10e478526bc8167f","Deepfake identification has become an essential issue in the digital world as AI generated manipulated videos are being utilized more and more maliciously. In order to provide a trustworthy technique for detecting deep-fake video, proposed system employs an integrated deep learning method involving Long Short-Term Memory (LSTM), ResNeXt, and Convolutional Neural Networks (CNN). CNN and ResNeXt are used to analyze video frames and extract spatial data, whereas LSTM analyzes the temporal dynamics between frames. These obtained features are employed to determine the variations and irregularities frequently found in deepfake movies. The Python and PyTorch frameworks used in the model's development and implementation provide high accuracy and efficiency. The results are verified as well and safely saved on blockchain, offering an infallible, transparent way to identify deepfakes. The fields of media integrity, digital forensics, and cybersecurity are greatly advanced by this program's helpful tool for preventing online fraud and disinformation. © 2025 IEEE.","Blockchain (BC); convolutional Neural Network (CNN); Deep Learning (DL); Deepfake Detection; Long Short-Term Memory (LSTM); PyTorch; ResNeXt; Temporal Analysis; Video Processing(VP)"
"Nadarajan, N.N.; Trivedi, A.R.; Shastha, K.; Mahesh, M.; Kolagatla, G.; Rana, S.; Nilin, M.","Nadarajan, Niranjan Nishan (60103469800); Trivedi, Aaditya Rajendra (60103724900); Shastha, K. (60103725000); Mahesh, M. (60103519200); Kolagatla, Gautham (60103469900); Rana, Shweta (60103674000); Nilin, M. (60103463400)","60103469800; 60103724900; 60103725000; 60103519200; 60103469900; 60103674000; 60103463400","A Dual-Stream CNN-LSTM Framework with ELA Preprocessing for Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016316778&partnerID=40&md5=ae61c30f93322d3a05d5769cd3bfc786","The proliferation of deepfake content presents critical threats to digital authenticity, privacy, and information integrity. This paper proposes a hybrid deepfake detection framework that integrates Error Level Analysis (ELA) with the ResNet-50 convolutional neural network to identify manipulated images. The system employs a dual-stream architecture that processes both raw frames and ELA-transformed inputs, enhancing spatial and temporal feature extraction through Long Short-Term Memory (LSTM) layers. Evaluated on both general-purpose and celebrity-based datasets, the proposed model demonstrates superior classification accuracy and robust generalization, particularly in detecting forgeries involving high-profile individuals. Experimental results affirm the model's effectiveness in capturing compression inconsistencies and subtle manipulations, positioning it as a scalable and reliable solution for deepfake detection in real-world scenarios. © 2025 IEEE.","deep learning; Deepfake detection; digital media authenticity; Error Level Analysis (ELA); image forensics; LSTM; multimedia security; ResNet-50"
"Hirpara, P.; Valangar, H.; Kachhadiya, V.; Chauhan, U.","Hirpara, Priyanshu (60102538500); Valangar, Hardi (60102319300); Kachhadiya, Vishwa (60102394400); Chauhan, Uttam (56045696400)","60102538500; 60102319300; 60102394400; 56045696400","Deepfake Detection: Demodulate Synthetic Videos Using Deep Learning Models","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016137250&partnerID=40&md5=f9c5def6ff4a7b673599b77e87285431","A deepfake detection system that uses machine learning (ML) and deep learning (DL) models to detect manipulated videos and images is presented in the study. Being aware of such synthetic content is crucial considering the emergence of deepfake technology, which might alter photos, videos, and audio for malevolent objectives including fraud, extortion, and disinformation. Deepfake technology has been applied to solve various real-time problems but is also exploited for unethical and illegal purposes. As a result, developing research and detection models is crucial to prevent its misuse. We proposed a CNN-LSTM hybrid model for analysis of cropped images to improve the performance of fake video detection. The suggested method focuses on identifying fake videos using the Celeb-DF dataset, which consists of 1203 videos (795 fake, 408 real). Moreover, the benefits and drawbacks of the various deepfake detection techniques are examined. The paper indicates potential improvements in model accuracy through more datasets and improved architectures, and it emphasizes the significance of sophisticated detection techniques to mitigate the negative consequences of deepfakes. With cropped video frames and deep learning techniques, the model's accuracy increased from 79.06% with the original dataset to 86.82% with cropped videos. © 2025 Bharati Vidyapeeth, New Delhi.","Artificial Intelligence; CNN-LSTM; Deep Learning; Deepfake Detection; Face Manipulation"
"Vinay, T.R.; Ajina, A.; Dongare, H.; Joshi, S.; Likith Sai Varma, G.; Rathod, R.D.","Vinay, T. R. (45561782700); Ajina, A. (56538485400); Dongare, Harshada (60097027000); Joshi, Soumya (60096084400); Likith Sai Varma, G. (60096863600); Rathod, Rohit D. (60095931000)","45561782700; 56538485400; 60097027000; 60096084400; 60096863600; 60095931000","Video Forensic Analysis Using Deep Learning Models","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015840865&partnerID=40&md5=d63606a6c30e4d06a2667300bbd3a508","Recent advancements in deep learning have significantly improved video forensics, crucial for security and law enforcement. This field addresses video tampering detection, source identification, and content analysis, requiring sophisticated methods due to complex manipulations. This paper introduces a novel framework leveraging Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks. CNNs extract spatial features, identifying tampering through pixel-level inconsistencies. LSTMs model temporal dependencies, detecting anomalies over time and sophisticated forgeries like DeepFakes. This hybrid combination of CNNs and LSTMs provides a comprehensive approach, capturing spatial details and temporal dynamics, and enhancing forgery detection accuracy. Experiments on Deepfake Detection Challenge(DFDC) dataset, it includes a wide variety of deepfake and real videos, to show this framework overcomes the limitations in previous technologies used in models like XceptionNet and is robust in real-world scenarios. This research highlights the potential for secure digital media platforms and future advancements in forensic techniques. ©2025 IEEE.","Convolutional Neural Networks; deep learning; Deepfake detection; LSTMs; video forensics"
"Meng, C.; Guo, Y.","Meng, Caixia (57401892200); Guo, Yurui (59988015600)","57401892200; 59988015600","A Deepfake Detection Method Based on Transformer Transformer Spatiotemporal Modeling","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015591220&partnerID=40&md5=7ffadf4866e0e8739199c638d1b041bd","This study proposes a novel spatiotemporal feature modeling model based on the Transformer architecture-STFormer-for deepfake detection tasks in videos. The model design features a four-stage pyramid structure, each incorporating a Spatiotemporal Embedding Layer (STEL) and multiple Transformer Encoder blocks. STEL extracts spatial features using multi-scale convolutional kernels and employs a Gate Recurrent Unit (GRU) to model inter-frame dynamic changes, ultimately fusing spatiotemporal features to form embedded representations. Furthermore, the model progressively reduces the number of embedding tokens at each stage while increasing the feature dimensions, thus efficiently extracting spatiotemporal features while maintaining information integrity and effectively managing computational budgets. Experimental validation on the FaceForensics++ and Celeb-DF datasets demonstrates that the STFormer model performs exceptionally well in detecting various forgery methods, substantiating its outstanding performance and broad applicability in deepfake detection. © 2025 IEEE.","deepfake detection; spatiotemporal embedding Layer(STEL); spatiotemporal features; STFormer"
"Subhasri, N.; Gangadhar, M.; Divakar, M.; Bhanu Chandra Goud, N.E.; Ramaraju, M.; Sreenivasulu, K.","Subhasri, Neereddula (60092824600); Gangadhar, Midde (60093650500); Divakar, Motte (60093529400); Bhanu Chandra Goud, Neelam Ediga (60093178800); Ramaraju, Marriboina (60092824700); Sreenivasulu, K. (60093178900)","60092824600; 60093650500; 60093529400; 60093178800; 60092824700; 60093178900","A Hybrid MobileNet-LSTM Model for Enhanced Detection of Deepfake Media in Real-Time Image and Video Analysis","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015391825&partnerID=40&md5=f11af74e5e5e224bcc65d33a1a122427","Deepfakes, or Al-generated manipulated media, pose a significant threat to media integrity, privacy, and public trust. Detecting deepfakes effectively remains a challenge due to their increasingly sophisticated nature. This research addresses the issue of detecting both spatial and temporal inconsistencies in deepfake images and videos by proposing a hybrid MobileNet-LSTM model. The MobileNet component extracts spatial features such as texture anomalies and facial misalignments, while the LSTM component analyzes temporal dependencies to identify inconsistencies in video sequences, such as unnatural facial expressions or motion patterns. The hybrid model is trained on a large and diverse dataset of real and deepfake media, ensuring its adaptability to emerging deepfake techniques. The outcome of this research is a highly accurate, real-time deepfake detection system that performs with an accuracy of 91.8%, achieving improved precision (89.4%), recall (90.5%), and F1-score (89.9%) compared to existing baseline models. The model's lightweight design ensures real-time performance, making it suitable for deployment in social media monitoring, digital forensics, and law enforcement. This work contributes to the ongoing efforts in combating misinformation and enhancing digital media authenticity by providing a scalable and efficient deepfake detection solution. © 2025 IEEE.","deepfake detection; image analysis; Media integrity; mobilenet lstm; real time detection; real-time detection; temporal modeling; video analysis"
"Guo, Y.","Guo, Yunfei (60002308900)","60002308900","A Dual-Network Architecture with Uncertainty-Aware Multimodal Fusion for Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014910177&partnerID=40&md5=c6d0ccfadd5510a7e91682cbeae42738","The detection of AI-generated content, particularly deepfakes, has become a critical issue due to the growing sophistication of generative models. This paper presents MUFDNet, a novel multimodal deepfake detection model that uses paired image and audio data for improved detection accuracy. The model integrates a Vision Transformer (ViT) for image feature extraction and a Graph Neural Network (GGCN) for audio spectrum analysis. By incorporating uncertainty-aware fusion and cross-modal attention mechanisms, MUF-DNet enhances the robustness and generalization of the detection process. Contrastive learning is employed to improve the semantic consistency between the visual and auditory features. Our experiments show that MUF-DNet outperforms existing models in terms of accuracy, precision, recall, and F1 score, demonstrating its potential for deepfake detection in real-world applications. © 2025 IEEE.","contrastive learning; Deepfake detection; Graph Neural Network; multimodal learning; uncertainty-aware fusion; Vision Transformer"
"Sharma, V.K.; Rawat, S.","Sharma, Vishal Kumar (57199918726); Rawat, Seema (56521132600)","57199918726; 56521132600","Enhancing Deepfake Detection Through Dynamics of Facial Expressions","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013073675&partnerID=40&md5=30de418a8e7de90a2320f9a0c5812b6a","The rapid evolution of deepfake technology poses significant threats to digital security, media integrity, and public trust, necessitating the development of robust detection frame-works. Traditional deepfake detection methods primarily rely on pixel inconsistencies, frequency-domain analysis, or handcrafted features, but these approaches are increasingly vulnerable to advanced generative models that produce high-fidelity manipulations. In this study, we introduce MADDM, a Masked Autoencoder-based deepfake detection model that leverages facial expression dynamics to identify inconsistencies in muscle coordination - an aspect that remains challenging for deepfake generators to replicate accurately. Our model is trained in a self-supervised manner, first learning natural facial expressions from real datasets and then detecting anomalies in synthetic videos by reconstructing masked facial regions. Evaluations on Celeb-DF, DFDC, and FaceForensics++ datasets demonstrate that MADDM significantly outperforms existing detection methods, achieving an average accuracy of 81.1%, with state-of-the-art performance on Celeb-DF (86.3%). Further analysis through intra-dataset and cross-dataset testing confirms the model's superior generalization capabilities. The results highlight the potential of expression-based deepfake detection as a powerful and scalable solution for digital forensics and misinformation control. Future research should explore real-time implementation, transformer-based optimizations, and adversarial training strategies to enhance detection efficiency against evolving deepfake techniques. © 2025 IEEE.","convolutional neural network; deepfake detection; facial expressions analysis; masked auto encoders; spatio temporal"
"Huang, X.; Zheng, Y.; Jiang, X.","Huang, Xingcheng (60040838500); Zheng, Yiheng (60040458500); Jiang, Xiuping (60040838600)","60040838500; 60040458500; 60040838600","A Deepfake Detection Model Based on the Integration of ResNet50 and LSTM","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013073179&partnerID=40&md5=a4693e83fc0f6f254c7758c68f105d8a","Video processing has consistently been a focal point of interest in both the media industry and academia, particularly against the backdrop of continuous advancements in digital signal processing technology. These videos are sometimes misused to disseminate false information or damage reputations. This paper aims to develop a video authenticity recognition system using advanced deep learning models. In terms of technical implementation, the ResNet50 model is employed for image feature extraction to detect subtle inconsistencies and artifacts, such as unnatural facial movements and irregular lighting variations in videos. Additionally, LSTM is used to capture the temporal sequence features of the video. Experimental results indicate that the model achieves an accuracy rate of 81.25% in detecting deepfake videos. Additionally, the model achieved a recall rate of 1, which holds significant practical value. This achievement provides strong support for the accurate detection and labeling of videos, playing a crucial role in preserving information authenticity and ensuring media security. © 2025 IEEE.","Computer Vision; Deepfake Detection; LSTM; ResNet50"
"Zou, B.; Tan, L.; Song, T.; Yin, H.; Jia, T.","Zou, Boya (59707408700); Tan, Li (16507903900); Song, Tianbao (57192670612); Yin, Hang (60038766800); Jia, Tan (60039187900)","59707408700; 16507903900; 57192670612; 60038766800; 60039187900","A Deepfake Detection Model Based on Optical Flow Estimation","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013051396&partnerID=40&md5=8706d200858ab922e026072b31642b44","The progress in artificial intelligence has resulted in the development of highly authentic AI-generated videos. This enables malicious individuals to effortlessly produce non-existent videos and disseminate them across the internet. This paper proposes a fake video detection method, Deepfake Detection Model based on Optical Flow Estimation(DFDOF), which is based on analyzing the speed of objects in videos. Specifically, a ResNet subdetector is employed to extract image features, a multi-scale optical flow feature extraction module is used to extract optical flow features, and a Co-Attention module is utilized to fuse the features from both branches. Finally, a transformer layer is applied to classify the fused features, further enhancing the discrimination capability. Extensive experimental results demonstrate the strong generalization ability and robustness of our method. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.","abnormal physical rules; AI generated video; attraction mechanism; Deepfake detection; feature fusion; optical flow"
"Kavitha, M.; Raushan, R.; Priya; Kumar, R.","Kavitha, M. (56786641500); Raushan, Ritik (60034521300); Priya (60034216900); Kumar, Rahul (60034217000)","56786641500; 60034521300; 60034216900; 60034217000","DEFENDAI: Enhancing Deepfake Detection with Hybrid Architectures using EfficientNet, XceptionNet, Vision Transform and LSTM","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012823904&partnerID=40&md5=9456d471e055d9dc1274e4bd2989946e","The rapid evolution of deepfake technologies presents a significant threat to digital authenticity and societal trust. This research introduces DEFENDAI, a hybrid deepfake detection system integrating EfficientNet, XceptionNet, Vision Transformers (ViT), and Long Short-Term Memory (LSTM) networks to harness both spatial and temporal features. Trained on a curated subset of the DFDC dataset, the system uses a multi-stage pipeline involving frame extraction, resizing, normalization, and augmentation for preprocessing. Feature extraction is performed using CNNs and ViTs, while LSTM captures sequential dependencies across frames. A web-based implementation using FastAPI and Streamlit enables real-time user interaction and deployment. Comparative results show the hybrid model significantly outperforms standalone models in precision, recall, and generalization. Future work aims at extending datasets, integrating adversarial robustness, and refining real-time detection pipelines for broader applicability. © 2025 IEEE.","AI Security; Deepfake Detection; DFDC; EfficientNet; FastAPI; LSTM; Streamlit; Vision Transformers; XceptionNet"
"Wani, T.M.; Qadri, S.A.A.; Amerini, I.","Wani, Taiba Majid (57216571275); Qadri, Syed Asif Ahmad (57211993044); Amerini, Irene (27567536300)","57216571275; 57211993044; 27567536300","STC-CapsNet: Detecting Audio Deepfakes with Spatio-Temporal Convolutions and Capsule Networks","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012116126&partnerID=40&md5=c502c1566a3dcace1c6692964dc79e59","Capsule networks are a powerful architecture designed to capture hierarchical relationships in data, making them effective for complex classification tasks. In audio deepfake detection, these networks effectively distinguish between real and synthetic audio by capturing subtle time and frequency patterns. Their ability to model intricate dependencies across both domains makes capsule networks especially suited for this challenging task. This study introduces the novel Spatio-Temporal Convolutional Capsule Network (STC-CapsNet), which utilizes mel-spectrograms and grayscale spectrograms for feature extraction. After preprocessing steps like noise reduction and segmentation, temporal and spectral convolutions are applied, followed by capsule layers with dynamic routing to enhance feature representation. The model is evaluated on the FoR dataset, achieving an F1-Score of 98.4% and a low EER of 2.8% using mel-spectrograms, outperforming grayscale spectrograms in both accuracy and error rate. © 2025 IEEE.","Audio Deepfake Detection; Grayscale spectrogram; Mel-spectrograms; Spatio-Temporal Convolutional Capsule Network"
"Agrawal, P.; Shaikh, A.; Bajaj, K.; Mahajan, J.","Agrawal, Palak (60022423400); Shaikh, Alisha (60022423500); Bajaj, Ketan (60021700200); Mahajan, Jayashree (59930026900)","60022423400; 60022423500; 60021700200; 59930026900","Incorporating Psychological and Behavioral Cues for Enhanced Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012109057&partnerID=40&md5=c5af787938a30100691b1a1da78a694a","The growing challenge of detecting deepfakes as generative models continues to advance, making traditional detection methods based on spatial inconsistencies less effective. We aim to harness machine learning techniques to identify subtle human responses that deepfake algorithms find difficult to mimic. We review existing detection techniques, pinpoint gaps in current approaches, and demonstrate why emphasizing temporal features - those related to physiological and behavioral responses - can offer a more resilient solution than relying solely on spatial anomalies. Additionally, we propose an implementation for advanced deepfake detection using a combination of pre-trained CNN models and LSTM networks to analyze the spatial and temporal aspects of videos. We also provide insights for future developments in this proposed approach. © 2025 IEEE.","Behavioral Analysis; CNN; Deepfake Detection; LSTM; Machine Learning; Micro-expressions; Psychological Signals; Temporal Cues"
"Bhandarkawthekar, V.; Navamani, T.M.; Sharma, R.; Shyamala, K.","Bhandarkawthekar, Varad (60010450500); Navamani, T. M. (49361944200); Sharma, Rishabh (60010281400); Shyamala, K. (59204746500)","60010450500; 49361944200; 60010281400; 59204746500","Design and development of an efficient RLNet prediction model for deepfake video detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011341559&partnerID=40&md5=098e7a288d8988a7ccfaa681982a7a03","Introduction: The widespread emergence of deepfake videos presents substantial challenges to the security and authenticity of digital content, necessitating robust detection methods. Deepfake detection remains challenging due to the increasing sophistication of forgery techniques. While existing methods often focus on spatial features, they may overlook crucial temporal information distinguishing real from fake content and need to investigate several other Convolutional Neural Network architectures on video-based deep fake datasets. Methods: This study introduces an RLNet deep learning framework that utilizes ResNet and Long Short Term Memory (LSTM) networks for high-precision deepfake video detection. The key objective is exploiting spatial and temporal features to discern manipulated content accurately. The proposed approach starts with preprocessing a diverse dataset with authentic and deepfake videos. The ResNet component captures intricate spatial anomalies at the frame level, identifying subtle manipulations. Concurrently, the LSTM network analyzes temporal inconsistencies across video sequences, detecting dynamic irregularities that signify deepfake content. Results and discussion: Experimental results demonstrate the effectiveness of the combined ResNet and LSTM approach, showing an accuracy of 95.2% and superior detection capabilities compared to existing methods like EfficientNet and Recurrent Neural Networks (RNN). The framework's ability to handle various deepfake techniques and compression levels highlights its versatility and robustness. This research significantly contributes to digital media forensics by providing an advanced tool for detecting deepfake videos, enhancing digital content's security and integrity. The efficacy and resilience of the proposed system are evidenced by deepfake detection, while our visualization-based interpretability provides insights into our model. © © 2025 Bhandarkawthekar, Navamani, Sharma and Shyamala.","deep learning; deepfake detection; explainable artificial intelligence; Long Short Term Memory Networks (LSTM); ResNet"
"Wang, X.; Zhong, G.; Song, Q.","Wang, Xuefei (60002335600); Zhong, Guoqiang (56393836400); Song, Qiang (60002889600)","60002335600; 56393836400; 60002889600","FakeFormer: Transformer-Based Lightweight Deepfake Video Detection Model","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010821114&partnerID=40&md5=e6dec0fb6c12373c4378d0f6ee02da85","In recent years, deepfake videos become increasingly realistic, making them nearly indistinguishable from the naked eye. The misuse of these deepfake videos poses a major threat to information security, leading researchers to develop effective deepfake video detection models. Video temporal features often contain rich and valuable information for identifying the authenticity of videos. Given that transformers have proven highly effective at modeling these features, researchers have been prompted to develop transformer-based detection models. However, due to the high-dimensional nature of video data, the efficiency of such models is often compromised, making it a challenging problem. In this paper, we propose a specialized transformer-based model for deepfake video detection, called FakeFormer. FakeFormer is a lightweight video-level detection model that maintains high efficiency, despite using a sequence of video frames as direct input. Additionally, FakeFormer fully integrates the strengths of convolutional neural networks and transformers, enabling it to effectively model both the temporal and spatial features of videos to accurately assess their authenticity. Numerous experiments validate the effectiveness of FakeFormer, including intra-dataset evaluation, cross-dataset evaluation and ablation study. © 2025 IEEE.","Deepfake detection; Lightweight video transformer; Spatio-temporal feature"
"Kumar, N.A.K.J.; Omkar Lakshmi Jagan, B.O.L.","Kumar, J. Naveen Ananda (59366191200); Omkar Lakshmi Jagan, B. (57164890100)","59366191200; 57164890100","Enhanced Deepfake Detection Using Deep Learning on Large-Scale Video Data: A Fused ResNet50 and LSTM Approach","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010612239&partnerID=40&md5=08e8830074c17be1c96d0858d7b3d4ba","In today’s digital world, where what we see isn’t always what it seems, the rise of Deepfake technology presents a big challenge to trust in videos. These manipulated videos, created using advanced AI, it helps in distinguishing between real and fake and makes it easier to spread false information. This study addresses the urgent need for reliable Deepfake detection methods using deep learning (DL) techniques within the Deepfake Detection Challenge (DFDC) dataset. Leveraging the vast and diverse DFDC dataset, this research develops robust detection algorithms capable of discerning between authentic and manipulated media content. Recently, researchers focused on Deepfakes detections, but accuracy is very low due to the content availability, rapid growth in digital content creation techniques. We proposed enhanced deep learning approach uses the fused ResNet50 and LSTM models, for more accurate detection of Deepfakes. When we evaluated the model over the DFDC dataset, it attains better F1-score, accuracy, and Logloss values compared to the existing models. The proposed model is intended to reduce the risks posed by Deepfake, including the spread of disinformation and the erosion of reputation and security. The study therefore helps in combating the negative impact of Deepfakes through the application of the DL models toward protecting trust, credibility and integrity in the digital space. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.","Deep learning; Deepfake; DFDC; GAN; LSTM; ResNet50"
"Wen, C.; Zhao, H.; Cheng, W.","Wen, Chunya (59937225300); Zhao, Hui (59938125100); Cheng, Wen (59937041900)","59937225300; 59938125100; 59937041900","Research on depth image tampering detection based on color space fusion","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007656096&partnerID=40&md5=79518cf852d9f8dfcfb6bb7a761ffd78","With the rapid development of image processing technology, image tampering has become increasingly common, posing serious challenges to information security. This paper proposes a depth image tampering detection method based on multicolor space fusion, which enhances the model's ability to perceive tampering features by combining information from RGB and YCbCr color spaces. ConvNeXt is used as the backbone network for feature extraction, and effective fusion of color space features is achieved through feature splicing. Experimental results on the comprehensive data set show that the proposed method reaches more than 99% in accuracy and AUC indicators, proving the effectiveness and reliability of the method in image tampering detection tasks. Future research will focus on further improving the model's detection capabilities in complex scenes. © 2025 SPIE.","color space; ConvNeXt; deepfake detection; feature fusion"
"Soundarya, P.; Vijayalakshmi, R.; Dharshini, P.P.","Soundarya, P. (59935780800); Vijayalakshmi, R. (59078481200); Dharshini, P. Priya (60206978500)","59935780800; 59078481200; 60206978500","Hybrid Deep Learning for Detecting Fake Medical Images and Heart Sounds","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007621674&partnerID=40&md5=5f5c40a5aa0ce64f83d8e0d9d2cae980","The rapid development of deepfake technology has raised serious concerns in numerous fields, particularly in healthcare, where manipulating medical data could compromise patient safety and undermine the reliability of diagnostic processes. This study presents innovative methodology for identifying deepfake alterations in medical images and audio, focusing on MRI and CT scans and heart sound recordings. The proposed system integrates advanced image and audio processing techniques to detect synthetic modifications. Medical images undergo pre-processing steps such as resizing, grayscale conversion, noise injection, and data augmentation, while heart sounds are examined using feature extraction techniques like the Zero Crossing Rate. To detect manipulated regions in images, YOLOv8 is employed, followed by classification through deep-learning models like VGG-19 and ResNet-50. Important metrics are implemented to assess the system's performance, namely accuracy 99.7, precision 86.6, recall 92.8, F1-score 89.6, error rate 0.215, and confusion matrix, ensuring comprehensive assessment of its reliability. Outcomes highlight effectiveness of this approach in identifying deepfake medical data with high accuracy 99.7, offering a vital solution for maintaining the integrity of healthcare diagnostics in the face of evolving threats from synthetic data manipulation. © 2025 IEEE.","Audio processing; Deep learning; Hybrid approach; Image processing; Medical deepfake detection; ResNet-50; VGG-19; Yolov8"
"Mohamed Faheem, M.; Bharathi Mohan, G.","Mohamed Faheem, M. A. (59520660700); Bharathi Mohan, G. (56026524900)","59520660700; 56026524900","Comparative Study: Deepfake Detection in Tamil Speech using Hybrid Convolutional Networks","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007521636&partnerID=40&md5=4228a5fec5bba893f5e029bc58bac6b8","In today's digital landscape, the rise of deepfake content poses a serious challenge to maintaining trust and authenticity online. This paper focuses on detecting deepfake Tamil speech by leveraging advanced machine-learning techniques. Our dataset consists of two classes: 'fake' and 'real,' representing synthetic and authentic Tamil speech recordings, respectively. We use simple yet effective methods like Chromagram, MFCC, and Mel spectrogram to extract key features from the audio data. These features are then fed into two models: a Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) and a Support Vector Machine (SVM). The CNN-LSTM excels at identifying patterns in spectrogram data, while the SVM handles classification tasks efficiently. Our experiments demonstrate the effectiveness of our approach in accurately distinguishing between genuine and falsified Tamil speech, achieving a validation accuracy of 0.9742 for the CNN-LSTM model and 0.96 for the CNN-GRU model. This research contributes to enhancing the security and credibility of digital content in Tamil-speaking communities. By employing straightforward machine learning techniques, we aim to combat the spread of synthetic media manipulation and foster a safer online environment for all users. © 2025 IEEE.","Audio Feature Extraction; Chromagram; CNN-LSTM; Deepfake Detection; Mel Spectrogram; MFCC; SVM"
"Sagar, N.K.; Arukonda, S.","Sagar, N. (57210633577); Arukonda, Srinivas (57214223923)","57210633577; 57214223923","A Novel CNN-LSTM Approach for Robust Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007157377&partnerID=40&md5=ddd8b78901a5058a5af5fbf4a5b91511","The rapid spread of deepfake videos poses significant challenges to the credibility of digital media, raising concerns over pri- vacy, misinformation, and trustworthiness. This research introduces a hybrid model combining Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs) to enhance deepfake detection. By leveraging ResNeXt-50 for extracting relevant features and LSTMs for capturing frame-to-frame dependencies, the proposed architecture effectively detects altered facial features in videos. Key preprocessing techniques, including face detection, extraction, and segmentation, optimize input data by isolating relevant facial regions. Experimental results demonstrate that this approach outperforms current methods in identifying subtle deepfake artifacts, underscoring the need for robust detection mechanisms to protect the credibility of digital media. Future work will explore improved scalability and real-time applications of this technique. © 2025 The Author(s).","Convolutional Neural Networks; Deep Learning; DeepFake Detection; Face Recognition; Image Classification; LSTM; Machine Learning"
"Devi, R.; Sujitha, B.B.","Devi, R. Sahila (57217332222); Sujitha, B. Ben (59362617400)","57217332222; 59362617400","Advancements in Deepfake Detection: A Comprehensive Review of AI-Driven Approaches","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004814208&partnerID=40&md5=a71fa72156b449db85a0f6d2ea070b53","The significant developments in artificial neural network (ANN)-based technologies are crucial for manipulating multimedia content. Although technology has predominantly been used for purposes, such as education, some individuals have exploited it for unlawful or harmful activities. This literature review offers a comprehensive analysis of the advancements in deepfake detection. This study explores state-of-the-art approaches, including convolutional neural network (CNNs), transformers, ensemble model, and binary neural network, that leverage both spatial and temporal feature for effective detection. In addition, the review examines the role of data augmentation, artifact attention, and multi-modal analysis in enhancing detection accuracy across diverse datasets. By analyzing current methodologies and evaluating their strengths and limitations, this paper provides insights into the evolving landscape of deepfake detection by analyzing current methodologies and highlights key challenges such as the need for robust datasets and cross-domain generalization. Future directions are also outlined, emphasizing the potential of self-supervised learning, ensemble approaches, and advancements in artifact detection for more reliable and scalable solutions. © 2025 IEEE.","Deep Learning methods; Deepfake detection; Ensemble methods; Machine Learning-Based Techniques; Temporal and Local Consistency Detection"
"Ghodake, A.B.; Yewale, D.B.; Katariya, T.D.; Khobare, B.S.; Shidore, A.S.","Ghodake, Abhijit Bapurao (59791962600); Yewale, Durgesh Bajirao (58708160000); Katariya, Tejal Dilip (59794324800); Khobare, Bhakti Sahebrao (59790769400); Shidore, Ashvini S. (59793153100)","59791962600; 58708160000; 59794324800; 59790769400; 59793153100","Hybrid Deepfake Detection System: Leveraging AlexNet and LSTM Networks for Enhanced Video Authentication","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004549652&partnerID=40&md5=f267560178789d51ec515a83f9b20553","The rapid proliferation of deepfake technology has presented significant challenges in ensuring the authenticity of digital media, posing security risks in various domains, including social media, politics, and corporate communications. This paper proposes a hybrid deepfake detection system that integrates AlexNet, a powerful convolutional neural network (CNN), and Long Short-Term Memory (LSTM) networks to enhance video authentication. The proposed system lever-ages AlexNet's strong capabilities in feature extraction to analyze spatial features within video frames, while LSTM networks process temporal sequences, capturing the dependencies across frames in video streams. By combining these models, our approach delivers an effective deepfake detection system that is robust to subtle manipulations and capable of identifying deepfakes with high precision. Experiments were conducted on publicly available datasets to evaluate the performance of the hybrid model against standalone models and other baseline approaches. The results indicate significant improvements in detection accuracy, with the hybrid system achieving a higher recall and precision rate compared to traditional CNN and RNN-based methods. Furthermore, the proposed system demonstrated resilience against a wide range of deepfake techniques, including facial reenactment and voice synchronization. This work highlights the importance of combining spatial and temporal analysis for reliable video authentication, paving the way for advanced deepfake mitigation strategies in both public and private sectors. The integration of AlexNet and LSTM networks offers a scalable solution for real-time video analysis, providing critical support in com-bating the spread of disinformation and enhancing trust in digital media. © 2025 IEEE.","AlexNet; Deepfake detection; hybrid model; LSTM networks; video authentication"
"Atamna, M.; Tkachenko, I.; Miguet, S.","Atamna, Mehdi (58784339900); Tkachenko, Iuliia (56038201900); Miguet, Serge (55932981600)","58784339900; 56038201900; 55932981600","WaveConViT: Wavelet-Based Convolutional Vision Transformer for Cross-Manipulation Deepfake Video Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004252373&partnerID=40&md5=b1262851d3a0da2b388d4c177e9705d4","The ease of use and wide availability of high-quality deepfake creation tools raises significant concerns about the reliability and trustworthiness of online content, and makes the task of detecting facial tampering more complicated. As such, the development of effective deepfake detection methods is of utmost importance. In recent years, the facial deepfake detection task took a leap thanks to the development of deep learning-based methods as well as the availability of large datasets of high-quality deepfake videos. Despite the aforementioned methods achieving excellent results when tasked with detecting deepfakes generated using methods seen during training, the cross-manipulation, or generalization, task—where a trained model is exposed to unseen manipulation techniques—is a major challenge which is attracting the attention of the research community. In this paper, we introduce WaveConViT, a novel spatio-temporal architecture for deepfake detection based on Vision Transformers and a two-dimensional discrete wavelet transform. Additionally, we introduce and evaluate a temporal sampling strategy based on frame skipping. We extensively test and benchmark this architecture in the challenging cross-manipulation scenario on the FaceForensics++, Celeb-DF, and DeeperForensics-1.0 datasets, comparing it to a selection of modern, representative Vision Transformer (ViT) and convolutional neural network (CNN) architectures and demonstrating the value of high-frequency features as well as our frame skipping strategy for deepfake detection. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.","Deepfake detection; Discrete wavelet transform; Spatio-temporal features; Video manipulation detection"
"Chu, B.; Xu, X.; Zhang, Y.; You, W.; Zhou, L.","Chu, Beilin (57421013200); Xu, Xuan (57405530900); Zhang, Yufei (58665518800); You, Weike (57195493773); Zhou, Linna (55966192200)","57421013200; 57405530900; 58665518800; 57195493773; 55966192200","Reduced Spatial Dependency for More General Video-level Deepfake Detection","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003890020&partnerID=40&md5=3f12594ded739126b49ab0c66cae616b","As one of the prominent AI-generated content, Deepfake has raised significant safety concerns. Although it has been demonstrated that temporal consistency cues offer better generalization capability, existing methods based on CNNs inevitably introduce spatial bias, which hinders the extraction of intrinsic temporal features. To address this issue, we propose a novel method called Spatial Dependency Reduction (SDR), which integrates common temporal consistency features from multiple spatially-perturbed clusters, to reduce the dependency of the model on spatial information. Specifically, we design multiple Spatial Perturbation Branch (SPB) to construct spatially-perturbed feature clusters. Subsequently, we utilize the theory of mutual information and propose a Task-Relevant Feature Integration (TRFI) module to capture temporal features residing in similar latent space from these clusters. Finally, the integrated feature is fed into a temporal transformer to capture long-range dependencies. Extensive benchmarks and ablation studies demonstrate the effectiveness and rationale of our approach. © 2025 IEEE.","deepfake; deepfake detection; temporal consistency detection"
"Wang, X.; Song, W.; Hao, C.; Liu, F.","Wang, Xinyi (57207046614); Song, Wanru (57205596782); Hao, Chuanyan (26867768500); Liu, Feng (57188574368)","57207046614; 57205596782; 26867768500; 57188574368","Deepfake Detection Method Based on Spatio-Temporal Information Fusion","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003321518&partnerID=40&md5=73f1d21f2ea03ed858e54e5642096667","As Deepfake technology continues to evolve, the distinction between real and fake content becomes increasingly blurred. Most existing Deepfake video detection methods rely on single-frame facial image features, which limits their ability to capture temporal differences between frames. Current methods also exhibit limited generalization capabilities, struggling to detect content generated by unknown forgery algorithms. Moreover, the diversity and complexity of forgery techniques introduced by Artificial Intelligence Generated Content (AIGC) present significant challenges for traditional detection frameworks, which must balance high detection accuracy with robust performance. To address these challenges, we propose a novel Deepfake detection framework that combines a two-stream convolutional network with a Vision Transformer (ViT) module to enhance spatio-temporal feature representation. The ViT model extracts spatial features from the forged video, while the 3D convolutional network captures temporal features. The 3D convolution enables cross-frame feature extraction, allowing the model to detect subtle facial changes between frames. The confidence scores from both the ViT and 3D convolution submodels are fused at the decision layer, enabling the model to effectively handle unknown forgery techniques. Focusing on Deepfake videos and GAN-generated images, the proposed approach is evaluated on two widely used public face forgery datasets. Compared to existing state-of-the-art methods, it achieves higher detection accuracy and better generalization performance, offering a robust solution for deepfake detection in real-world scenarios. © © 2025 The Authors.","Deepfake detection; spatio-temporal information; vision transformer"
"Kasera, G.; Solanki, M.; Kaur, H.; Shah, K.","Kasera, Gungun (59739131300); Solanki, Manya (59738928700); Kaur, Hargeet (57195739115); Shah, Kaushal Arvindbhai (57191625358)","59739131300; 59738928700; 57195739115; 57191625358","A Detailed Exploration to Deepfake: A Cybersecurity Threat","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002707954&partnerID=40&md5=4006cc6180deb92d61f387e055261ca1","The advancement of AI-technologies has brought both innovation and considerable challenges particularly with the emergence of deep fakes-realistic but false images, videos or audios created using deep learning algorithms. These deepfakes pose a serious threat as they can spread misleading information or content which are utilized for malicious purposes. This paper focuses on the creation of deepfake and emphasizing on various models like GANs, CNNs, and LSTMs. This paper also provides a comparative analysis of various deepfake detection techniques and prevention measures to take into considerations. © 2025 IEEE.","Convolutional Neural Networks (CNNs); Cybercrime; Deepfake; Deepfake Detection; Fake Media Prevention; Generative Adversarial Networks (GANs); Long Short-Term Memory (LSTM); Misinformation; Recurrent Neural Networks"
"Mudegol, N.L.; Urunkar, A.","Mudegol, Nandinee L. (57485827300); Urunkar, Abhijeet (57490499300)","57485827300; 57490499300","Supervised Learning Techniques for Deepfake Detection: Integrating ResNet50 and LSTM","2025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002443582&partnerID=40&md5=dd8a622abbdb131f9f1f9b9c285c39af","Deepfake technology has garnered considerable scholarly interest owing to its capacity to generate exceptionally lifelike yet artificially constructed media artifacts, posing serious threats to various aspects of society, including privacy, security, and misinformation. In response to this growing concern, this project aims to develop an effective deepfake detection system using deep learning models. This paper focuses on leveraging supervised learning techniques to distinguish between authentic and deepfake videos. The proposed approach involves collecting a diverse dataset of both authentic and deepfake videos, covering various scenarios and manipulation techniques. Preprocessing techniques are applied to extract relevant features from the videos, preparing them for input to the deep learning models. Several neural network architectures, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and their combinations [13], are explored for their suitability in detecting deepfake content. Transfer learning methodologies are utilized to capitalize on models that have undergone prior training, such as those trained on ImageNet, as feature extractors or initializations for the network. The models are trained on the labeled dataset. The paper aims to detect deepfake in videos by using ResNet50 for feature extraction and training those features on the LSTM Model. © 2025 IEEE.","CNN; Deepfake Detection; ResNet50-LSTM; RNN; Transfer Learning"
"Wang, R.; Mariano, V.Y.","Wang, Ruofan (59549766000); Mariano, Vladimir Y. (15060192800)","59549766000; 15060192800","Research and Performance Evaluation of Deepfake Video Detection Method Integrated Spatio temporal Attention Mechanism","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217360906&partnerID=40&md5=d167b4e9186c86771e36c0c416131993","With the rapid development of deep learning and computer vision technology, Deepfake technology has been able to generate highly realistic fake videos, which brings serious security and ethical challenges to society. Existing detection methods have insufficient ability to deal with the evolving deepfake technology, especially when dealing with the spatio-temporal characteristics of videos. To solve this problem, a deepfake video detection algorithm based on Spatio-Temporal Attention mechanism is proposed. By fusing the Spatio-Temporal information of the video, an Attention-Driven LSTM network (ADLNet) is designed, which uses the attention mechanism to accurately identify the forgery traces and significantly improves the detection effect. The ACC and AUC on the public deepfake dataset FaceForensics++ are 97.17%and 99.33%re-spectively, and the ACC and AUC on the Celeb-DF dataset are 96.78%and 98.36%respectively. The experimental results and visualization verify that the proposed ADLNet model has strong robustness and application prospects in extracting spatio-temporal artifacts. © 2024 Copyright held by the owner/author(s).","Computer Vision; Deep Learning; Deepfake Detection; Temporal Attention"
"Wang, G.; Cai, M.; Sun, Z.; Hu, W.","Wang, Ge (56144355500); Cai, Menghuan (59521022700); Sun, Zikai (59520476900); Hu, Weiyang (59846496000)","56144355500; 59521022700; 59520476900; 59846496000","Spatiotemporal Fusion for Deepfakes Detection in Face Videos","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215533400&partnerID=40&md5=45c15f138103b4205538808e44159573","Deep forgery, a deep learning-based image tampering technique, poses significant challenges to existing detection methods, which primarily focus on identifying spatial artifacts within individual video frames. However, as deep forgery techniques advance, capturing these spatial inconsistencies has become increasingly difficult. To address this, we shift our focus to detecting spatiotemporal features that exist across consecutive video frames. When extracting temporal features, traditional Transformer models struggle to fully capture the temporal dependencies inherent in time series data. We use TimeSformer to extract spatiotemporal features from video frames, which are then fused and used for detection. To further enhance feature extraction, we integrate the SE (Squeeze-and-Excitation) module into the Middle Flow of the Xception network. This module uses global information to highlight important features and ignore irrelevant ones. To test our method’s effectiveness, we conducted comparisons using the FaceForensics++ dataset and performed cross-dataset evaluations with the Celeb-DF dataset. Experimental results show that our detection method achieves 96.12% accuracy on the FaceForensics++ dataset, surpassing other methods. Additionally, the cross-dataset test yields an accuracy of 73.73%, showcasing a degree of generalization across different datasets. © 2024 Copyright held by the owner/author(s).","deep learning; Deepfake detection; image processing; spatiotemporal fusion; TimeSformer"
"Kingra, S.; Aggarwal, N.; Kaur, N.","Kingra, Staffy (57192255022); Aggarwal, Naveen (36875216400); Kaur, Nirmal (57189220908)","57192255022; 36875216400; 57189220908","SFormer: An end-to-end spatio-temporal transformer architecture for deepfake detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202157768&partnerID=40&md5=d569627607c1ddc9a4ccbcc20e813f63","Growing AI advancements are continuously pacing GAN enhancement that eventually facilitates the generation of deepfake media. Manipulated media poses serious risks pertaining court proceedings, journalism, politics, and many more where digital media have a substantial impact on society. State-of-the-art techniques for deepfake detection rely on convolutional networks for spatial analysis, and recurrent networks for temporal analysis. Since transformers are capable of recognizing wide-range dependencies with a global spatial view and along temporal sequence too, a novel approach called “SFormer” is proposed in this paper, utilizing a transformer architecture for both spatial and temporal analysis to detect deepfakes. Further, state-of-the-art techniques suffer from high computational complexity and overfitting which causes loss in generalizability. The proposed model utilized a Swin Transformer for spatial analysis that resulted in low complexity, thereby enhancing its generalization ability and robustness against the different manipulation types. Proposed end-to-end transformer based model, SFormer, is proven to be effective for numerous deepfake datasets, including FF++, DFD, Celeb-DF, DFDC and Deeper-Forensics, and achieved an accuracy of 100%, 97.81%, 99.1%, 93.67% and 100% respectively. Moreover, SFormer has demonstrated superior performance compared to existing spatio-temporal and transformer-based approaches for deepfake detection. © 2024 Elsevier Ltd","Deepfake detection; Digital forensics; Facial manipulation detection; Spatio-temporal; Transformer"
"Tipper, S.; Atlam, H.F.; Lallie, H.S.","Tipper, Sarah (59400692900); Atlam, Hany F. (57194943646); Lallie, Harjinder Singh (36459976300)","59400692900; 57194943646; 36459976300","An Investigation into the Utilisation of CNN with LSTM for Video Deepfake Detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208484692&partnerID=40&md5=3fe5e640b835c306a8b32900a43cb320","Video deepfake detection has emerged as a critical field within the broader domain of digital technologies driven by the rapid proliferation of AI-generated media and the increasing threat of its misuse for deception and misinformation. The integration of Convolutional Neural Network (CNN) with Long Short-Term Memory (LSTM) has proven to be a promising approach for improving video deepfake detection, achieving near-perfect accuracy. CNNs enable the effective extraction of spatial features from video frames, such as facial textures and lighting, while LSTM analyses temporal patterns, detecting inconsistencies over time. This hybrid model enhances the ability to detect deepfakes by combining spatial and temporal analysis. However, the existing research lacks systematic evaluations that comprehensively assess their effectiveness and optimal configurations. Therefore, this paper provides a comprehensive review of video deepfake detection techniques utilising hybrid CNN-LSTM models. It systematically investigates state-of-the-art techniques, highlighting common feature extraction approaches and widely used datasets for training and testing. This paper also evaluates model performance across different datasets, identifies key factors influencing detection accuracy, and explores how CNN-LSTM models can be optimised. It also compares CNN-LSTM models with non-LSTM approaches, addresses implementation challenges, and proposes solutions for them. Lastly, open issues and future research directions of video deepfake detection using CNN-LSTM will be discussed. This paper provides valuable insights for researchers and cyber security professionals by reviewing CNN-LSTM models for video deepfake detection contributing to the advancement of robust and effective deepfake detection systems. © 2024 by the authors.","convolutional neural network (CNN); deepfake; feature extraction; long short-term memory (LSTM); video deepfake detection"
"Saif, S.; Tehseen, S.; Ali, S.S.","Saif, Shahela (57195215570); Tehseen, Samabia (36148756300); Ali, Syed Sohaib (59955271500)","57195215570; 36148756300; 59955271500","Fake news or real? Detecting deepfake videos using geometric facial structure and graph neural network","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195171350&partnerID=40&md5=e7b132d9029fd6ad387c7ed6a02b087b","Deepfake videos are increasingly used in spreading fake news or propaganda having a serious impact on people and society. Traditional deepfake detectors exploit spatial and/or temporal inconsistencies to differentiate between real and fake videos. Owing to the rapidly advancing deepfake creation algorithms, the latest detectors have made use of physiological and biological facial features to create more generic solutions. Our proposed solution uses facial landmarks as the physiological identifiers of a person's face and through them develops a relationship between facial areas in normal speech and tampered speech. By creating a graph structure from the resulting sparse data, we were able to use a spatio-temporal graph convolutional network for classification, which has significantly fewer parameters and a shorter training time than traditional CNNs. We conducted a multitude of experiments on 3 datasets, utilizing spatio-temporal features. The results demonstrate that this technique has better generalization, and high performance compared to latest research in deepfake detection without the reliance on large deep learning models which are tuned to learning image discrepancies more than data patterns. Moreover, our use of facial landmark-based features with a graph structure paves the way for the development of an explainable AI model that can be relied on. © 2024 Elsevier Inc.","Deepfake detection; Deepfake videos; Deepfakes; Forgery detection; Graph convolution network; Video forgery"
"Anandhasivam, V.S.; Anusri, A.K.; Logeshwar, M.; Gopinath, R.","Anandhasivam, V. S. (59672991300); Anusri, A. K. (59673758400); Logeshwar, M. (59672687500); Gopinath, R. (56594634800)","59672991300; 59673758400; 59672687500; 56594634800","Enhancing Deepfake Detection Through Hybrid MobileNet-LSTM Model with Real-Time Image and Video Analysis","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000244278&partnerID=40&md5=83582d8bdac27d2adb7392d055e9ef0e","The race to crush information integrity and public trust is being won by one thing: deepfakes, AI manipulated media. The goal is to enhance the Deepfake detection using MobileNet V3 and LSTM network. MobileNet's lightweight CNN architecture is able to induce the visual features in an image by an image and in a video and can pick up the slight visual clues of textures and facial structure. Some temporal inconsistencies that cannot be seen by image base methods are then analyzed using an LSTM network. The hybrid model is trained on real and deepfake media datasets, and is thus adaptable to emerging deepfake techniques. This has a user face interface to analyze the real time and fly media to get the analysis and analysis score and visual feedback of the identified artifacts. Unique to this system is its versatility for images and videos, and its real time capability, making it a suitable choice for practical use in social media, journalism, and law enforcement combating the spread of misinformation with a guarantee of digital media authenticity © 2024 IEEE.","deepfake detection; image analysis; media integrity; mobilenet-lstm; real-time detection; temporal modeling; video analysis"
"Rajeev, F.P.; Shetty, S.D.","Rajeev, Faaizah Panapparambil (59664587100); Shetty, Sujala Deepak (26421402900)","59664587100; 26421402900","Detecting Deepfakes and Artificial Intelligence-Generated Art","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219545405&partnerID=40&md5=7afde224d4bfc3d0106a503f8ca2d658","Recent years have seen innovative advancements in Artificial Intelligence (AI) in the realms of image and video processing, speech and audio recognition, and pattern recognition, leading to the boom in AI art and deepfakes. The evolution of ChatGPT, Dalle-2, and Midjourney has sparked debate about whether AI art qualifies as true art and whether it will eventually displace human artists and creators. AI art generation models are trained on large datasets, often consisting of copyrighted art. Due to this, the resulting picture produced is frequently a manipulated version of the art piece, namely through various techniques - CutMix, Adversarial Data Poisoning, Inpainting, and Style Transfer. This manipulation constitutes a violation of the rights held by the artists. On a similar note, deep fakes pose threats to privacy and security. Society is entering a new age of misinformation and confusion, where they are victims of deepfake phishing scams, identity theft, and malicious political interference. These issues necessitate the development of state-of-the-art algorithms to detect real-life photos and videos, as well as painting and art manipulation. This paper dives deep into the topic of AI art and Deepfake detection, highlighting the most efficient machine learning classification techniques on the DeepfakeArt Challenge and Deepfake Detection Challenge dataset. It aims to highlight the best algorithm that provides reliable results and assists future research in this newly emerging area. ©2024 IEEE.","AI Art; Deepfake Detection; Image Authenticity; Visual Manipulation"
"Shifna, N.F.; Baalaji, K.; Niharika, P.; Swathi, P.; Krishna, P.R.; Leena, R.","Shifna, N. Fathima Shrene (59007923900); Baalaji, K. (57210600572); Niharika, Pasapala (58175176000); Swathi, P. S. (57262326700); Krishna, Pasangulapati Rama (59662615400); Leena, Reddimala (59661588900)","59007923900; 57210600572; 58175176000; 57262326700; 59662615400; 59661588900","Identifying Machine Generated Tweets: Deepfake Detection on Social Media","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219344138&partnerID=40&md5=639b0bb642d5cb200b3f666955944a68","In the digital age, the expansion of machine-generated content poses significant challenges to the integrity of online communications. This project addresses the critical issue of detecting fake tweets on Social networking websites platforms by utilizing modern advanced deep learning algorithms and fast text embedded data. Deep fakes, particularly machine-generated text, are becoming increasingly sophisticated, making it essential to develop robust methods for identifying such content. This approach integrates current deep learning models with fast text embedding techniques to differentiate between human-written and machine-generated tweets. These models are fine-tuned to classify tweets based on semantic and syntactic features captured through pre-trained embeddings. Specifically, quick text embeddings in conjunction with convolutional neural networks (CNN) and long short-term memory networks (LSTM) are used to propose a novel approach to deepfake detection using deep learning techniques in this study. Rapid text embeddings strengthen the model's capacity to distinguish between real and fake information by capturing semantic subtleties and contextual linkages in textual data. In order to extract hierarchical features from tweet sequences and to capture temporal dependencies, our approach combines these embeddings with CNNs and LSTMs. Using a dataset of tweets, the suggested method was assessed and found to perform better than conventional techniques in identifying content that was created by machines. © 2024 IEEE.","CNN; Deep learning; Deepfake Detection; Fake tweets; LSTM; Machine-generated content; rapid text embeddings; semantic analysis; syntactic features; Text embeddings"
"Abisha, M.B.; Kathrine, J.W.; Kushmitha, S.","Abisha, M. B. (59511603500); Kathrine, Jaspher W. (57130545500); Kushmitha, S. (59546405800)","59511603500; 57130545500; 59546405800","Capsule Networks and LSTM Models for Robust Deepfake Detection in Audio and Video","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217181497&partnerID=40&md5=4404f08e4804e94140ad60e8bd904f45","Due to the fast advancement of deepfakes, notable difficulties have been presented by deepfake innovation in confirming the believability of computerized content, especially on the audio and video front. This paper introduces a new hierarchical model that articulates Capsule Networks within LSTM for the effective and robust deepfake detection in both audio-video frameworks. It is argued that using Capsule Networks, spatial mindfulness capabilities enable the detection of subtle spatial objects in video outlines, whereas, LSTM models capture transient objects in audio arrangements and video frames over time. Thus, our approach implements two designs at once that reached progressed discovery accuracy, addressing spatial and temporary disorders, inherent in deepfake media. It is discovered from the outcomes that this model is very much effective on several datasets suggesting its better generalizing capability in different types of Deepfake media. In its current form, the design of this approach seems capable of being used for the real-time detection task. In this regard, this research provides practical insights into the development of media forensics offering an approach that can be more effective in the role of dissemination of fake news prevention, and fills the gap in the current literature, thus advancing the overall understanding of media forensics. © 2024 IEEE.","Audio Deepfake; Audio Pre-processing; Capsule Networks; Data Pre-processing; Deepfake Detection; Digital Content Authenticity; Face and Voice Manipulation; Face-Swapping; Feature Extraction; Long-Term Memory; LSTM Networks; Sequence Analysis; Video Deepfake; Video Frame Extraction; Voice Synthesis"
"Koritala, S.P.; Chimata, M.; Polavarapu, S.N.; Vangapandu, B.S.; Gogineni, T.K.; Manikandan, V.M.","Koritala, Sai Pragna (59483790200); Chimata, Mahitha (59172107400); Polavarapu, Sai Naren (59483514800); Vangapandu, Bhavya Sri (59484181200); Gogineni, Tarun Krishna (59298629400); Manikandan, V. M. (58626529300)","59483790200; 59172107400; 59483514800; 59484181200; 59298629400; 58626529300","A Deepfake detection technique using Recurrent Neural Network and EfficientNet","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212870064&partnerID=40&md5=277d7f9d9582cb79c45cfb024c56dd83","A deepfake is a computer-generated image or video that appears to be real but is a fabricated representation created to make an individual appear to be saying or doing something that did not occur. Deepfakes generate misleading or deceptive information by manipulating and superimposing faces onto pre-existing footage using artificial intelligence. This paper introduces a novel approach for deepfake detection through a combination of EfficientNet and Recurrent Neural Networks (RNNs). This method enhances detection efficiency by leveraging the hierarchical features acquired by EfficientNet and employing RNNs, specifically Long Short-Term Memory (LSTM) networks, to capture temporal dependencies. Application of this approach to the Celeb-DF dataset resulted in an accuracy of 99.98%. ©2024 IEEE.","Deepfake detection; EfficientNet; LSTM; Recurrent Neural Network(RNN)"
"Tantawy, O.; Elshafee, A.","Tantawy, Omar (59480817300); Elshafee, Ahmed M. (14031411900)","59480817300; 14031411900","Advancements in Deepfake Detection: Leveraging Bi-LSTM-CNN Architecture for Robust Identification","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212584890&partnerID=40&md5=078a246c7d232e426e78cdad080bcaab","In our modern era, the saying 'visual evidence is conclusive' no longer holds true, presenting significant implications across various spheres of our lives. With the rapid advancement of technology, the creation of synthetic media, particularly deepfakes, has become remarkably facile. Some applications even facilitate the creation of deepfakes directly on handheld devices. The identification of deepfakes poses a formidable challenge as they can be imperceptible to the human eye. Nevertheless, researchers are actively engaged in developing methodologies to discern deepfakes. Deepfakes constitute media generated through AI algorithms. These algorithms assimilate attributes from a reference image and overlay them onto a source image. In our pursuit of identifying video deepfakes, we employ deep learning architectures such as Bi-LSTM integrated with CNN. Our approach capitalizes on 50 epochs of training to attain a remarkable accuracy of up to 98.7%. We leverage transfer learning to construct a robust deepfake detection model. Initially, we utilize a pretrained CNN, dubbed Bi-LSTM-CNN, to extract salient features and construct feature vectors. Subsequently, the Bi-LSTM layer is trained using these feature vectors. The resulting confusion matrix substantiates the efficacy of our model, with validation and testing accuracies reaching an impressive level. © 2024 IEEE.","Advancements in Deepfake Detection; Leveraging Bi-LSTM-CNN Architecture for Robust Identification"
"Li, R.; Yin, H.; Li, Y.; Li, H.","Li, Ruotong (59415767300); Yin, Huanpu (56984958900); Li, Yan (59415648900); Li, Haisheng (55707571100)","59415767300; 56984958900; 59415648900; 55707571100","V3CViT: Deepfake Detection Based on Video Vision Transformer and 3D Convolution Network","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209570938&partnerID=40&md5=d3d6b7e5ea7915470d42cd7e0fb69fa4","With the advancement of Generative Adversarial Networks in synthesizing fake face videos, the proliferation of forged products poses a significant threat to societal security. In video-based detection methods, compression operations cause the disappearance of discriminative features, and the use of deep learning methods often overlooks the interrelationships between features, resulting in limited model generalization. Therefore, we propose a deepfake video detection framework named V3CViT. This framework combines a video vision transformer and 3D convolutional neural networks to extract facial features, capturing facial characteristics in both temporal and spatial dimensions and combining them to extract effective features using attention mechanisms. Subsequently, the structured facial feature maps are utilized with a Gated Graph Convolutional Network to learn facial relationship information for the detection task. The experiments show that compared to existing detection methods, our model can comprehensively capture facial features in forged videos and achieves more significant accuracy on the mixed dataset. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","Deepfake detection; Facial relationship learning; Spatio-temporal modeling; Video vision transformer"
"Vaidya, A.O.; Dangore, M.; Borate, V.K.; Raut, N.; Mali, Y.K.; Chaudhari, A.","Vaidya, Anusha Omkar (58909155800); Dangore, Monika Y. (56029154700); Borate, Vishal Kisan (56642826300); Raut, Nutan (59397701500); Mali, Yogesh Kisan (25723576800); Chaudhari, Ashvini R. (57289544700)","58909155800; 56029154700; 56642826300; 59397701500; 25723576800; 57289544700","Deep Fake Detection for Preventing Audio and Video Frauds Using Advanced Deep Learning Techniques","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208283475&partnerID=40&md5=c6b295f59e6d76c28be156c4079d15cb","Deepfakes allow for the automated gen- eration of fake video content, often accomplished through the use of generative adversarial networks. To address the increasing issue of deepfakes, this study focuses on constructing a model that incorporates advanced techniques. The researchers combined the ResNeXt, Long Short-Term Memory (LSTM), and ResNet architectures, selecting them based on their effectiveness in handling complex visual data and capturing temporal dependencies. Prior to detection, the dataset underwent pre-processing using the Multi-Task Cascaded Convolutional Neural Network (MTCNN), which facilitated the accurate extraction of facial regions. Importantly, the study evaluated the model across three diverse and significant datasets: the Face Forensics++ dataset (FF-DF), the Celeb-DF dataset, and the Facebook Deepfake Detection Challenge (DFDC) dataset. This comprehensive evaluation en- sured the model's ability to generalize and its suitability for real-world scenarios, as demonstrated by its exceptional detection accuracy. The combination of models employed in this study yielded highly accurate results and remained robust in the face of evolving deepfake technology. © 2024 IEEE.","CelebDF; Detection Accuracy; DFDC; Face Forensics++; LSTM; Model Architecture; MultiTask Cascaded Convolutional Neural Network (MTCNN); Preprocessing; ResNet; ResNeXt"
"Chowdary, B.V.; Prabhakar, M.; Akhil, M.; Pavan, K.; Teja Reddy, B.P.T.","Chowdary, B. V. (57443139500); Prabhakar, Marry (59363403100); Akhil, Mavoori (58981882800); Pavan, Komirishetty (58981322700); Teja Reddy, B. Pavana (58982437500)","57443139500; 59363403100; 58981882800; 58981322700; 58982437500","Deep Fake Detection using Adversarial Ensemble Techniques","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206120130&partnerID=40&md5=a778e07a417adba34b5780d6ca6bc66f","The rise of deepfake technology has resulted in unprecedented challenges in the field of digital media verification, posing significant threats to personal security, misinformation, and trust in digital content. In response to this pressing issue, this study presents an advanced deepfake detection system using deep learning approach. Specifically, the proposed system integrates ResNeXt and Long Short-Term Memory (LSTM) architectures to accurately identify the manipulated content. By leveraging the spatial-temporal features captured by these combined architectures, the proposed system aims to enhance the detection rate of deepfakes across various media formats. The proposed approach offers a robust solution to counteract the evolving sophistication of deepfake technology, thereby enhancing the integrity and authenticity of digital media content. © 2024 IEEE.","Deep Learning; Deepfake Detection; Digital Media Verification; LSTM; ResNeXt"
"Teng, H.; Lin, C.-Y.","Teng, Hao (59357743000); Lin, Chia Yu (57129215700)","59357743000; 57129215700","Dynamic and Static Features Extraction for Deepfake Detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205815704&partnerID=40&md5=edc739d0d147b57c85a0dfe83445d62e","Deepfake has emerged as a significant concern due to its ability to generate fake images and synthesize realistic videos. The increasing development of new techniques for deep-fake creation raises concerns about the cross-forgery issue. Cross-forgery indicates that a model is initially trained to recognize a particular fake and must work against a different unknown forgery. Training a model needs substantial quantities of data, a challenge compounded if the deepfake generation technique is relatively new. Addressing cross-forgery is a critical and essential challenge that requires resolution. In order to solve cross-forgery, our effort presents a method that combines dynamic and static features to identify forgery. For the static component, we extract features similar to general deepfake detection techniques using a single RGB frame as input. Simultaneously, we utilize optical flow analysis to capture changes between consecutive frames for the dynamic part. Our experiments reveal a clear advantage in utilizing combined features, which is particularly evident in cross-forgery scenarios. Specifically, when encountering certain categories, the performance improvement is significant, demonstrating four times better than single-feature models. © 2024 IEEE.","Deepfake detection; multimodel; optical flow; vision transformer"
"She, H.; Hu, Y.; Liu, B.; Li, J.; Li, C.-T.","She, Huimin (58504567900); Hu, Yongjian (35766130600); Liu, Beibei (55544736500); Li, Jicheng (57201859261); Li, Chang Tsun (26648782200)","58504567900; 35766130600; 55544736500; 57201859261; 26648782200","Using Graph Neural Networks to Improve Generalization Capability of the Models for Deepfake Detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202750625&partnerID=40&md5=7402f969a1c0852df0b00c7435ca4ba5","Deepfake detection plays a key role in preventing the misuses of artificial intelligence in video editing. Current deep learning-based deepfake detection methods often perform quite well in intra-dataset testing, but they may lose good performance in cross-dataset testing. In other words, generalization capability is still a crucial problem to be resolved. In this paper, we address deepfake detection by treating an image as non-Euclidean data and representing it as a graph so as to infer the informative connections between image patches/nodes to improve the detector's generalization capability. Specifically, we propose a graph neural network-based paradigm that casts deepfake detection as a graph binary classification problem. First, we propose a dual-branch network to extract node features from both RGB images and their color difference images (CDIs) via the Transformer-based trainable node encoder module (TNEM). Second, we adopt the adjacency matrix to establish the connections of the nodes and further optimize the graph representation by applying the adaptive threshold to the adjacency matrix. Third, multi-head graph convolutional neural networks are carried out for node feature extraction. RGB node features and CDI node features are concatenated and separately fed into the graph classifier and node classifier for forgery detection and forgery localization. Experimental results demonstrate that our method can overall outperform other state-of-the-art methods on 7 popular benchmark datasets. Notably, our model achieves the highest AUC values of 96.19%, 80.99% and 87.68% on Celeb-DF-V2, DFDC and DFDCP in turn when trained on FF++ (C23). The visualization of node classification results also provides good interpretability of our proposed approach. © 2005-2012 IEEE.","color difference; Deepfake detection; generalization; graph classification; graph neural network; transformer"
"Vignesh, T.; Tarun, P.H.; Parthav, R.; Bhargavi, V.","Vignesh, Thipparthi (59297471800); Tarun, Potharlanka Harish (58420512500); Parthav, Ryagalla (59172007300); Bhargavi, V. (59397740800)","59297471800; 58420512500; 59172007300; 59397740800","DeepFake Face Detection using Machine Learning with LSTM","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195980968&partnerID=40&md5=0f998366ff82456671b6f5178c4da6c9","Fake face images that are increasingly convincing and realistic can be created because to the development of face image manipulation (FIM) technologies like Face to Face and Deepfake, which can damage the legitimacy and trustworthiness of online content. Malicious uses of these technology include blackmailing people, posing as celebrities, and disseminating false information. As a result, creating trustworthy and strong techniques to identify FIM and safeguard the integrity of digital media is essential. Numerous current techniques utilize on models built on convolutional neural networks (CNNs), which are capable of detecting FIM by examining a face's visual characteristics. But because these models are frequently tested and trained on certain datasets or circumstances. Furthermore, they might not be able to record the temporal information that is included in video data and can be used to identify irregularities or strange anomalies in FIM videos. We provide a novel method that uses both geographical and temporal information to detect FIM in order to get over these difficulties. We present a new type of residual network called CRNet, which is dependent on Convolutional Long Short-Term Memory (LSTM) and is capable of processing a series of consecutive pictures taken from a movie. The model can learn temporal information because to its design, which is essential for spotting oddities that occur in between frames of FIM movies. We performed extensive tests with several kinds of FIM videos from the Kaggle dataset. © 2024 IEEE.","Deepfake detection; Image manipulation; Kaggle; Long-Short Term Memory (LSTM); Residual next convolution neural network (Xception CNN)"
"Chen, T.; Yang, S.; Hu, S.; Fang, Z.; Fu, Y.; Wu, X.; Wang, X.","Chen, Tiewen (58897683400); Yang, Shanmin (57208777244); Hu, Shu (57221150967); Fang, Zhenghan (57204103923); Fu, Ying (55712884400); Wu, Xi (59463954300); Wang, Xin (57225117859)","58897683400; 57208777244; 57221150967; 57204103923; 55712884400; 59463954300; 57225117859","Masked Conditional Diffusion Model for Enhancing Deepfake Detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188605005&partnerID=40&md5=fc750cb056a49708d96b4e544adbb36c","Recent studies on deepfake detection have achieved promising results when training and testing faces are from the same dataset. However, their results severely degrade when confronted with forged samples that the model has not yet seen during training. In this paper, deepfake data to help detect deepfakes. this paper present we put a new insight into diffusion model-based data augmentation, and propose a Masked Conditional Diffusion Model (MCDM) for enhancing deepfake detection. It generates a variety of forged faces from a masked pristine one, encouraging the deepfake detection model to learn generic and robust representations without overfitting to special artifacts. Extensive experiments demonstrate that forgery images generated with our method are of high quality and helpful to improve the performance of deepfake detection models. © 2024 IEEE.","Conditional Diffusion Model; Data Augmentation; Deepfake Detection"
"Guo, Z.; Li, Z.; Peng, Y.; Kong, X.","Guo, Ziteng (59730425400); Li, Zehang (60098998100); Peng, Yingshan (60094078900); Kong, Xinyu (60098854400)","59730425400; 60098998100; 60094078900; 60098854400","Research on Real-Time Deepfake Image Detection System Based on a Self-Supervised Learning Framework","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016014846&partnerID=40&md5=1851e535d71533f5f1c8507d0139ae19","The rapid evolution of deepfake technology raises significant security and ethical concerns, as its potential misuse could lead to the spread of disinformation. In this study, we propose an advanced real-time deepfake image detection system based on a self-supervised learning framework. The model is based on the most advanced visual transformers (ViTs) and integrates contrastive representation learning in order to effectively distinguish between manipulated content and real images. Furthermore, we have incorporated a temporal consistency module that leverages subtle temporal artefacts to enhance the precision of detection, particularly in video frame sequences. The architectural design is devised to capture spatial and temporal anomalies, thereby ensuring robust detection performance in real-time scenarios. The experimental outcomes on benchmark datasets of Celeb-DF demonstrate that the proposed model exhibits a notable superiority to the existing deepfake detection methods in terms of accuracy and efficiency. © 2024 IEEE.","Contrast means learning; Deepfake detection; self-supervised learning; Visual Transformers"
"Singh, D.; Singh, P.; Bhandari, R.","Singh, Deepanshu (59472338700); Singh, Prabhdeep (58728154000); Bhandari, Rahul (57201373110)","59472338700; 58728154000; 57201373110","A Comprehensive Review of Deepfake Detection In Advanced Neural Network Architectures and Deep Learning Strategies","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003299521&partnerID=40&md5=b1d1bb7b215a1aa9abb9cd117796594e","The proliferation of deepfakes, facilitated by advancements in machine learning and artificial intelligence, poses a significant challenge to online security and information integrity. This study reviews current deep fake detection techniques, focusing on methodologies for identifying manipulated content in images, audio, and video. 17 articles were selected for this study using Prisma guidelines. The function of sophisticated machine learning models, such as Long Short-Term Memory (LSTM) networks and (CNNs), is highlighted for automating detection processes. The review also examines notable algorithms and datasets, including FaceForensics++, MesoNet, and XceptionNet, and their effectiveness in enhancing detection accuracy. A case study using frame extraction and resizing techniques demonstrates a detection accuracy of 62%, with precision at 61 %, recall at 63%, and an F1 score of 62% is also included. The findings underscore the need for ongoing refinement and adaptation of detection systems to counteract the evolving nature of deepfake technologies. The study providing thorough overview of the state-of-the-art in deepfake detection, offering insights into effective methodologies and future research directions. © 2024 IEEE.","CNNs; Deepfake detection; Digital media integrity; DL; Generative adversarial networks(GAN); Introduction; LSTMs; Machine learning; Misinformation detection; Neural network fusion; Spatial-temporal features; Video analysis"
"Bharti, A.; Sinha, V.K.; Peter, J.S.P.; Esther, B.","Bharti, Ayush (59540669400); Sinha, Vishal Kumar (59540687100); Peter, J. Selvin Paul (60023910100); Esther, B. Priya (57076747600)","59540669400; 59540687100; 60023910100; 57076747600","ForgeryDetect: AI-driven Deep Fake Identification","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001659409&partnerID=40&md5=e3e21ffa340b5130bddf53a016999693","The rise of deepfake videos presents substantial obstacles in guaranteeing the genuineness and reliability of visual media material. This study focuses on the crucial problem of identifying deepfake content by introducing a sophisticated method that combines ResNext CNNs along with LSTM models, utilizing transfer learning approaches. Employed a comprehensive approach by using a wide array of datasets such as FaceForensics++, Celeb-DF, and the Deepfake Detection Challenge. Utilizing transfer learning, a ResNext CNN that has been pre-trained is employed to extract complex features from video frames. This enables the subsequent training of an LSTM layer to identify temporal patterns and distinctive cues that are exclusive to modified videos. The effectiveness of fusion model has been demonstrated through extensive experimentation and validation on these rich datasets. It demonstrates a strong ability to reliably distinguish between authentic and manipulated movies, regardless of the different facial alteration techniques, environmental variables, and complex situations seen in real-life situations. The integration of ResNext-based feature extraction with LSTM-based temporal modeling shows substantial progress in the detection of deepfake videos. The efficacy of approach is highly promising, since it provides both dependability and scalability. This study makes a significant contribution to the field of deepfake identification by offering a powerful collection of techniques to tackle the growing concerns surrounding disinformation and manipulation in visual media. © 2024 IEEE.","Celeb-DF; Convolutional neural networks (CNNs); Deepfake detection; Deepfake Detection Challenge; FaceForensics++; long short-term memory (LSTM); LSTM; ResNext; Transfer learning; Video manipulation"
"Sundaram, V.; Babitha, S.; Vekkot, S.","Sundaram, Vikram (59137402700); Babitha, S. (59714848800); Vekkot, Susmitha (36615942400)","59137402700; 59714848800; 36615942400","Leveraging Acoustic Features and Deep Neural Architectures for Audio Deepfake Detection","2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001410570&partnerID=40&md5=c5d4c8c18c8ff81577310de74469bcaa","This research presents a comparative analysis of various audio features and high-level architectures for deefake detection with emphasis on computational efficiency. Several light-weight models are proposed as opposed to GAN-based approaches in literature for evaluating custom-generated deep-fakes. The model is trained on Fake or Real dataset and achieved commendable performance using MFCC-Conformer and MFCC-LSTM feature-model combinations by achieving 87.61% and 87.52% accuracy, respectively. Specifically, the MFCC-Conformer recorded a TN of 526 and a FN of 18, along with an AUC score of 0.96, while the MFCC-DenseNet achieved a TN of 535, an FN of 9, and an AUC score of 0.96, underscoring their effectiveness in identifying fake audios. The outcomes underscore the effectiveness of the proposed models in combating the proliferation of misleading media content. © 2024 IEEE.","Audio Deepfake detection; Audio features; Conformer; DenseNet; LSTM; MFCC"
"Yang, G.; Xu, K.; Fang, X.; Zhang, J.","Yang, Gaoming (36976637300); Xu, Kun (57920584000); Fang, Xianjin (26423619800); Zhang, Ji (57225122203)","36976637300; 57920584000; 26423619800; 57225122203","Video face forgery detection via facial motion-assisted capturing dense optical flow truncation","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139500002&partnerID=40&md5=4e68234623468347addaa31a2254a450","Deep learning advancements have resulted in breakthroughs in facial forgery techniques. Facial forgery videos are growing increasingly lifelike, making it impossible for people to tell the difference between the real and the fake. The proliferation of facial forgery techniques, as well as the slowness with which they may be detected, could jeopardize personal data security. As a result, it’s critical to look at approaches that can be trained on both real and fake videos and then utilized to identify facial forgeries or not in videos. This study found that forged face videos undergo truncation between consecutive frames of optical flow imaging after dense optical flow processing. We present an approach for detecting video face forgery that extracts and analyzes characteristics from real and fake material, then use those features to train classification models on Celeb-DF and FaceForensics++. In addition, we employ a unique facial double-triangle region to assist in the extraction of video inter-frame feature data. Experiments results show that the facial motion features extracted from the double triangle region successfully assist in capturing the dense optical flow truncation. Extensive evaluation suggests that our proposed approach is effective for video face forgery detection. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Deepfakes detection; Dense optical flow; Face forgery detection"
"Khalid, F.; Javed, A.; Ain, Q.-U.; Ilyas, H.; Irtaza, A.","Khalid, Fatima (58034449300); Javed, Ali (57190125008); Ain, Qurat Tul (59676107900); Ilyas, Hafsa (57894630000); Irtaza, Aun (54882450900)","58034449300; 57190125008; 59676107900; 57894630000; 54882450900","DFGNN: An interpretable and generalized graph neural network for deepfakes detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150031931&partnerID=40&md5=3edfb1f198f3f130968e2c5e40bd4c42","Deepfakes are generated using sophisticated deep-learning models to create fake images or videos. As the techniques for creating deepfakes improve, issues like defamation, impersonation, fraud, and misinformation on social media are becoming more prevalent. Existing deep learning-based deepfakes detection models are not interpretable and don't generalize well when tested across diverse deepfakes generating techniques and datasets. Therefore, the creation of reliable and effective deepfakes detection algorithms is required which are not only generalizable but also interpretable. This paper introduces a novel graph neural network-based architecture to identify hyper-realist deepfake content. Currently, very limited efforts have been done to address the problem of deepfakes detection using graph neural networks. The proposed model is based on the pyramid structure that takes advantage of multi-scale images property by extracting features with progressively smaller spatial sizes as layer depth increases. The method first sliced the image into patches, which are referred to as nodes, and then constructed a graph by connecting the nearest neighbors. To transform and exchange information between all nodes, the proposed model has two basic modules: GraphNet, which uses graph convolution layers to aggregate and update graph information, and FFN, which has linear layers for the transformation of node features. The effectiveness of the method is assessed using the diverse Deepfake Detection Challenge dataset (DFDC), FaceForensics++ (FF++), World Leaders dataset (WLRD), and the Celeb-DF. To demonstrate the generalizability of the proposed method for accurate deepfakes detection, open/close set, cross-set, and cross-corpora evaluations were also performed. The AUC values of 0.98 on FF++, 0.95 on Celeb-DF, 0.92 on DFDC, and 1.00 on most of the sets of WLRD datasets demonstrate the efficacy of the method for identifying manipulated facial images produced using various deepfakes techniques. © 2023 Elsevier Ltd","Celeb-DF; Deepfakes; DFDC; FaceForensics++; Graph Neural Network; World Leaders Dataset"
"Liang, P.; Liu, G.; Xiong, Z.; Fan, H.; Zhu, H.; Zhang, X.","Liang, Peifeng (59775033100); Liu, Gang (56403521800); Xiong, Zenggang (22636267500); Fan, Honghui (34871760900); Zhu, Hongjin (55637664700); Zhang, Xuemin (55715193100)","59775033100; 56403521800; 22636267500; 34871760900; 55637664700; 55715193100","A facial geometry based detection model for face manipulation using CNN-LSTM architecture","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150044861&partnerID=40&md5=1ebee9494551bb9b3c516cfb837567f9","This issue of DeepFake technique that may cause great threat to privacy, democracy, and national security has attracted the attention of deep learning researchers. DeepFake detection, therefore, has been a very hot issue in deep learning research. The face landmark feature maps are often used by many DeepFake approaches in generating fake faces. It also provides key information to help to detect manipulated face images. In this paper, we propose a detection approach for manipulated face images. To make full use of face landmark information, we propose a facial geometry prior module (FGPM) to extract facial geometry feature maps. Then the facial geometry feature maps are embedded into upsampling feature maps generated by the CNN-LSTM network. The proposed FGPM exploits facial maps and frequency domain correlation to analyze the discriminative characteristics between manipulated and non-manipulated regions by incorporating the CNN-LSTM network. Finally, a decoder is used to learn the mapping from low-resolution feature maps to pixel-wise to predict manipulation localization. Or a softmax classifier is used to predict real or fake face images. By experimenting on several popular datasets, the proposed detection model has demonstrated the capability of localizing manipulation at the pixel level and with a high prediction on real or fake face images. © 2023","CNN-LSTM; DeepFake detection; Facial analysis; Facial geometry prior module; Resampling"
"Guan, J.; Zhou, H.; Guo, Z.; Hu, T.; Deng, L.; Quan, C.; Fang, M.; Zhao, Y.","Guan, Jiazhi (57220211108); Zhou, Hang (57211168696); Guo, Zhizhi (57913633700); Hu, Tianshu (57563712500); Deng, Lirui (57206484377); Quan, Chengbing Bin (57098796100); Fang, Meng (55445603900); Zhao, Youjian (35933102400)","57220211108; 57211168696; 57913633700; 57563712500; 57206484377; 57098796100; 55445603900; 35933102400","Dual-Modality Co-Learning for Unveiling Deepfake in Spatio-Temporal Space","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163697185&partnerID=40&md5=8cebfc10382ee49c66e589eb2c74fafd","The emergence of photo-realistic deepfakes on a large scale has become a significant societal concern, which has garnered considerable attention from the research community. Several recent studies have identified the critical issue of ""temporal inconsistency""resulting from the frame reassembling process of deepfake generation techniques. However, due to the lack of task-specific design, the spatio-temporal modeling of current methods remains insufficient in three critical aspects: 1) inapparent temporal changes are prone to be undermined compared to abundant spatial cues; 2) minor inconsistent regions are often concealed by motions with greater amplitude during downsampling; 3) capturing both transient inconsistencies and persistent motions simultaneously remains a significant challenge. In this paper, we propose a novel Dual-Modality Co-Learning framework tailored for these characteristics, which achieves more effectual deepfake detection with complementary information from RGB and optical flow modalities. In particular, we designed a Multi-Scale Motion Regularization module to encourage the network to equally prioritize both the significant spatial cues and the subtle temporal facial motion cues. Additionally, we developed a Multi-Span Cross-Attention module to effectively integrate the information from both RGB and optical flow modalities and improve the detection accuracy with multi-span predictions. Extensive experiments validate the effectiveness our ideas and demonstrate the superior performance of our approach. © 2023 Owner/Author.","Deepfake Detection; Digital Forensics; Spatio-Temporal Analysis"
"Chotaliya, H.; Khatri, M.A.; Kanojiya, S.; Bivalkar, M.","Chotaliya, Harsh (58939914800); Khatri, Mohammed Adil (58940233600); Kanojiya, Shubham (58940133500); Bivalkar, Mandar K. (39861049600)","58939914800; 58940233600; 58940133500; 39861049600","Review: DeepFake Detection Techniques using Deep Neural Networks (DNN)","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187774750&partnerID=40&md5=1e896f294db590bfb767cfd098945c3e","In the age of advanced digital manipulation, deepfake videos pose a significant threat to society by allowing the creation of highly convincing counterfeit footage. The application of deep learning has proven instrumental in tackling an extensive array of practical issues and real-world applications. However, alongside its substantial benefits, there exist certain disadvantages. Deepfake videos involve the substitution of one person's characteristics with those of another, achieved through the application of advanced Deep Learning techniques. This technology can be exploited with harmful intentions, leading to the dissemination of misinformation, manipulation, and persuasive content. This paper explores multiple deep learning techniques designed for detecting deep fake images and videos. It conducts a comparative analysis of these techniques, including CNN models like ResNet, VGG16, and Efficient Net, along with RNN models like LSTM, to assess their effectiveness in deepfake video detection. © 2023 IEEE.","CNN; DeepFake detection; GAN; LSTM; RNN"
"Chen, J.; Lin, W.; Xu, J.","Chen, Jia (59291234800); Lin, Weiguo (56292213700); Xu, Junfeng (58136966000)","59291234800; 56292213700; 58136966000","Deepfake Detection Using Graph Representation with Multi-dimensional Features","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187389855&partnerID=40&md5=4014d85caa73d5a9dd119c3910df0455","The proliferation of fake video poses a significant threat to the authority and authenticity of news across multiple domains. The most existing methods of deepfake detection primarily focus on identifying the face as a whole in a video, ignoring the correlation between the facial components. However, our investigation indicates that constituent potions of a face have different effects in deepfake detection. To address this issue, we divided the face in a video frame into several regions and explored the relationship between these regions. Our approach involves constructing a feature graph of this correlation, aiming to make use of the relationship and temporal characteristics between regions of a face in a deepfake video. To begin with, the features of each facial region are extracted through CNN. Subsequently, the feature graph of the entire video is constructed with these features being the vertices and the correlation being the edge. A graph neural network is finally utilized to determine whether the video has been tampered with. Our experiments on several publicly accessible datasets demonstrate that the proposed approach outperforms other state-of-the-art deepfake detection techniques in most scenarios. © 2023 IEEE.","deepfake detection; graph representation; temporal characteristics"
"Sharma, V.K.; Garg, R.; Caudron, Q.","Sharma, Vishal Kumar (57199918726); Garg, Rakesh (57205734819); Caudron, Quentin (56254835500)","57199918726; 57205734819; 56254835500","Spatio-Temporal Convolutional Neural Networks for Deepfake Detection: An Empirical Study","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186122371&partnerID=40&md5=183c580909ec207b85c3aa7f91aa85e8","As the creation of deepfakes becomes more prevalent and sophisticated, the need for accurate and robust detection methods intensifies. This paper presents a comprehensive empirical study on the efficacy of S patio-Temporal Convolutional Neural Networks (ST-CNNs) for deepfake detection. It explores how the rich spatio-Temporal information contained within video frames can be exploited by ST-CNNs to distinguish between genuine and manipulated content. The study is underpinned by a robust testing framework, wherein a range of deepfake generation techniques are used to evaluate the detection model. It further investigates the effect of various layers and architectural elements on detection performance. The results demonstrate that ST-CNNs, by leveraging spatio-Temporal correlations, can offer superior deepfake detection performance compared to the conventional CNN models. This work can guide the development of more efficient and effective deepfake detection strategies by providing empirical insights into the utilization of ST-CNNs. © 2023 IEEE.","convolutional neural network; deepfake detection; spatio temporal; synthetic media; video forgery"
"Bharadwaj, R.; Ratnaparkhi, S.; Rajpurohit, R.; Rahate, K.; Pandita, R.; Thosar, S.","Bharadwaj, Rakhi Joshi (57210460508); Ratnaparkhi, Soham (58485869400); Rajpurohit, Rajendrasingh (58908245100); Rahate, Kashish (58907790800); Pandita, Rahul (36675522700); Thosar, Samarth (58908704100)","57210460508; 58485869400; 58908245100; 58907790800; 36675522700; 58908704100","Deepfake detection for preventing Audio and Video frauds using Advanced Deep Learning Techniques","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186082979&partnerID=40&md5=93523ccb77c3599820cd812e2ab56eef","Deepfakes, powered by generative adversarial networks (GANs), automate the creation of deceptive videos. To combat this menace, the study assembles an advanced model that fuses ResNeXt, Long Short-Term Memory (LSTM), and ResNet architectures, renowned for their efficacy in handling visual and temporal aspects, for the detection of deepfakes in audio and video content. Pre-processing, facilitated by a Multi-Task Cascaded-Convolutional Neural Network (MTCNN), accurately extracts facial regions. The model undergoes rigorous evaluation across three key datasets which are, FaceForensics++(FF-DF), Celeb-DF, and Facebook Deepfake Detection Challenge (DFDC), affirming its real-world readiness and exceptional accuracy. The combined models consistently deliver highly precise results, maintaining their robustness against evolving deepfake technologies. © 2023 IEEE.","Celeb-DF; Deepfakes; DFDC; FaceForensics++; Facial region extraction; GANs; LSTM; MTCNN; ResNet; ResNeXt"
"Ritter, P.; Lucian, D.; Anderies, A.; Chowanda, A.","Ritter, Pattrick (58694162900); Lucian, Devan (58694163000); Anderies, Anderies (57250942900); Chowanda, Andry (56335823800)","58694162900; 58694163000; 57250942900; 56335823800","Comparative Analysis and Evaluation of CNN Models for Deepfake Detection","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176592192&partnerID=40&md5=71a5c0f14de619aef7236ac234d5cc0a","Deepfake technology has become a significant concern due to its ability to create highly realistic fake videos and images, leading to the potential deception of individuals. Detecting deepfakes has become a critical research area in computer vision and multimedia forensics. This paper presents a comparative analysis of deepfake detection models, focusing on evaluating their accuracy and robustness. Four CNN models, namely ResNet-152, MobilenetV3, Convnext Large, and EffecientNetB7, were implemented and trained using a custom dataset obtained from FaceForensics++. The models were evaluated based on training accuracy, average loss, and testing accuracy. An LSTM layer was also incorporated into each model's architecture to leverage sequential information. The results demonstrate varying performance among the models, with EfficientNet B7 achieving the highest testing accuracy of 75%. The findings of this study provide insights for future research in this critical area. © 2023 IEEE.","accuracy; CNN models; comparative analysis; deepfake detection; LSTM layer"
"Guo, W.; Du, S.; Deng, H.; Yu, Z.; Feng, L.","Guo, Wenxuan (58559615500); Du, Shuo (58560725100); Deng, Huiyuan (57222188054); Yu, Zikang (58560095400); Feng, Lin (36561284000)","58559615500; 58560725100; 57222188054; 58560095400; 36561284000","Towards Spatio-temporal Collaborative Learning: An End-to-End Deepfake Video Detection Framework","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169560939&partnerID=40&md5=9c515954992d10a601cac61d46f110fd","With the rapid development of facial tampering techniques, the deepfake detection task has attracted widespread social concerns. Most existing video-based methods adopt temporal convolution to learn temporal discontinuities directly, where they might neglect to explore both local detail mutation and inconsistent global expression semantics in the temporal dimension. This makes it difficult to learn more discriminative forgery cues. To mitigate this issue, we introduce a novel deepfake video detection framework specifically designed to capture fine-grained traces of tampering. Concretely, we first present a Multi-layered Feature Extraction module (MFE) that constructs comprehensive spatio-temporal representations by stitching different levels of features together. Afterward, we propose a Bidirectional temporal Artifact Enhancement module (BAE), which exploits local differences between adjacent frames to enhance frame-level features. Moreover, we present a Cross temporal Stride Aggregation strategy (CSA) to mine inconsistent global semantics and adaptively obtain multi-timescale representations. Extensive experiments on several benchmarks demonstrate that the proposed method outperforms state-of-the-art performance compared to other competitive approaches. © 2023 IEEE.","Deep Learning; Deepfake Detection; Face Forensics; Spatio-temporal Modeling"
"Thakkar, K.; Lo, D.","Thakkar, Kirtan (58266748400); Lo, Dan Chia Tien (49663466400)","58266748400; 49663466400","Video Normalization in Identifying Fake Videos Using a Long Short-Term Memory Model","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159785774&partnerID=40&md5=22b487f02d9c53c7726794266f000b31","As the misinformation crisis continues, it creates a generation more politically divided than ever before. One of the most concerning types of misinformation are Deepfake videos, which use Generative Adversarial Networks to replace an existing video with another person's face. Deepfake videos can interfere with diplomatic relations, reduce trust in journalism, and can tamper with court video evidence, which is why it is imperative to detect these videos accurately. Long Short-Term Memory models (LSTMs) are a type of Recurrent Neural Network, meaning that they are able to remember sequential information, which is helpful for processing the frames of a video. LSTMs are special in that they can account for lags between frames of a video, which makes them perfect for Deepfake video detection. One of the potential ways to increase the accuracy of a neural network is normalization, which ensures the input data are on the same scale, so the machine has to process a lower range of data. Because the effect of normalization varies for each dataset, in this study, image normalization was used- each pixel of the frames of Deepfake videos were converted to an RGB value of 0 to 255 to see if this can increase the accuracy of the LSTM model for Deepfake detection. First, a baseline LSTM model was created for Deepfake detection with the Pytorch library, and a classification accuracy of 88.191% was achieved. After that, the first ten frames of the Deepfake videos in the dataset were passed through an image normalization algorithm. This yielded an accuracy of 94.274%, illustrating that the addition of a video normalization algorithm to Deepfake videos increased the accuracy of the Deepfake detection LSTM model by 6.083%. This is a substantial improvement, and the study showed that video normalization can be very beneficial for Deepfake video detection. © 2023 IEEE.","Deepfake Detection; Long Short-term Memory (LSTM); Video Normalization"
"Lu, T.; Bao, Y.; Li, L.","Lu, Tianliang (55343546900); Bao, Yuxuan (58451934300); Li, Lanting (58102084700)","55343546900; 58451934300; 58102084700","Deepfake Video Detection Based on Improved CapsNet and Temporal–Spatial Features","2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148000937&partnerID=40&md5=a969d8229bd49132b7ee0ac8e944c6fe","Rapid development of deepfake technology led to the spread of forged audios and videos across network platforms, presenting risks for numerous countries, societies, and individuals, and posing a serious threat to cyberspace security. To address the problem of insufficient extraction of spatial features and the fact that temporal features are not considered in the deepfake video detection, we propose a detection method based on improved CapsNet and temporal–spatial features (iCapsNet–TSF). First, the dynamic routing algorithm of CapsNet is improved using weight initialization and updating. Then, the optical flow algorithm is used to extract interframe temporal features of the videos to form a dataset of temporal–spatial features. Finally, the iCapsNet model is employed to fully learn the temporal–spatial features of facial videos, and the results are fused. Experimental results show that the detection accuracy of iCapsNet–TSF reaches 94.07%, 98.83%, and 98.50% on the Celeb-DF, FaceSwap, and Deepfakes datasets, respectively, displaying a better performance than most existing mainstream algorithms. The iCapsNet–TSF method combines the capsule network and the optical flow algorithm, providing a novel strategy for the deepfake detection, which is of great significance to the prevention of deepfake attacks and the preservation of cyberspace security. © 2023 Tech Science Press. All rights reserved.","CapsNet; Deepfake detection; optical flow algorithm; temporal–spatial features"
"Yesugade, T.; Kokate, S.; Patil, S.; Varma, R.; Pawar, S.","Yesugade, Tejaswini (58110140100); Kokate, Shrikant R. (57204112911); Patil, Sarjana (58110147600); Varma, Ritik (58110137000); Pawar, Sejal (58110137100)","58110140100; 57204112911; 58110147600; 58110137000; 58110137100","Deepfake detection using LSTM-based neural network","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148481985&partnerID=40&md5=2992c5a3f0f106f4df65684d37398231","In this rapid growth of social media, new developments in deep generative network have improve quality of creating more realistic fake videos; such videos are called as deepfake video. Such deepfake videos are used in politics to create political distress, for blackmail and for terrorism events. In order to reduce harm that can be done using such methods and prevent spread of such fake images or videos, we proposed a method that can detect such deepfake. In this paper, we proposed a new method to detect AI-generated fakeIvideos using algorithm such as CNN and LSTM. Our method will detect deepfake by using ResNext50 and LSTM algorithms, which gives accuracy around 88%. © 2022 Scrivener Publishing LLC.","CNN; Data loader; Deepfake detection; LSTM; ResNext50"
"Chen, B.; Li, T.; Ding, W.","Chen, Beijing (36805188500); Li, Tianmu (57574695400); Ding, Weiping (57193448087)","36805188500; 57574695400; 57193448087","Detecting deepfake videos based on spatiotemporal attention and convolutional LSTM","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128119994&partnerID=40&md5=581010604140a13109c1122b7435cd0a","Fake face detection is in dilemma with the rapid development of face manipulation technology. One way to improve the effectiveness of detector is to make full use of intra and inter frame information. In this paper, a novel Xception-LSTM algorithm is proposed by using our new spatiotemporal attention mechanism and convolutional long short-term memory (ConvLSTM). In the algorithm, the spatiotemporal attention mechanism, including spatial and temporal attention mechanism, is proposed to capture and enhance spatiotemporal correlations before dimension reduction of Xception. Thereafter, the ConvLSTM is introduced to consider frame structure information while modeling temporal information. The experimental results on three widely used datasets demonstrate that the proposed algorithms perform better than the state-of-the-art algorithms. In addition, the effectiveness of the spatiotemporal attention mechanism and ConvLSTM are illustrated in ablation experiments. © 2022 Elsevier Inc.","Attention mechanism; Convolutional LSTM; Deepfake detection; Face identification"
"Liu, D.; Yang, Z.; Zhang, R.; Liu, J.","Liu, Dazhuang (58175923000); Yang, Zhen (57198698830); Zhang, Ru (56890004200); Liu, Jianyi (55705849900)","58175923000; 57198698830; 56890004200; 55705849900","A Robust Deepfake Video Detection Method based on Continuous Frame Face-swapping","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152142619&partnerID=40&md5=9795e8daa7d5899d8009518b540c6894","Detection of deepfake videos faces serious generalization problem in real world application scenarios. Existing robust deepfake detection methods can only works on single frame image but not continuous frame videos. In this paper, we propose a robust deepfake video detection method based on continuous frame face-swapping. We design our face-swapping dataset with Delaunay triangulation and piecewise affine transform to achieve continuous frame face-swapping. We design a feature enhancement module with facial and background information covered to make the method focus on the mask fusion zone. We build our detection model with Efficient Net to extract intra-frame fusion feature and LSTM to extract inter-frame time feature. Cross-domain experiments show that our method achieves better detection AUC than existing methods, which proves our method is robust because of generalization. © 2022 IEEE.","continuous frame face-swapping; deepfake detection; Efficient Net; generalization; LSTM"
"Ilyas, H.; Irtaza, A.; Javed, A.; Malik, K.M.","Ilyas, Hafsa (57894630000); Irtaza, Aun (54882450900); Javed, Ali (57190125008); Malik, Khalid Mahmood (57200448301)","57894630000; 54882450900; 57190125008; 57200448301","Deepfakes Examiner: An End-to-End Deep Learning Model for Deepfakes Videos Detection","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147686689&partnerID=40&md5=c891a3a8ef70adf828fb4425e29e9ed2","Deepfakes generation approaches have made it possible even for less technical users to generate fake videos using only the source and target images. Thus, the threats associated with deepfake video generation such as impersonating public figures, defamation, and spreading disinformation on media platforms have increased exponentially. The significant improvement in the deepfakes generation techniques necessitates the development of effective deepfakes detection methods to counter disinformation threats. Existing techniques do not provide reliable deepfakes detection particularly when the videos are generated using different deepfakes generation techniques and contain variations in illumination conditions and diverse ethnicities. Therefore, this paper proposes a novel hybrid deep learning framework, InceptionResNet-BiLSTM, that is robust to different ethnicities and varied illumination conditions, and able to detect deepfake videos generated using different techniques. The proposed InceptionResNet-BiLSTM consists of two components: customized InceptionResNetV2 and Bidirectional Long-Short Term Memory (BiLSTM). In our proposed framework, faces extracted from the videos are fed to our customized InceptionResNetV2 for extracting frame-level learnable features. The sequences of features are then used to train a temporally aware BiLSTM to classify between the real and fake video. We evaluated our proposed approach on the diverse, standard, and largescale FaceForensics++ (FF++) dataset containing videos manipulated using different techniques (i.e., DeepFakes, FaceSwap, Face2Face, FaceShifter, and NeuralTextures) and the FakeA VCeleb dataset. Our method achieved an accuracy greater than 90% on DeepFakes, FaceSwap, and Face2Face subsets. Performance and generalizability evaluation highlights the effectiveness of our method for detecting deepfake videos generated through different techniques on diverse FF++ and FakeA VCeleb datasets. © 2022 IEEE.","Bidirectional LSTM; Deepfakes Detection; Face-swap; FaceForensics++; FakeAVCeleb; InceptionResNetV2; Puppet-master"
"Nguyen, H.T.; Dao, T.C.; Phan, T.M.N.; Phan, T.T.","Nguyen, Thanh Hai (57209166507); Dao, Congtinh (57220594100); Phan, Thao Minh Nguyen (57221946242); Phan, Tai Tan (57188881882)","57209166507; 57220594100; 57221946242; 57188881882","Fake face detection in video using shallow deep learning architectures","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147539891&partnerID=40&md5=5c086fb8ad5e36b81cd3217cc01c1713","Deep learning techniques have been used in various disciplines, ranging from simple data processing to complicated image classification tasks. Deepfakes is a deep learning approach with benefits and drawbacks impacting the world. However, deepfakes are now cutting-edge technology being exploited for nefarious purposes such as the breach of human privacy and identity. Because deep learning is advancing rapidly daily, people use AI to produce deepfakes videos and images. Hence newer AI technology to detect deepfakes is critical. Therefore, the study has proposed detecting video and image deepfakes based on convolutional neural network (CNN) combined with the long short-term memory (LSTM) model, which constructs a deep learning model classifying images and video deepfakes. The proposed model investigated a novel approach to research more powerful models which can be applied to any large dataset. The experimental results demonstrated that the proposed method had achieved promising performance on modified datasets from Celeb-DF with high AUC performance up to 0.7584 and MCC reaching 0.558. Besides, this paper presents brief research on creating and detecting the image and video deepfakes technologies and points out the challenges of using deepfakes in many different contexts. © © 2022 Inderscience Enterprises Ltd.","CNN; convolutional neural network; deep learning; deepfake detection; deepfakes; long short-term memory; LSTM"
"John, J.; Sherif, B.V.","John, Jerry (57271904100); Sherif, Bismin V. (57216592661)","57271904100; 57216592661","Multi-model DeepFake Detection Using Deep and Temporal Features","2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135770471&partnerID=40&md5=7321f7bdcb13c6f8ed3b4703bb2a3475","Deepfakes are one of the most advanced technological frauds that can be seen in the outer world, and it has been classified as one of the significant adverse impacts of deep learning. Deepfakes are synthetic media created by superimposing a targeted person’s visual characteristics into a source video. This results in a video which contains content that the targeted person has never done. This kind of digital fraud can cause many social relevant problems like damaging the image and dignity of famous public figures, hate campaigns, blackmailing etc. Because of these reasons, it is high time to find some methods to detect these deep fakes even before they are published. For that, a deepfake detection method is proposed using a deep neural network. A combination of a temporal model-based and a deep model-based deepfake detection is used. For the temporal based model, a combination of ResNext and LSTM architectures and for the deep model based deepfake detection, a triplet model architecture is used. The datasets used for training this model are DFDC, Celeb-DF, and Faceforensics++, composed of different deepfake creation techniques. The extensive experiments show that the temporal model obtained the highest testing accuracy of 92.42% accuracy, at a frame rate of 100, and the triplet model obtained an accuracy of 91.88%. The final pipeline of these models obtain a testing accuracy of 94.31%. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Deepfake; DeepFake detection; LSTM Resnext model; Multi model deepfake detection; Temporal model; Triplet CNN"
"Su, Y.; Xia, H.; Liang, Q.; Nie, W.","Su, Yishan (35202127500); Xia, Huawei (57244303200); Liang, Qi (57205701510); Nie, Weizhi (54080080700)","35202127500; 57244303200; 57205701510; 54080080700","Exposing DeepFake Videos Using Attention Based Convolutional LSTM Network","2021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114146226&partnerID=40&md5=4ed867b68b7307466f84115a1f21d6f3","The detection of face tampering in videos created by artificial intelligence techniques (commonly known as the Deep Fakes) has become an important and challenging task in network security defense. In this paper, we propose a novel attention-based deep fake video detection method, which captures the sharp changes in terms of the facial features caused by the composite video. We utilize the convolutional long short-term memory to extract both spatial and temporal information of DeeFake videos. Meanwhile, we apply the attention mechanism to emphasize the specific facial area of each video frame. Finally, we design a decoder to further fusion multiple frames information for more accurate detection results. Experimental results and comparisons with state-of-the-art methods demonstrate that our framework achieves superior performance. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Attention; Convolutional LSTM; DeepFake detection"
"Wang, Z.; Li, X.; Ni, R.; Zhao, Y.","Wang, Zhibing (57330972800); Li, Xin (57775419200); Ni, Rongrong (55632437300); Zhao, Yao (35304414700)","57330972800; 57775419200; 55632437300; 35304414700","Attention Guided Spatio-Temporal Artifacts Extraction for Deepfake Detection","2021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118942938&partnerID=40&md5=7fb41a1abc4766eb0e8e92911f9f41c0","Recently, deep-learning based model has been widely used for deepfake video detection due to its effectiveness in artifacts extraction. Most of the existing deep-learning detection methods with the attention mechanism attach more importance to the information in the spatial domain. However, the discrepancy of different frames is also important and should pay different levels of attention to temporal regions. To address this problem, this paper proposes an Attention Guided LSTM Network (AGLNet), which takes into consideration the mutual correlations in both temporal and spatial domains to effectively capture the artifacts in deepfake videos. In particular, sequential feature maps extracted from convolution and fully-connected layers of the convolutional neural network are receptively fed into the attention guided LSTM module to learn soft spatio-temporal assignment weights, which help aggregate not only detailed spatial information but also temporal information from consecutive video frames. Experiments on FaceForensics++ and Celeb-DF datasets demonstrate the superiority of the proposed AGLNet model in exploring the spatio-temporal artifacts extraction. © 2021, Springer Nature Switzerland AG.","Attention; Deepfake detection; Spatio-temporal artifacts"
"Zhao, L.; Ge, W.; Mao, Y.; Han, M.; Li, W.; Li, X.","Zhao, Lei (56461651900); Ge, Wanfeng (57203191424); Mao, Yuzhu (57218292735); Han, Meng (57218292590); Li, Wenxin (57215563720); Li, Xue (58663674000)","56461651900; 57203191424; 57218292735; 57218292590; 57215563720; 58663674000","Deepfake Video Detection Model Based on Consistency of Spatial-Temporal Features; 基于时空特征一致性的Deepfake视频检测模型","2020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088657032&partnerID=40&md5=8c0156d45d7ed5a1882211b167b01b1d","In order to improve the feature utilization rate of the image to be detected, a Deepfake video detection model based on consistency of spatial-temporal features was proposed, inspired by the observation that there are slight inconsistency and discontinuity in the facial expression changes of the characters in Deepfake videos. In the model, the convolutional neural network (CNN) was used to extract the spatial features from the video to be detected, and an optical flow method was used to perform temporal features between consecutive frames of the video. Then another CNN was used to extract the abstract and in-depth features from the optical flow map. After the temporal features and spatial features were transformed from original representation space to a new feature space by neural networks, a fully connected neural network was used to classify the combined spatial and temporal features space to achieve the detection target. The model proposed in the paper was trained on the Faceforensics++, an open source Deepfake dataset. The experimental results indicated that the detection accuracy of the proposed model reaches 98.1%, and the AUC value reaches 0.998 1. By comparing with the existing Deepfake detection models, the proposed model is superior to the existing models in terms of detection accuracy and AUC value, which verifies the effectiveness of the proposed model. © 2020, Editorial Department of Advanced Engineering Sciences. All right reserved.","Deepfake detection; Fake images; Spatial features; Temporal features"
"Zhao, Y.; Ge, W.; Li, W.; Wang, R.; Zhao, L.; Ming, J.","Zhao, Yiru (57215564031); Ge, Wanfeng (57203191424); Li, Wenxin (57215563720); Wang, Run (55939516400); Zhao, Lei (56461651900); Ming, Jiang (50861739500)","57215564031; 57203191424; 57215563720; 55939516400; 56461651900; 50861739500","Capturing the Persistence of Facial Expression Features for Deepfake Video Detection","2020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081173102&partnerID=40&md5=1f8c95fa77c0da45c34b2cb2547bf2ff","The security of the Deepfake video has become the focus of social concern. This kind of fake video not only infringes copyright and privacy but also poses potential risks to politics, journalism, social trust, and other aspects. Unfortunately, fighting against Deepfake video is still in its early stage and practical solutions are required. Currently, biological signal based and learning-based are two major ways in detecting Deepfake video. We explore that facial expression between two adjacent frames appears significant differences in generative adversarial network (GAN)-synthesized fake video, while in a real video the facial expression looks naturally and transforms in a smooth way across frames. In this paper, we employ optical flow to capture the obvious differences of facial expressions between adjacent frames in a video and incorporate the temporal characteristics of consecutive frames into a convolutional neural network (CNN) model to distinguish the Deepfake video. In our experiments, we evaluate the effectiveness of our approach on a publicly fake video dataset, FaceForensics++. Experimental results show that our proposed approach achieves an accuracy higher than 98.1% and the AUC score reaches more than 0.9981. © 2020, Springer Nature Switzerland AG.","Deepfake Detection; Optical flow; Spatial features; Temporal features"
